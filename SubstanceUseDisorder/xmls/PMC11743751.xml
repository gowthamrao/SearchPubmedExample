<?xml version="1.0"  ?><!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd"><pmc-articleset><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" article-type="research-article" xml:lang="en" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">NPJ Digit Med</journal-id><journal-id journal-id-type="iso-abbrev">NPJ Digit Med</journal-id><journal-id journal-id-type="pmc-domain-id">3605</journal-id><journal-id journal-id-type="pmc-domain">npjdigitmed</journal-id><journal-title-group><journal-title>NPJ Digital Medicine</journal-title></journal-title-group><issn pub-type="epub">2398-6352</issn><publisher><publisher-name>Nature Publishing Group</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC11743751</article-id><article-id pub-id-type="pmcid-ver">PMC11743751.1</article-id><article-id pub-id-type="pmcaid">11743751</article-id><article-id pub-id-type="pmcaiid">11743751</article-id><article-id pub-id-type="pmid">39828800</article-id><article-id pub-id-type="doi">10.1038/s41746-024-01377-1</article-id><article-id pub-id-type="publisher-id">1377</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Clinical entity augmented retrieval for clinical information extraction</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" equal-contrib="yes"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0003-0246-2180</contrib-id><name name-style="western"><surname>Lopez</surname><given-names initials="I">Ivan</given-names></name><address><email>ivlopez@stanford.edu</email></address><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author" equal-contrib="yes"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0003-3426-9289</contrib-id><name name-style="western"><surname>Swaminathan</surname><given-names initials="A">Akshay</given-names></name><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Vedula</surname><given-names initials="K">Karthik</given-names></name><xref ref-type="aff" rid="Aff3">3</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Narayanan</surname><given-names initials="S">Sanjana</given-names></name><xref ref-type="aff" rid="Aff4">4</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Nateghi Haredasht</surname><given-names initials="F">Fateme</given-names></name><xref ref-type="aff" rid="Aff4">4</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0003-3738-9569</contrib-id><name name-style="western"><surname>Ma</surname><given-names initials="SP">Stephen P.</given-names></name><xref ref-type="aff" rid="Aff5">5</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Liang</surname><given-names initials="AS">April S.</given-names></name><xref ref-type="aff" rid="Aff6">6</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Tate</surname><given-names initials="S">Steven</given-names></name><xref ref-type="aff" rid="Aff7">7</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Maddali</surname><given-names initials="M">Manoj</given-names></name><xref ref-type="aff" rid="Aff2">2</xref><xref ref-type="aff" rid="Aff8">8</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-2601-0173</contrib-id><name name-style="western"><surname>Gallo</surname><given-names initials="RJ">Robert Joseph</given-names></name><xref ref-type="aff" rid="Aff9">9</xref><xref ref-type="aff" rid="Aff10">10</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0001-9385-7158</contrib-id><name name-style="western"><surname>Shah</surname><given-names initials="NH">Nigam H.</given-names></name><xref ref-type="aff" rid="Aff4">4</xref><xref ref-type="aff" rid="Aff11">11</xref><xref ref-type="aff" rid="Aff12">12</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-4387-8740</contrib-id><name name-style="western"><surname>Chen</surname><given-names initials="JH">Jonathan H.</given-names></name><xref ref-type="aff" rid="Aff2">2</xref><xref ref-type="aff" rid="Aff4">4</xref><xref ref-type="aff" rid="Aff5">5</xref><xref ref-type="aff" rid="Aff12">12</xref><xref ref-type="aff" rid="Aff13">13</xref></contrib><aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/00f54p054</institution-id><institution-id institution-id-type="GRID">grid.168010.e</institution-id><institution-id institution-id-type="ISNI">0000000419368956</institution-id><institution>Stanford University School of Medicine, </institution></institution-wrap>Stanford, CA USA </aff><aff id="Aff2"><label>2</label>Department of Biomedical Data Science, Stanford, CA USA </aff><aff id="Aff3"><label>3</label>Poolesville High School, Poolesville, MD USA </aff><aff id="Aff4"><label>4</label>Stanford Center for Biomedical Informatics Research, Stanford, CA USA </aff><aff id="Aff5"><label>5</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/00f54p054</institution-id><institution-id institution-id-type="GRID">grid.168010.e</institution-id><institution-id institution-id-type="ISNI">0000000419368956</institution-id><institution>Division of Hospital Medicine, </institution><institution>Stanford University School of Medicine, </institution></institution-wrap>Stanford, CA USA </aff><aff id="Aff6"><label>6</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/00f54p054</institution-id><institution-id institution-id-type="GRID">grid.168010.e</institution-id><institution-id institution-id-type="ISNI">0000000419368956</institution-id><institution>Division of Clinical Informatics, </institution><institution>Stanford University School of Medicine, </institution></institution-wrap>Stanford, CA USA </aff><aff id="Aff7"><label>7</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/00f54p054</institution-id><institution-id institution-id-type="GRID">grid.168010.e</institution-id><institution-id institution-id-type="ISNI">0000000419368956</institution-id><institution>Department of Psychiatry and Behavioral Sciences, </institution><institution>Stanford University School of Medicine, </institution></institution-wrap>Stanford, CA USA </aff><aff id="Aff8"><label>8</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/00f54p054</institution-id><institution-id institution-id-type="GRID">grid.168010.e</institution-id><institution-id institution-id-type="ISNI">0000000419368956</institution-id><institution>Division of Pulmonary, Allergy, and Critical Care Medicine, </institution><institution>Stanford University School of Medicine, </institution></institution-wrap>Stanford, CA USA </aff><aff id="Aff9"><label>9</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/00nr17z89</institution-id><institution-id institution-id-type="GRID">grid.280747.e</institution-id><institution-id institution-id-type="ISNI">0000 0004 0419 2556</institution-id><institution>Center for Innovation to Implementation, </institution><institution>VA Palo Alto Healthcare System, </institution></institution-wrap>Menlo Park, CA USA </aff><aff id="Aff10"><label>10</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/00f54p054</institution-id><institution-id institution-id-type="GRID">grid.168010.e</institution-id><institution-id institution-id-type="ISNI">0000 0004 1936 8956</institution-id><institution>Department of Health Policy, </institution><institution>Stanford University, </institution></institution-wrap>Stanford, CA USA </aff><aff id="Aff11"><label>11</label>Technology and Digital Solutions, Stanford Healthcare, Palo Alto, USA </aff><aff id="Aff12"><label>12</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/00f54p054</institution-id><institution-id institution-id-type="GRID">grid.168010.e</institution-id><institution-id institution-id-type="ISNI">0000000419368956</institution-id><institution>Clinical Excellence Research Center, </institution><institution>Stanford University School of Medicine, </institution></institution-wrap>Stanford, CA USA </aff><aff id="Aff13"><label>13</label>Department of Medicine, Stanford, CA USA </aff></contrib-group><pub-date pub-type="epub"><day>19</day><month>1</month><year>2025</year></pub-date><pub-date pub-type="collection"><year>2025</year></pub-date><volume>8</volume><issue-id pub-id-type="pmc-issue-id">478273</issue-id><elocation-id>45</elocation-id><history><date date-type="received"><day>1</day><month>7</month><year>2024</year></date><date date-type="accepted"><day>8</day><month>12</month><year>2024</year></date></history><pub-history><event event-type="pmc-release"><date><day>19</day><month>01</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>20</day><month>01</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-01-29 00:25:14.043"><day>29</day><month>01</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; The Author(s) 2025</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article&#8217;s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article&#8217;s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p></license></permissions><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pmc-pdf" xlink:href="41746_2024_Article_1377.pdf"/><abstract id="Abs1"><p id="Par1">Large language models (LLMs) with retrieval-augmented generation (RAG) have improved information extraction over previous methods, yet their reliance on embeddings often leads to inefficient retrieval. We introduce CLinical Entity Augmented Retrieval (CLEAR), a RAG pipeline that retrieves information using entities. We compared CLEAR to embedding RAG and full-note approaches for extracting 18 variables using six LLMs across 20,000 clinical notes. Average F1 scores were 0.90, 0.86, and 0.79; inference times were 4.95, 17.41, and 20.08&#8201;s per note; average model queries were 1.68, 4.94, and 4.18 per note; and average input tokens were 1.1k, 3.8k, and 6.1k per note for CLEAR, embedding RAG, and full-note approaches, respectively. In conclusion, CLEAR utilizes clinical entities for information retrieval and achieves &gt;70% reduction in token usage and inference time with improved performance compared to modern methods.</p></abstract><kwd-group kwd-group-type="npg-subject"><title>Subject terms</title><kwd>Machine learning</kwd><kwd>Data mining</kwd></kwd-group><funding-group><award-group><funding-source><institution-wrap><institution-id institution-id-type="FundRef">https://doi.org/10.13039/100000060</institution-id><institution>U.S. Department of Health &amp; Human Services | NIH | National Institute of Allergy and Infectious Diseases (NIAID)</institution></institution-wrap></funding-source><award-id>1R01AI17812101</award-id><principal-award-recipient><name name-style="western"><surname>Chen</surname><given-names>Jonathan H.</given-names></name></principal-award-recipient></award-group></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#169; Springer Nature Limited 2025</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1" sec-type="introduction"><title>Introduction</title><p id="Par2">Free-text notes in electronic health records (EHRs) are rich with data not found within structured fields, like symptoms, diagnoses, disease course, social determinants of health, family history, and patient perspectives<sup><xref ref-type="bibr" rid="CR1">1</xref>,<xref ref-type="bibr" rid="CR2">2</xref></sup>. The ability to process this data unlocks various important research and quality improvement use cases, including cohort selection<sup><xref ref-type="bibr" rid="CR3">3</xref></sup>, phenotyping<sup><xref ref-type="bibr" rid="CR4">4</xref></sup>, observational data analysis<sup><xref ref-type="bibr" rid="CR5">5</xref></sup>, and predictive modeling<sup><xref ref-type="bibr" rid="CR6">6</xref></sup>.</p><p id="Par3">Despite the amount of valuable information in EHRs, extracting information from clinical notes remains challenging<sup><xref ref-type="bibr" rid="CR7">7</xref>,<xref ref-type="bibr" rid="CR8">8</xref></sup>. Clinical information extraction comprises several tasks, including named entity recognition (NER) (e.g., recognizing &#8220;t2dm&#8221; as type II diabetes mellitus)<sup><xref ref-type="bibr" rid="CR9">9</xref></sup>, sense disambiguation (e.g., understanding &#8220;mi&#8221; as &#8220;myocardial infarction&#8221; or &#8220;mitral insufficiency&#8221; depending on the context)<sup><xref ref-type="bibr" rid="CR10">10</xref></sup>, and relation extraction (e.g., linking a symptom with medication if reported as a side-effect)<sup><xref ref-type="bibr" rid="CR11">11</xref></sup>.</p><p id="Par4">The simplest clinical information extraction approaches use rules and dictionaries to identify entities of interest<sup><xref ref-type="bibr" rid="CR12">12</xref>,<xref ref-type="bibr" rid="CR13">13</xref></sup>, such as diagnosis codes like the International Classification of Diseases (ICD). In a 2018 review of 263 clinical information extraction methods, 65% were rule-based<sup><xref ref-type="bibr" rid="CR8">8</xref></sup>. These systems are interpretable, easy to deploy, and achieve reasonable performance on many tasks<sup><xref ref-type="bibr" rid="CR12">12</xref></sup>. However, structured fields like diagnosis codes are unable to fully capture a patient&#8217;s medical history in the current state. For example, despite a recent increase in the use of diagnosis codes to represent social determinants of health, they remain underutilized and often miss crucial contextual details only found in the unstructured text of EHRs<sup><xref ref-type="bibr" rid="CR14">14</xref>,<xref ref-type="bibr" rid="CR15">15</xref></sup>. Moreover, for many conditions, such as cancer, ICD codes do not reflect the true source of diagnosis; in these cases, pathology reports are the gold standard<sup><xref ref-type="bibr" rid="CR16">16</xref>,<xref ref-type="bibr" rid="CR17">17</xref></sup>. Natural language processing methods are therefore necessary to extract these insights, allowing for a more comprehensive understanding of patient health. Additionally, hard-coded rules and word lists fail to capture the wide variation in clinical language, including synonyms and abbreviations, and miss nuanced descriptions in EHR notes<sup><xref ref-type="bibr" rid="CR18">18</xref></sup>.</p><p id="Par5">Supervised machine learning approaches that take in a labeled dataset can recognize more complex linguistic relationships than rules- or dictionary-based methods. Neural network architectures like bi-directional Long Short-Term Memory networks (LSTMs) are well suited for sequence data-based tasks like NER, given their ability to learn relationships between a token and its neighbor tokens in either direction<sup><xref ref-type="bibr" rid="CR19">19</xref></sup>. For example, Stanza<sup><xref ref-type="bibr" rid="CR20">20</xref>,<xref ref-type="bibr" rid="CR21">21</xref></sup>, a widely used Python library for NER, uses a bi-directional LSTM with a Conditional Random Field trained on the 2010 i2b2/VA dataset<sup><xref ref-type="bibr" rid="CR22">22</xref></sup>. A disadvantage of machine learning approaches is that they often require large, labeled training datasets, which can be time-consuming and expensive to obtain. Weak supervision offers an alternative to human-labeled data, where programmatic labeling functions are used to automatically assign &#8220;weak&#8221; labels. Although the quality of weak labels is lower than human labels, training on a large number of weak labels has been shown to outperform training on a small number of human labels. Labeling functions can be manually curated or sourced from ontologies or smaller models. For example, TROVE uses ontologies like the Unified Medical Language System (UMLS) to create labeling functions and uses these weak labels to fine-tune a BERT-based model for identifying symptoms and risk factors for COVID-19<sup><xref ref-type="bibr" rid="CR23">23</xref></sup>.</p><p id="Par6">Pre-trained deep learning models like BERT use multidimensional embeddings learned from large, unlabeled corpuses. These embeddings represent semantic information that can be used as features for a variety of downstream tasks. For instance, fine-tuned BERT-based models have been employed for tasks including named entity recognition, assertion status determination, sense disambiguation, and relation extraction<sup><xref ref-type="bibr" rid="CR24">24</xref>,<xref ref-type="bibr" rid="CR25">25</xref></sup>, and are often adapted to specific clinical domains, like radiology<sup><xref ref-type="bibr" rid="CR26">26</xref>,<xref ref-type="bibr" rid="CR27">27</xref></sup>. Although these models can be fine-tuned to perform tasks like diagnostic code assignment, treatment assignment<sup><xref ref-type="bibr" rid="CR28">28</xref></sup>, and open-ended reasoning<sup><xref ref-type="bibr" rid="CR29">29</xref>&#8211;<xref ref-type="bibr" rid="CR32">32</xref></sup>, decoder-only models are typically better equipped for this task.</p><p id="Par7">Recently, large language models (LLMs) trained with transformer-based architectures on large unlabeled text corpuses have demonstrated impressive performance on both information extraction and natural language understanding tasks, such as information extraction (e.g., &#8220;does this patient have diabetes?&#8221;)<sup><xref ref-type="bibr" rid="CR33">33</xref></sup>, text summarization (e.g., &#8220;summarize this patient&#8217;s history&#8221;)<sup><xref ref-type="bibr" rid="CR34">34</xref></sup>, and conversational capabilities (e.g., &#8220;draft a response to this patient&#8217;s message&#8221;)<sup><xref ref-type="bibr" rid="CR35">35</xref></sup>. One advantage of LLMs is their &#8220;few-shot&#8221; and &#8220;zero-shot&#8221; prompting capabilities, enabling them to accomplish tasks with few to no labeled examples&#8212;tasks that previously required training or fine-tuning separate models with labeled datasets<sup><xref ref-type="bibr" rid="CR36">36</xref>&#8211;<xref ref-type="bibr" rid="CR38">38</xref></sup>. Recent work has used LLMs to extract clinical variables from EHR notes, including social determinants of health, medications, and postpartum hemorrhage<sup><xref ref-type="bibr" rid="CR4">4</xref>,<xref ref-type="bibr" rid="CR39">39</xref>,<xref ref-type="bibr" rid="CR40">40</xref></sup>. While LLMs show great promise in clinical information retrieval, they face several limitations. For instance, the length of patient notes can surpass an LLM&#8217;s context window&#8212;the amount of text that can be passed into the model. Naive approaches like truncation or selecting only documents that fit within the context window risk excluding valuable information<sup><xref ref-type="bibr" rid="CR4">4</xref>,<xref ref-type="bibr" rid="CR39">39</xref>,<xref ref-type="bibr" rid="CR41">41</xref></sup>. Dividing a note into smaller chunks with adjoining strides can address context window limits but still requires multiple LLM queries per patient, which can be computationally expensive. In addition, LLM performance has been shown to degrade on reasoning tasks as input length increases, even on models with large context windows<sup><xref ref-type="bibr" rid="CR42">42</xref>&#8211;<xref ref-type="bibr" rid="CR45">45</xref></sup>, suggesting that inputting long EHR excerpts containing extraneous information can reduce performance.</p><p id="Par8">Retrieval-augmented generation (RAG) attempts to address this limitation by retrieving and appending query-relevant information to the input context. The retriever typically uses an encoder model to represent both the query and reference information in embedding space and retrieve information whose embeddings are close to that of the query<sup><xref ref-type="bibr" rid="CR46">46</xref></sup>. Some RAG workflows embed small chunks of text that can fit within the model&#8217;s context window and store those embeddings in a database for downstream retrieval. Other approaches, mostly explored in the general domain, involve embedding and retrieving fact triplets from knowledge graphs<sup><xref ref-type="bibr" rid="CR47">47</xref>&#8211;<xref ref-type="bibr" rid="CR49">49</xref></sup>.</p><p id="Par9">An important challenge in RAG-based methods is ensuring that the retrieved information is relevant to the query and does not contain extraneous information that can hinder LLM reasoning and add to inference costs<sup><xref ref-type="bibr" rid="CR50">50</xref>,<xref ref-type="bibr" rid="CR51">51</xref></sup>.</p><p id="Par10">To address the above limitations of LLMs for clinical information extraction, we propose CLinical Entity Augmented Retrieval (CLEAR), a RAG pipeline that retrieves note chunks containing clinical entities relevant to the input query. We hypothesized that retrieval based on relevant clinical entities would lead to more efficient and relevant information retrieval compared to RAG approaches based on note chunk embeddings. We make three contributions. First, we validate the entity recognition and entity selection steps of the CLEAR pipeline, which identify clinical entities in clinical notes and select a subset relevant to the input query. Second, we compare CLEAR to a RAG approach that embeds note chunks and a full-note retrieval approach in performing information extraction for 18 clinical variables. Third, we explore the feasibility of using CLEAR to generate labels to fine-tune a BERT-sized model in performing information extraction. We conduct all experiments on two real-world EHR-derived datasets that include labels for substance use (e.g., alcohol dependence, tobacco dependence), mental health (e.g., attention-deficit/hyperactivity disorder [ADHD], bipolar disorder, depression), social determinants of health (e.g., homelessness, unemployment), and chest radiograph findings (e.g., pneumonia, cardiomegaly).</p></sec><sec id="Sec2" sec-type="results"><title>Results</title><sec id="Sec3"><title>Inter-rater reliability</title><p id="Par11">In the Stanford MOUD dataset, the unweighted Cohen&#8217;s Kappa value was 0.86 (95% CI: 0.79-0.93). In the CheXpert dataset, the unweighted Cohen&#8217;s Kappa was 0.93 (95% CI: 0.88&#8211;0.98). These values indicate excellent agreement between annotators.</p></sec><sec id="Sec4"><title>NER and entity selection evaluation</title><p id="Par12">Zero-shot NER using Flan-T5 identified 1269 out of 1382 entities (96% sensitivity) in the NCBI disease dataset and 440 out of 450 entities (99% sensitivity) in the Stanford MOUD dataset. We measured to what extent ontology and LLM augmentation recover entities missed in the NER step (false negatives) by using the UMLS ontology and GPT-4 to generate synonyms as if they were the target entity. This augmentation step increases sensitivity to 99% and 100% in the NCBI disease dataset and Stanford MOUD dataset, respectively, indicating that even if the target entity is missed during NER, it is very likely to be detected through ontology and LLM augmentation (Supplementary Table <xref rid="MOESM1" ref-type="media">1</xref>). The performance of each of the four zero-shot NER prompts on the Stanford MOUD dataset is detailed in Supplementary Table <xref rid="MOESM1" ref-type="media">2</xref>. We report the classification of false negatives for this analysis in Supplementary Table <xref rid="MOESM1" ref-type="media">3</xref>.</p><p id="Par13">We studied the impact of the initial NER step on the overall performance of CLEAR. Overall, removing NER from CLEAR and relying only on ontology and LLM augmentation hurts downstream information extraction task performance, resulting in a 0.11 decrease in F1 across all 13 variables in the Stanford MOUD dataset (0.86 without NER vs. 0.97 with NER). For unhoused, personality disorder, ADHD, PTSD, suicidal behavior, liver disease, and unemployment, removing NER resulted in an F1 drop of &#8804;0.02. For other variables, removing NER led to a drop in F1 from 0.18 (bipolar disorder) to 0.40 (substance use disorder) (Fig. <xref rid="Fig1" ref-type="fig">1</xref> and Supplementary Table <xref rid="MOESM1" ref-type="media">4</xref>). This suggests that for several variables, LLMs and ontologies do not capture the natural variation in clinical variables as effectively as NER. For a full list of high-yield terms missed by Ontology+LLM augmentations, refer to Supplementary Table <xref rid="MOESM1" ref-type="media">5</xref>.<fig id="Fig1" position="float" orientation="portrait"><label>Fig. 1</label><caption><title>CLEAR information retrieval ablation F1 scores on Stanford MOUD dataset.</title><p>F1 scores for information retrieval using NER, Ontology, LLM augmentation, or Ontology&#8201;+&#8201;LLM Augmentation on the Stanford MOUD Dataset. F1 scores were calculated for all 13 variables using GPT-4.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" id="d33e567" position="float" orientation="portrait" xlink:href="41746_2024_1377_Fig1_HTML.jpg"/></fig></p></sec><sec id="Sec5"><title>Information extraction evaluation</title><p id="Par14">On the Stanford MOUD Dataset, the average F1 score across all 13 variables and all 6 models was 0.90, ranging from 0.78 (Med42) and 0.97 (GPT-4) across models. GPT-4 had the highest F1 score for 10 out of 13 variables. F1 scores across variables ranged from 0.61 (Med42 on depression) to 1.00 (GPT-4 on personality disorder, bipolar disorder, PTSD, and unemployment; Llama-3 on unhoused; Flan-T5 on unemployment). On the CheXpert Dataset, the average F1 score across all 5 test sets for all 6 models was 0.96, ranging from 0.91 (Flan-UL2) and 0.98 (Flan-T5 and Mixtral). Flan-T5 had the highest F1 score for 3 out of 5 variables. F1 scores across variables ranged from 0.90 (Med42 on pneumothorax) to 1.00 (Flan-T5 on cardiomegaly and pleural effusion) (Table <xref rid="Tab1" ref-type="table">1</xref>). A full breakdown of CLEAR, chunk embedding, and full-note performance per variable is reported in Supplementary Table <xref rid="MOESM1" ref-type="media">6</xref>.<table-wrap id="Tab1" position="float" orientation="portrait"><label>Table 1</label><caption><p>CLEAR F1 scores for information extraction on the Stanford MOUD and CheXpert datasets</p></caption><table frame="hsides" rules="groups"><thead><tr><th colspan="1" rowspan="1">Variable</th><th colspan="1" rowspan="1">Flan-T5</th><th colspan="1" rowspan="1">Flan-UL2</th><th colspan="1" rowspan="1">GPT-4</th><th colspan="1" rowspan="1">Med42</th><th colspan="1" rowspan="1">Llama-3</th><th colspan="1" rowspan="1">Mixtral</th><th colspan="1" rowspan="1">Range</th><th colspan="1" rowspan="1">Fine-tuned BERT</th></tr></thead><tbody><tr><td colspan="9" rowspan="1">CheXpert</td></tr><tr><td colspan="1" rowspan="1">&#8195;Cardiomegaly</td><td colspan="1" rowspan="1">1.00</td><td colspan="1" rowspan="1">0.96</td><td colspan="1" rowspan="1">0.95</td><td colspan="1" rowspan="1">0.97</td><td colspan="1" rowspan="1">0.95</td><td colspan="1" rowspan="1">0.99</td><td colspan="1" rowspan="1">1.00&#8211;0.95</td><td colspan="1" rowspan="1">0.95</td></tr><tr><td colspan="1" rowspan="1">&#8195;Pulmonary edema</td><td colspan="1" rowspan="1">0.98</td><td colspan="1" rowspan="1">0.96</td><td colspan="1" rowspan="1">0.96</td><td colspan="1" rowspan="1">0.91</td><td colspan="1" rowspan="1">0.96</td><td colspan="1" rowspan="1">0.98</td><td colspan="1" rowspan="1">0.98&#8211;0.91</td><td colspan="1" rowspan="1">0.97</td></tr><tr><td colspan="1" rowspan="1">&#8195;Pleural effusion</td><td colspan="1" rowspan="1">1.00</td><td colspan="1" rowspan="1">0.84</td><td colspan="1" rowspan="1">0.97</td><td colspan="1" rowspan="1">0.97</td><td colspan="1" rowspan="1">0.98</td><td colspan="1" rowspan="1">0.98</td><td colspan="1" rowspan="1">1.00&#8211;0.84</td><td colspan="1" rowspan="1">0.89</td></tr><tr><td colspan="1" rowspan="1">&#8195;Pneumonia</td><td colspan="1" rowspan="1">0.95</td><td colspan="1" rowspan="1">0.84</td><td colspan="1" rowspan="1">0.99</td><td colspan="1" rowspan="1">0.88</td><td colspan="1" rowspan="1">0.94</td><td colspan="1" rowspan="1">0.95</td><td colspan="1" rowspan="1">0.99&#8211;0.84</td><td colspan="1" rowspan="1">0.94</td></tr><tr><td colspan="1" rowspan="1">&#8195;Pneumothorax</td><td colspan="1" rowspan="1">0.98</td><td colspan="1" rowspan="1">0.95</td><td colspan="1" rowspan="1">0.99</td><td colspan="1" rowspan="1">0.90</td><td colspan="1" rowspan="1">0.97</td><td colspan="1" rowspan="1">0.98</td><td colspan="1" rowspan="1">0.99&#8211;0.90</td><td colspan="1" rowspan="1">0.96</td></tr><tr><td colspan="1" rowspan="1">&#8195;Average</td><td colspan="1" rowspan="1">0.98</td><td colspan="1" rowspan="1">0.91</td><td colspan="1" rowspan="1">0.97</td><td colspan="1" rowspan="1">0.93</td><td colspan="1" rowspan="1">0.96</td><td colspan="1" rowspan="1">0.98</td><td colspan="1" rowspan="1">0.98&#8211;0.91</td><td colspan="1" rowspan="1">0.94</td></tr><tr><td colspan="9" rowspan="1">Stanford MOUD</td></tr><tr><td colspan="1" rowspan="1">&#8195;Depression</td><td colspan="1" rowspan="1">0.86</td><td colspan="1" rowspan="1">0.87</td><td colspan="1" rowspan="1">0.97</td><td colspan="1" rowspan="1">0.61</td><td colspan="1" rowspan="1">0.88</td><td colspan="1" rowspan="1">0.93</td><td colspan="1" rowspan="1">0.97&#8211;0.61</td><td colspan="1" rowspan="1">0.91</td></tr><tr><td colspan="1" rowspan="1">&#8195;Alcohol dependence</td><td colspan="1" rowspan="1">0.85</td><td colspan="1" rowspan="1">0.81</td><td colspan="1" rowspan="1">0.91</td><td colspan="1" rowspan="1">0.69</td><td colspan="1" rowspan="1">0.74</td><td colspan="1" rowspan="1">0.75</td><td colspan="1" rowspan="1">0.91&#8211;0.69</td><td colspan="1" rowspan="1">0.91<sup>a</sup></td></tr><tr><td colspan="1" rowspan="1">&#8195;Substance use disorder</td><td colspan="1" rowspan="1">0.89</td><td colspan="1" rowspan="1">0.88</td><td colspan="1" rowspan="1">0.91</td><td colspan="1" rowspan="1">0.71</td><td colspan="1" rowspan="1">0.84</td><td colspan="1" rowspan="1">0.94</td><td colspan="1" rowspan="1">0.94&#8211;0.71</td><td colspan="1" rowspan="1">0.87</td></tr><tr><td colspan="1" rowspan="1">&#8195;Unhoused</td><td colspan="1" rowspan="1">0.97</td><td colspan="1" rowspan="1">0.97</td><td colspan="1" rowspan="1">0.97</td><td colspan="1" rowspan="1">0.96</td><td colspan="1" rowspan="1">1.00</td><td colspan="1" rowspan="1">0.97</td><td colspan="1" rowspan="1">1.00&#8211;0.96</td><td colspan="1" rowspan="1">0.94</td></tr><tr><td colspan="1" rowspan="1">&#8195;Tobacco dependence</td><td colspan="1" rowspan="1">0.95</td><td colspan="1" rowspan="1">0.98</td><td colspan="1" rowspan="1">0.99</td><td colspan="1" rowspan="1">0.70</td><td colspan="1" rowspan="1">0.90</td><td colspan="1" rowspan="1">0.92</td><td colspan="1" rowspan="1">0.99&#8211;0.70</td><td colspan="1" rowspan="1">0.90</td></tr><tr><td colspan="1" rowspan="1">&#8195;Personality disorder</td><td colspan="1" rowspan="1">0.81</td><td colspan="1" rowspan="1">0.90</td><td colspan="1" rowspan="1">1.00</td><td colspan="1" rowspan="1">0.67</td><td colspan="1" rowspan="1">0.97</td><td colspan="1" rowspan="1">0.86</td><td colspan="1" rowspan="1">1.00&#8211;0.67</td><td colspan="1" rowspan="1">0.95</td></tr><tr><td colspan="1" rowspan="1">&#8195;Bipolar disorder</td><td colspan="1" rowspan="1">0.90</td><td colspan="1" rowspan="1">0.94</td><td colspan="1" rowspan="1">1.00</td><td colspan="1" rowspan="1">0.91</td><td colspan="1" rowspan="1">0.89</td><td colspan="1" rowspan="1">0.94</td><td colspan="1" rowspan="1">1.00&#8211;0.89</td><td colspan="1" rowspan="1">0.90</td></tr><tr><td colspan="1" rowspan="1">&#8195;PTSD</td><td colspan="1" rowspan="1">0.95</td><td colspan="1" rowspan="1">0.95</td><td colspan="1" rowspan="1">1.00</td><td colspan="1" rowspan="1">0.89</td><td colspan="1" rowspan="1">0.96</td><td colspan="1" rowspan="1">0.94</td><td colspan="1" rowspan="1">1.00&#8211;0.89</td><td colspan="1" rowspan="1">0.85</td></tr><tr><td colspan="1" rowspan="1">&#8195;ADHD</td><td colspan="1" rowspan="1">0.94</td><td colspan="1" rowspan="1">0.97</td><td colspan="1" rowspan="1">0.97</td><td colspan="1" rowspan="1">0.77</td><td colspan="1" rowspan="1">0.87</td><td colspan="1" rowspan="1">0.84</td><td colspan="1" rowspan="1">0.97&#8211;0.77</td><td colspan="1" rowspan="1">0.77</td></tr><tr><td colspan="1" rowspan="1">&#8195;Suicidal behavior</td><td colspan="1" rowspan="1">0.96</td><td colspan="1" rowspan="1">0.95</td><td colspan="1" rowspan="1">0.99</td><td colspan="1" rowspan="1">0.83</td><td colspan="1" rowspan="1">0.91</td><td colspan="1" rowspan="1">0.97</td><td colspan="1" rowspan="1">0.99&#8211;0.83</td><td colspan="1" rowspan="1">0.87</td></tr><tr><td colspan="1" rowspan="1">&#8195;Liver disease</td><td colspan="1" rowspan="1">0.82</td><td colspan="1" rowspan="1">0.97</td><td colspan="1" rowspan="1">0.99</td><td colspan="1" rowspan="1">0.62</td><td colspan="1" rowspan="1">0.81</td><td colspan="1" rowspan="1">0.94</td><td colspan="1" rowspan="1">0.99&#8211;0.62</td><td colspan="1" rowspan="1">0.89</td></tr><tr><td colspan="1" rowspan="1">&#8195;Chronic pain</td><td colspan="1" rowspan="1">0.95</td><td colspan="1" rowspan="1">0.97</td><td colspan="1" rowspan="1">0.95</td><td colspan="1" rowspan="1">0.88</td><td colspan="1" rowspan="1">0.94</td><td colspan="1" rowspan="1">0.94</td><td colspan="1" rowspan="1">0.97&#8211;0.88</td><td colspan="1" rowspan="1">0.98<sup>a</sup></td></tr><tr><td colspan="1" rowspan="1">&#8195;Unemployment</td><td colspan="1" rowspan="1">1.00</td><td colspan="1" rowspan="1">0.98</td><td colspan="1" rowspan="1">1.00</td><td colspan="1" rowspan="1">0.88</td><td colspan="1" rowspan="1">0.84</td><td colspan="1" rowspan="1">0.95</td><td colspan="1" rowspan="1">1.00&#8211;0.84</td><td colspan="1" rowspan="1">0.98</td></tr><tr><td colspan="1" rowspan="1">&#8195;Average</td><td colspan="1" rowspan="1">0.91</td><td colspan="1" rowspan="1">0.93</td><td colspan="1" rowspan="1">0.97</td><td colspan="1" rowspan="1">0.78</td><td colspan="1" rowspan="1">0.89</td><td colspan="1" rowspan="1">0.91</td><td colspan="1" rowspan="1">0.97&#8211;0.78</td><td colspan="1" rowspan="1">0.90</td></tr></tbody></table><table-wrap-foot><p><sup>a</sup>Fine-tuned BERT F1 score higher than the trainer model&#8217;s F1 score on the same held-out test set.</p></table-wrap-foot></table-wrap></p><p id="Par15">We used CLEAR to label a dataset to fine-tune a Bio+Clinical BERT for information extraction of the 13 variables in the Stanford MOUD dataset and 5 variables in the CheXpert dataset. Within the 13 Stanford MOUD Dataset classifiers, two showed perfect discrimination on the test set (AUC&#8201;=&#8201;1). The &#8220;suicidal behavior&#8221; classifier had the lowest AUC (0.83). Within the CheXpert Dataset, the &#8220;cardiomegaly&#8221; classifier had the highest AUC (AUC&#8201;=&#8201;1), and the &#8220;pulmonary edema&#8221; classifier had the lowest AUC (AUC&#8201;=&#8201;0.97) (Supplementary Table <xref rid="MOESM1" ref-type="media">7</xref>). Using a predicted probability threshold of 0.5, the fine-tuned BERT model&#8217;s F1 scores were consistently within the range of the larger models&#8217; F1 scores. For alcohol dependence and chronic pain, the fine-tuned BERT model F1 was higher than the trainer model&#8217;s F1 score (Table <xref rid="Tab1" ref-type="table">1</xref>).</p><p id="Par16">Additionally, results from the weak labeling experiments suggest that our CLEAR outperforms weak supervision using regular expressions, which resulted in lower average F1 scores compared to all LLMs used with CLEAR (Supplementary Table <xref rid="MOESM1" ref-type="media">8</xref>).</p></sec><sec id="Sec6"><title>Comparison to chunk embedding and full-note approaches</title><p id="Par17">Across all models, chunk embedding (top-5) and full-note methods performed worse on the information extraction task compared to CLEAR (Supplementary Table <xref rid="MOESM1" ref-type="media">9</xref>). The performance delta was largest for GPT-4 (average F1 0.97 CLEAR vs. 0.88 chunk embedding vs. 0.90 full note) and smallest for Flan-T5 (average F1 0.91 CLEAR vs. 0.88 chunk embedding vs. 0.88 full note) (Fig. <xref rid="Fig2" ref-type="fig">2a</xref>, Supplementary Table <xref rid="MOESM1" ref-type="media">9</xref>). Increasing top-k improves chunk embedding performance, but even with <italic toggle="yes">k</italic>&#8201;=&#8201;10, CLEAR outperformed chunk embedding across all models except Med42, where the chunk embedding approach outperformed CLEAR by 0.01 (F1 0.79 vs. 0.78) (Supplementary Table <xref rid="MOESM1" ref-type="media">10</xref>). We conducted additional experiments by increasing the CLEAR context window size from +/&#8722; 150 words to +/&#8722; 185 words, and reducing token chunks for chunk embeddings from 490 to 390. As a result, the token counts for CLEAR became larger than those for chunk embedding. We re-ran our analysis using Mixtral, and the results showed that the average F1 score for CLEAR increased by 0.01, while the average F1 score for chunk embedding decreased by 0.02 (Supplementary Table <xref rid="MOESM1" ref-type="media">11</xref>), compared to the original results in Supplementary Table <xref rid="MOESM1" ref-type="media">9</xref>.<fig id="Fig2" position="float" orientation="portrait"><label>Fig. 2</label><caption><title>LLM information extraction comparisons on Large Token Stanford MOUD Dataset.</title><p>Average F1 Score comparison between CLEAR and full-note or chunk embedding approach. F1 scores were averaged across our 13 held-out test sets. <italic toggle="yes">P</italic>-values reflect the Wilcoxon Signed-Rank Test on F1 scores across all 13 held-out test sets between CLEAR and full-note or chunk embedding comparisons (<bold>a</bold>). Chunk embedding top-k equals 5 in these experiments. We evaluated average inference time per note (<bold>b</bold>), average model queries per note (<bold>c</bold>), and average input tokens per note (<bold>d</bold>) on the Large Token Stanford MOUD Dataset across full note, chunk embeddings, and CLEAR methods for five models. Chunk embedding top-k equals 5 in these experiments. All metrics are calculated on 4xNVIDIA A100 80GB GPUs. To calculate the total tokens retrieved for GPT-4, we used Med42 as the representative tokenizer. *<italic toggle="yes">p</italic>&#8201;&lt;&#8201;0.05, **<italic toggle="yes">p</italic>&#8201;&lt;&#8201;0.01.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" id="d33e1078" position="float" orientation="portrait" xlink:href="41746_2024_1377_Fig2_HTML.jpg"/></fig></p><p id="Par18">CLEAR outperformed chunk embedding and full-note approaches on nearly all efficiency metrics. Average inference time per note ranged from 1.04&#8201;s (Flan-T5) to 10.24&#8201;s (Med42) for CLEAR; from 4.92&#8201;s (Flan-T5) to 35.07&#8201;s (Med42) for chunk embedding; and from 7.20&#8201;s (Flan-UL2) to 42.43&#8201;s (Med42) for full note. The average number of model calls per note was 1.68 for CLEAR vs. 4.94 for chunk embedding. These numbers were the same across models since all models were called once per retrieved chunk. For the full-note approach, the note was chunked according to the context window of each model. As a result, models with large input token limits&#8212;like GPT-4 (125k) and Mixtral (128k)&#8212;required fewer model calls. The average number of input tokens per note was substantially less in CLEAR compared to chunk embedding and full note. On average, CLEAR had 81% fewer input tokens than the full-note approach and 71% fewer input tokens than chunk embedding (Fig. <xref rid="Fig2" ref-type="fig">2b&#8211;d</xref> and Supplementary Table <xref rid="MOESM1" ref-type="media">12</xref>). We estimated the time required for human evaluators to extract the same information from the training data by recording how long it took our domain expert to annotate 100 notes. On average, it took 57&#8201;s for a human to annotate one variable in a clinical note, which would result in approximately 3299&#8201;h to complete the annotation of 13 variables in 16,031 notes. In comparison, CLEAR would process approximately 1.681 note chunks per note, resulting in 26,948 model calls for this same annotation task. Our fastest model takes an average time of 1.039&#8201;s per note chunk, reducing the task to 101&#8201;h, while the slowest model takes approximately 10.241&#8201;s, totaling 997&#8201;h. This represents a 96.9% efficiency gain with the fastest model and a 69.8% gain with the slowest, compared to human annotation.</p><p id="Par19">We calculated ROUGE-L F-measures to test whether chunk embedding performed worse at information extraction when the retrieved text overlapped less with the text retrieved by CLEAR. When both CLEAR and chunk embedding succeeded (true positives and true negatives), the average ROUGE-L was 77%. When CLEAR succeeded, but chunk embedding failed (false positives and false negatives), ROUGE-L was also 77%, suggesting that performance differences cannot be attributed to lack of overlap in the retrieved text (<italic toggle="yes">p</italic>-value&#8201;&gt;&#8201;0.05). However, the average top-k ranks for TPs and TNs with the highest F-measure (TPs&#8201;=&#8201;3.12, TNs&#8201;=&#8201;4.08) were more favorable than those for FPs and FNs (FPs&#8201;=&#8201;4.11, FNs&#8201;=&#8201;5) (<italic toggle="yes">p</italic>-value&#8201;=&#8201;0.01), indicating that the embedding similarity measure used by the chunk embedding method may not effectively prioritize the most relevant chunks (Supplementary Table <xref rid="MOESM1" ref-type="media">13</xref>). Overall, while both CLEAR and chunk embedding methods retrieve similarly high-yield content, CLEAR proves to be a more efficient information retrieval tool, returning relevant content in fewer chunks (4.94 average chunk embedding chunks per note vs. 1.681 average CLEAR embedding chunks per note) (Supplementary Table <xref rid="MOESM1" ref-type="media">12</xref>).</p></sec></sec><sec id="Sec7" sec-type="discussion"><title>Discussion</title><p id="Par20">In this paper, we propose CLEAR, a RAG pipeline that retrieves note excerpts containing clinical-named entities relevant to the input query. We show that CLEAR, when used for extraction of 13 variables from clinical notes, outperformed chunk embedding and full-note approaches, achieving 3% higher F1 on average with 71% fewer input tokens, 72% faster inference time, and 66% fewer model queries. We also demonstrated that CLEAR outputs can be used to fine-tune BERT-sized models for variable extraction, resulting in performance comparable to larger models.</p><p id="Par21">Our analysis suggests that CLEAR outperforms chunk embedding and full-note approaches for two main reasons. First, CLEAR retrieves shorter context segments. Prior studies have shown that longer contexts can degrade LLM performance. For example, in the FlenQA dataset, which involves three reasoning tasks, Levy et al. observed that as input length increases, model performance deteriorates regardless of whether the key information is located at the beginning, middle, or end of the input context, and that degradation occurs well before reaching the context limit of the models<sup><xref ref-type="bibr" rid="CR42">42</xref></sup>. Similarly, Liu et al. report the &#8220;lost in the middle&#8221; phenomenon, where LLMs perform worse when key information is buried in the middle of the input context compared to being at the beginning or end<sup><xref ref-type="bibr" rid="CR52">52</xref></sup>. They also noted that models with longer context capabilities, such as the 16k versions of GPT-3.5, did not outperform shorter context models. In our own analysis, models like Mixtral, Llama, and GPT-4, despite having context windows large enough to accommodate multiple notes, did not perform as well as CLEAR when processing the full note.</p><p id="Par22">Second, we noted that the embedding model tends to rank chunks differently than CLEAR, often downranking critical chunks. This observation is consistent with our findings that chunk embedding performance improves as the number of chunks retrieved increases from 3 to 5 to 10. Note that we processed each chunk in separate model calls rather than within a single large context. Prior research supports the idea that retrieval of most similar document chunks is not always optimal. For instance, Gan et al. propose METRAG, which combines a similarity model with a utility model for retrieval, finding that their approach outperforms traditional similarity-based RAG approaches across various QA datasets.</p><p id="Par23">CLEAR&#8217;s use of NER aligns with a robust precedent in RAG methodologies. A recent review of RAG approaches included 16 studies that incorporate entity recognition and entity-based reasoning in different ways for RAG<sup><xref ref-type="bibr" rid="CR53">53</xref></sup>. For instance, NER can be employed to edit or revise generated content. In CBRKBQA, NER aids in revising results by aligning generated relations with those in the local neighborhood of the query entity within a knowledge graph<sup><xref ref-type="bibr" rid="CR54">54</xref></sup>. Similarly, GMT-KBQA re-ranks retrieved entities and relations and conducts relation classification and entity disambiguation prior to generation<sup><xref ref-type="bibr" rid="CR55">55</xref></sup>. Beyond content revision, several approaches use entities to extract information directly from knowledge graphs. For example, FC-KBQA, StructGPT, and KAPING retrieve relevant triplets and facts based on entity matching<sup><xref ref-type="bibr" rid="CR49">49</xref>,<xref ref-type="bibr" rid="CR56">56</xref>,<xref ref-type="bibr" rid="CR57">57</xref></sup>. Xu et al. search across entities to identify relevant subgraphs in knowledge graphs for customer support issues<sup><xref ref-type="bibr" rid="CR58">58</xref></sup>, and KnowledgeNavigator leverages NER for iterative filtering of relations to retrieve pertinent triplets from knowledge graphs<sup><xref ref-type="bibr" rid="CR59">59</xref></sup>. Furthermore, RHO integrates entity embedding with knowledge graph embeddings to enhance dialog generation<sup><xref ref-type="bibr" rid="CR60">60</xref></sup>. These methodologies underscore the versatility of NER in RAG, not only for retrieving information but also for structuring and refining content generation. NER can also be used to facilitate automated knowledge graph generation, suggesting that CLEAR could be used to both generate knowledge graphs<sup><xref ref-type="bibr" rid="CR61">61</xref></sup> and retrieve from them to improve LLM performance<sup><xref ref-type="bibr" rid="CR47">47</xref>&#8211;<xref ref-type="bibr" rid="CR49">49</xref></sup>.</p><p id="Par24">Our study faces certain limitations. First, we restricted our evaluation to the task of clinical variable extraction. Future research should explore the performance of CLEAR on other tasks that can benefit from retrieval, including summarization, question answering, and clinical reasoning. Utilizing benchmarks like MedAlign<sup><xref ref-type="bibr" rid="CR62">62</xref></sup> can provide a more comprehensive evaluation of CLEAR&#8217;s capabilities across a broader range of tasks. Second, in our chunk embedding comparison, we segmented chunks based on the context window of the embedding model. While this approach is consistent with prior methods<sup><xref ref-type="bibr" rid="CR3">3</xref></sup>, it is possible that using different-sized embedding chunks could yield similar accuracy to CLEAR. However, in our experiment, where we increased the CLEAR token size and decreased the chunk embedding token size, CLEAR still outperformed chunk embedding (see Supplementary Table <xref rid="MOESM1" ref-type="media">11</xref>). These findings are consistent with the data in Supplementary Table <xref rid="MOESM1" ref-type="media">13</xref>, where CLEAR and chunk embedding methods do not retrieve substantially different information, and increasing CLEAR&#8217;s context size does not negatively impact CLEAR&#8217;s performance. Instead, chunk embedding underperforms because the embedding similarity measures may not effectively prioritize the most relevant chunks. We believe further exploration is warranted, although a deep dive was beyond the scope of this paper. Future experiments should investigate the impact of chunk size tuning on performance. Third, our task required the information retrieval LLM process only one note chunk at a time. This can be adapted if the task requires extracting information from multiple note types. Several CLEAR note chunks from different notes can be combined into a single prompt for LLM inference, however, this was not explored in our paper. Fourth, additional prompt tuning for CLEAR steps (NER, LLM augmentation, and entity selection) is needed for full optimization, and language could have been made more consistent between the prompts used at different stages of the pipeline. Fifth, changes in data over time are inherent in medical studies. The data split we selected resulted in a higher proportion of COVID and post-COVID era notes in the Stanford MOUD testing dataset, which may contain a higher proportion of notes reflecting worsened mental health among patients<sup><xref ref-type="bibr" rid="CR63">63</xref></sup>. Although we made efforts to check for imbalances in our training and testing datasets, these inherent differences may still exist and could impact our evaluation. Lastly, our analysis did not incorporate model quantization methods for LLM inference. Implementing model quantization could strike a balance between efficiency and performance, making it a valuable area for future research. By optimizing model configurations through quantization, we can enhance scalability and applicability in diverse contexts without compromising on performance, thereby providing more comprehensive insights into the optimal use of CLEAR in clinical information extraction.</p><p id="Par25">Traditional methods of using LLM for clinical information extraction are time-consuming and cost-prohibitive. Our work introduces a more efficient RAG pipeline that identifies relevant note chunks using clinical NER before performing variable extraction, leading to a more than 70% reduction in both token usage and processing time. Importantly, these efficiencies were achieved with a slight gain in performance when compared to approaches that utilize entire documents or embed note chunks for retrieval. This work demonstrates that the application of LLMs in healthcare can be made more affordable and practical. We have validated this method in the context of variable extraction, showing its potential to transform the landscape of clinical information processing in healthcare settings.</p></sec><sec id="Sec8"><title>Methods</title><sec id="Sec9"><title>Data source</title><p id="Par26">We used data from two EHR-derived datasets from Stanford Hospital. The first was the Stanford Medication for Opioid Use Disorder (MOUD) cohort<sup><xref ref-type="bibr" rid="CR6">6</xref>,<xref ref-type="bibr" rid="CR64">64</xref></sup>. This cohort includes data from patients treated for opioid use disorder at Stanford Hospital between 2009 and 2023. Patients aged 18&#8211;89 who were prescribed buprenorphine-naloxone for more than a day were included. The cohort was split into a training and testing dataset by treatment start dates, using data up to 2020 for training and from 2021 onwards for testing. The treatment start date was used to split the data to simulate data cut-offs that would be expected in a real-world deployment of CLEAR. To evaluate the similarity between the training and testing data after the date split, we analyzed the proportion of key concept mentions (unemployment, homelessness, food insecurity, substance dependence, suicidal ideation, depression, overdose) in both sets. Using a regular expression search with synonyms for each variable (Supplementary Table <xref rid="MOESM1" ref-type="media">14</xref>), we found minor differences in concept mentions across the datasets (Supplementary Table <xref rid="MOESM1" ref-type="media">15</xref>). There were 767 patients in the training dataset with 16031 unique notes and 505 patients in the testing dataset with 12319 unique notes. Combined, the testing and training datasets had a min, median, and max token lengths of 218, 1778, and 10,981, respectively. Thirteen variables were selected for manual annotation by a board-certified addiction medicine physician due to their importance in delivering medication-assisted therapy. These variables included clinical diagnoses (depression, alcohol dependence, substance use disorder, ADHD, bipolar disorder, chronic pain, liver disease, personality disorder, PTSD, suicidal behavior, tobacco dependence) and social determinants of health (housing and employment status. All data were de-identified using the Safe Harbor method according to NIST guidelines, with clinical text undergoing additional anonymization via the TiDE algorithm<sup><xref ref-type="bibr" rid="CR65">65</xref></sup>. Approval for the study was obtained from the Stanford University Institutional Review Board, protocol number 67423. This study was an analysis of routinely collected EHR data, and posed no additional risk to patients.</p><p id="Par27">The second data source was CheXpert, a dataset of radiology reports from Stanford Hospital<sup><xref ref-type="bibr" rid="CR66">66</xref></sup> with programmatic labels for five well-defined clinical entities commonly found in chest x-ray reports: cardiomegaly, pulmonary edema, pleural effusion, pneumonia, and pneumothorax. These five were selected at random out of the 14 labeled observations in CheXpert. We downsampled the CheXpert dataset into a testing and training dataset by using the existing CheXpert agent&#8217;s labels to randomly sample from the larger CheXpert dataset. For the training dataset, we randomly sampled 700 notes for each of the five selected clinical entities. We only sampled notes that the CheXpert agent labeled as &#8220;present&#8221; or &#8220;negated&#8221; to upsample notes with information relevant to our retrieval task. We used a similar approach for the testing dataset, sampling 200 notes per entity, 100 of which had been labeled by CheXpert as &#8220;present&#8221; and 100 as &#8220;negated&#8221;. After selection, CheXpert labels were discarded for both datasets. In total, we had 3500 patients containing 3500 unique notes in the testing dataset, and 1000 patients containing 1000 unique notes in the training dataset. Combined, the testing and training datasets had a min, median, and max token length of 41, 189, and 1025, respectively.</p><p id="Par28">To prevent data leakage, we removed testing notes for any patients whose IDs were present in the training data. This step ensured that no patient appeared in both the testing and training datasets for the Stanford MOUD and CheXpert tasks.</p></sec><sec id="Sec10"><title>Data annotation</title><p id="Par29">Five board-certified physicians and one medical student collaboratively performed manual annotation of clinical variables to obtain reference labels. We randomly sampled 420 unique notes from the Stanford MOUD testing dataset to generate reference labels for 13 clinical entities outlined in Supplementary Table <xref rid="MOESM1" ref-type="media">16</xref>. To reduce class imbalance skewed towards negative and absent cases, we filtered the 420 unique notes using patient-level structured data (ICD-10 codes) and a regular expression search, returning notes containing any of the specified strings or from patients with at least one relevant ICD-10 code (Supplementary Table <xref rid="MOESM1" ref-type="media">17</xref>). Ultimately, we created 13 individual annotation datasets. The labels generated from these 13 datasets were used as our held-out test sets. 247 notes (20 from each dataset) were randomly selected for duplicate annotation to calculate inter-rater reliability (IRR). For the CheXpert test set, we generate reference labels for all 1000 notes outlined in Supplementary Table <xref rid="MOESM1" ref-type="media">1</xref>. 100 notes from this subset were randomly selected for duplicate annotation to calculate inter-rater reliability. For the Stanford MOUD and CheXpert datasets, labelers received specific instructions that outlined the criteria for annotating each variable. Their task involved identifying and labeling notes for the presence, absence, or uncertainty of a variable.</p><p id="Par30">For each annotated variable, annotators received instructions to improve consistency. Instructions for the Stanford MOUD Dataset annotation task can be found in Supplementary Table <xref rid="MOESM1" ref-type="media">18</xref>. Annotators labeled positive mentions of a variable as present. Negation or absent mentions were both treated similarly. Ambiguous instances were marked as uncertain. 12 notes in the Stanford MOUD Dataset and 8 notes in CheXpert had conflicting duplicate-annotated labels for IRR. These notes were excluded from the held-out test sets.</p><p id="Par31">To evaluate the sensitivity of our information retrieval pipeline, one medical student annotated a specialized dataset known as the Stanford MOUD NER Dataset. This dataset was created by randomly selecting 215 zero-shot NER input texts from the Stanford MOUD Dataset and manually extracting clinically relevant entities and concepts. Instructions for the annotation task can be found in Supplementary Table <xref rid="MOESM1" ref-type="media">19</xref>. We used this to evaluate the sensitivity of our information retrieval pipeline on real-world clinical datasets. After annotation, we had 450 unique clinical entities and concepts for our evaluation. Details outlining the creation of the zero-shot NER input texts can be found below.</p><p id="Par32">Full details on all datasets used in this study can be found in Supplementary Fig. <xref rid="MOESM1" ref-type="media">1</xref>.</p></sec><sec id="Sec11"><title>Clinical entity augmented retrieval</title><p id="Par33">CLEAR uses NER to improve the accuracy and efficiency of clinical LLM tasks. CLEAR takes in two inputs: clinical notes and entities of interest. The pipeline begins with NER to identify all clinical entities within the notes. Next, the identified entities are filtered down to those relevant to the entities of interest. The filtered list is then augmented using ontologies and LLMs to increase sensitivity. The augmented list is fed to a target matcher that retrieves a context window surrounding each relevant entity. The retrieved-context windows can be used for downstream tasks like summarization, question answering, or information extraction. This multi-step approach is outlined below and in Fig. <xref rid="Fig3" ref-type="fig">3</xref>.<fig id="Fig3" position="float" orientation="portrait"><label>Fig. 3</label><caption><title>Overview of CLEAR pipeline.</title><p>CLEAR requires two inputs: (1) clinical notes and (2) a target entity. Initially, our CLEAR implementation applies an NER model to the clinical notes to extract a dataset of relevant entities. These entities are then filtered using word embeddings and cosine similarity to ensure relevance to the target entity. Next, additional entities related to the target entity are identified using ontologies and LLMs. The final list of entities is used to retrieve note chunks through regular expression matches. These chunks support a downstream LLM task (clinical information extraction).</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" id="d33e1260" position="float" orientation="portrait" xlink:href="41746_2024_1377_Fig3_HTML.jpg"/></fig></p><p id="Par34">The first step in the CLEAR pipeline is identifying all clinical entities in the input clinical notes using a NER model. The output of this step is a list of unique clinical entities contained in the notes. We implemented the initial NER step using zero-shot NER with Flan-T5-XXL due to its specialized NER instruction tuning<sup><xref ref-type="bibr" rid="CR67">67</xref></sup>. Most clinical NER models are domain-specific and are highly dependent on the dataset they were fine-tuned on. To minimize these limitations, we chose Flan-T5, a domain-agnostic model, for NER. The model was run on a PHI-compliant virtual machine with 8xNVIDIA L4 24GB GPUs. Given the 512 context window limit of Flan-T5, we used prompts of fewer than 20 tokens and chunked input text to under 100 tokens with a 15-token stride. We used four distinct prompts, and the output-named entities from each prompt were aggregated and de-duplicated to form the final list of entities. Illustrations of each prompt type are provided in Supplementary Fig. <xref rid="MOESM1" ref-type="media">2</xref>. Our approach leverages NER prompts to capture all clinically named entities; however, users have the ability to craft more focused NER prompts (e.g., &#8220;Return all named entities related to congestive heart failure&#8221;).</p><p id="Par35">To evaluate the performance of our zero-shot NER approach, we used two datasets: (1) the NCBI Disease Dataset, which contains annotations for 1382 unique disease names and concepts<sup><xref ref-type="bibr" rid="CR68">68</xref></sup>, (2) the Stanford MOUD NER Dataset, which contains annotations for 450 clinical entities and concepts. We report the sensitivity of Flan-T5-XXL in identifying these entities. Additionally, we characterize the false negatives into the following categories:<list list-type="order"><list-item><p id="Par36"><bold>Acronym recognition failures</bold>: the model recognized either the full term or an acronym for a concept, but not both (ex: &#8220;colorectal cancer&#8221; was identified, but &#8220;crc&#8221; was missed).</p></list-item><list-item><p id="Par37"><bold>Morphological variance failures</bold>: the model recognized either the pleural or singular noun version of the concept, but not both (ex: &#8220;glioblastoma&#8221; was identified, but &#8220;glioblastomas&#8221; was missed).</p></list-item><list-item><p id="Par38"><bold>Partial failures</bold>: model failed to recognize the same concept in different contexts (ex: &#8220;tay sachs disease&#8221;, &#8220;tay sachs mutation&#8221;, &#8220;ashkenazi tay sachs disease&#8221;, &#8220;tay sachs disease gene&#8221; were identified, but &#8220;tay sachs&#8221; was missed).</p></list-item><list-item><p id="Par39"><bold>Other failures</bold>: zero-shot NER failures that do not fall into any of the other categories (ex: &#8220;retinitis punctata albescens&#8221; was missed).</p></list-item></list></p><p id="Par40">We also investigated the impact of removing the NER step on the overall performance of CLEAR. On all 13 variables in the Stanford MOUD Dataset, we ran the CLEAR pipeline with and without the initial NER step. When running CLEAR without NER, the only entities selected are those identified during entity augmentation (described below) with ontologies and LLMs. The selected entities were then used to retrieve sections of the note that were passed to GPT-4 to extract information about a variable of interest. We report the average F1 of the information extraction task.</p><p id="Par41">Once the unique clinical entities from all notes are identified, the entities relevant to the input target entity are selected. These selected entities are eventually used to retrieve relevant context windows for downstream LLM tasks. First, all entities identified via NER and the target entity were embedded using Bio+Clinical BERT<sup><xref ref-type="bibr" rid="CR69">69</xref></sup>, and those entities with a cosine similarity &#8805;0.85 compared to the target entity were retained. We selected a cosine similarity threshold of 0.85 after empirical testing attempting to balance the exclusion of irrelevant entities with the retention of relevant ones. The resulting entities were passed to GPT-4 with a prompt to filter the list to those most relevant to the target entity (Supplementary Fig. <xref rid="MOESM1" ref-type="media">3</xref>). The entity filtering step is modular, allowing users to apply Bio+Clinical BERT cosine similarity, an LLM, a human, or any combination to improve entity selection.</p><p id="Par42">Next, the filtered entity list is augmented to account for entities missed during NER. Without an augmentation step, entities missed during NER could lead to incorrect context retrieval downstream. For example, if the entity &#8220;lesch nyhan&#8221; was missed by NER, and the target entity is &#8220;Lesch-Nyhan syndrome&#8221;, the downstream information retrieval might fail to retrieve sections of the note that mention &#8220;lesch nyhan&#8221;. Here, we used the UMLS ontology<sup><xref ref-type="bibr" rid="CR70">70</xref></sup> and GPT-4 to augment the list of entities from NER. We used the search endpoint from the UMLS API to retrieve concept names related to the target entity, and retained concept names originating from the National Library of Medicine Metathesaurus or SNOMED CT<sup><xref ref-type="bibr" rid="CR71">71</xref></sup>. We also prompted GPT-4 to generate synonyms for the target entity (Supplementary Fig. <xref rid="MOESM1" ref-type="media">4</xref>).</p><p id="Par43">To evaluate the impact of the entity augmentation step, we measured to what extent ontology and LLM augmentation recover entities missed in the NER step (false negatives). To evaluate the impact of the entity augmentation step, we measured how well ontology and LLM augmentation recover entities missed in the NER step (false negatives), as minimizing false negatives is crucial for downstream information retrieval. We prioritized maximizing sensitivity/recall, as false positives can be managed by the downstream LLM task, whereas false negatives result in complete loss of information. For each entity missed by the NER step, we treat a variant of the missed entity as the target entity and use the UMLS ontology and GPT-4 to generate synonyms as described above. For example, the formal name of an entity was used (&#8220;Lesch-Nyhan syndrome&#8221;) if a variant was missed (&#8220;lesch nyhan&#8221;). If the formal name was the missed term, a broader term that would encompass the formal name (&#8220;purine salvage deficiencies&#8221;) was used as the target entity. We report the proportion of entities missed during NER that were recovered through this augmentation step.</p><p id="Par44">The selected entities are used to develop a regular expression tool for information retrieval. Specifically, we employed the Target Matcher provided by MedSpaCy<sup><xref ref-type="bibr" rid="CR72">72</xref></sup>. We used the TargetRule class from the MedSpaCy NER module for identifying mentions of the selected entities within clinical notes and then pulled a context window of 150 words before and after the target entity. These retrieved-context windows are passed to an LLM for downstream inference tasks.</p></sec><sec id="Sec12"><title>Information extraction</title><p id="Par45">We used the retrieved-context windows from CLEAR to extract the information (ex: is the feature present, negated/absent, or uncertain) of 13 variables in the Stanford MOUD dataset and 5 variables in the CheXpert dataset. We compared the performance of several models with different context windows on this task. These models included: Med42&#8211;70b<sup><xref ref-type="bibr" rid="CR73">73</xref></sup>, Mixtral-8x7B-Instruct-v0.1<sup><xref ref-type="bibr" rid="CR74">74</xref></sup>, Llama-3&#8211;70b<sup><xref ref-type="bibr" rid="CR75">75</xref></sup>, Flan-T5-XXL<sup><xref ref-type="bibr" rid="CR67">67</xref></sup>, Flan-UL2<sup><xref ref-type="bibr" rid="CR76">76</xref></sup>, and GPT-4<sup><xref ref-type="bibr" rid="CR77">77</xref></sup>. We ran GPT-4 via a secure Azure PHI-compliant instance. The other five models were run on a PHI-compliant virtual machine with 4xNVIDIA A100 80GB GPUs.</p><p id="Par46">We designed prompts that included synthetic in-context examples generated by GPT-4<sup><xref ref-type="bibr" rid="CR78">78</xref></sup>. We included five examples for each entity, covering all possible labels that the LLM was to discern: &#8220;0&#8221; for entity negated/absence, &#8220;1&#8221; for presence, and &#8220;2&#8221; for uncertainty. We selected 5-shot prompting based on its demonstrated performance gains in prior work<sup><xref ref-type="bibr" rid="CR36">36</xref></sup>. To mitigate any potential issues with the context window limitations of each model, we kept the synthetic data points under 100 tokens. We kept each example under 100 tokens to ensure they provide meaningful insights into the task, improving LLM instruction following without consuming excessive tokens. In comparing our methods to the traditional approach, also known as the full-note method, we accounted for the increase in token length due to our prompting strategies, ensuring the models&#8217; token limits were not exceeded. An example of our LLM information extraction prompt is provided in Supplementary Fig. <xref rid="MOESM1" ref-type="media">5</xref>.</p><p id="Par47">For each model, we report classification metrics (sensitivity, specificity, NPV, PPV, F1) comparing the LLM information extraction labels to human-annotated labels.</p></sec><sec id="Sec13"><title>Weak labeling</title><p id="Par48">We compared CLEAR to a weak labeling approach. Weak supervision was used to label the 2000 CheXpert notes in the training set for our five CheXpert entities of interest. Labeling functions are rough heuristics used to programmatically generate weak labels from unlabeled data. We manually reviewed the 250 notes in the training set and created labeling functions using keyword matching and regular expressions from a list of synonyms created by a domain expert and supplemented by GPT-4 using the prompt in Supplementary Fig. <xref rid="MOESM1" ref-type="media">4</xref>. The final list of synonyms can be found in Supplementary Table <xref rid="MOESM1" ref-type="media">20</xref>. For example, a labeling function might label a note as having cardiomegaly if it contains the strings &#8220;enlarged cardiac silhouette,&#8221; &#8220;enlarged heart,&#8221; or &#8220;ventricular hypertrophy,&#8221; and abstain otherwise. The labeling functions for each entity were used to train a model that combines the outputs of multiple labeling functions for a given entity, leveraging their collective knowledge and handling their conflicts<sup><xref ref-type="bibr" rid="CR79">79</xref></sup>.</p></sec><sec id="Sec14"><title>Model distillation</title><p id="Par49">We investigated whether the output of CLEAR could be used to fine-tune a smaller language model to perform the information extraction task. We used the output of CLEAR to fine-tune BERT models to perform a binary classification task (present vs. negated/absent) for each of the variables in the Stanford MOUD and CheXpert datasets. We omitted the &#8220;uncertain&#8221; class due to small sample sizes in certain fine-tuning datasets (Supplementary Table <xref rid="MOESM1" ref-type="media">21</xref>). We fine-tuned Bio+Clinical BERT, which was initialized from BioBERT and trained on all MIMIC notes<sup><xref ref-type="bibr" rid="CR69">69</xref></sup>. For each variable, we selected the best performing LLM, excluding GPT-4 on the information extraction task to weakly label the fine-tuning dataset (Supplementary Table <xref rid="MOESM1" ref-type="media">21</xref>). We excluded GPT-4 since OpenAI terms of use prohibit using GPT-4 outputs to develop competitor models<sup><xref ref-type="bibr" rid="CR80">80</xref></sup>. The fine-tuning dataset for each variable consisted of every note chunk containing an entity of interest (inputs) and label generated by an LLM (label). We removed note chunks that contained &gt;10% overlapping words, resulting in less than 2% of note chunks being filtered out.</p><p id="Par50">The fine-tuning datasets for each variable were divided into a 70% training set and a 30% validation set. Hyperparameters were tuned using 10-fold cross-validation on 70% of the training data to maximize the area under the receiver operating curve (AUC). We selected a range of variables for each hyperparameter and performed a grid search to find the best hyperparameter configurations. Our grid search included learning rate (5e-5, 3e-5, 2e-5), batch size (8, 16, 32), number of training epochs (4, 5, 10), and weight decay (0.01, 0.05, 0.1). The final models were fine-tuned on 100% of the fine-tuning dataset. Performance metrics for the fine-tuned classifier were generated using the held-out test sets. To prevent data leakage, we removed testing notes for any patients whose IDs were present in the fine-tuning data (training and validation datasets). This ensured that no patient appeared in both the held-out test set and fine-tuning datasets for the Stanford and CheXpert variables.</p></sec><sec id="Sec15"><title>Comparison to chunk embedding and full-note approaches</title><p id="Par51">To quantify the impact of retrieving text around entities, we compared CLEAR to a RAG pipeline leveraging note chunk embeddings and a naive approach that retrieves the full note. We filtered the test sets down to longer notes to focus this comparison on notes that approached or exceeded models&#8217; input context window. Specifically, we select the 50% longest notes in the Stanford MOUD Dataset.</p><p id="Par52">For the chunk embedding RAG pipeline, we used the BAAI Generalized Embeddings (BGE) model as our embedding model and cosine similarity as the retriever (Supplementary Fig. <xref rid="MOESM1" ref-type="media">6</xref>). BGE is a high-performance embedding model known for its accuracy on retrieval benchmarks<sup><xref ref-type="bibr" rid="CR81">81</xref></sup>. We first segmented all patient notes into chunks of 490 tokens with a stride of 128 tokens, given BGE&#8217;s maximum context window of 512. We then generated embeddings for each chunk as well as every target entity and its definition and stored them in an embedding database. To select the most relevant note chunks for our information extraction task, we perform cosine similarity to measure the alignment of each note chunk against the target entity&#8217;s definition embedding. We retrieved the top-<italic toggle="yes">k</italic> (where <italic toggle="yes">k</italic>&#8201;=&#8201;3, 5, 10) note chunks based on cosine similarity scores. For notes with fewer than k chunks, we retrieved all chunks. The retrieved chunks were passed to an LLM for the information extraction task. For notes with multiple chunks, we aggregated the LLM labels from these chunks to generate a final label for the note.</p><p id="Par53">For the full-note approach, we chunked notes based on each model&#8217;s context window limit, using a stride of 128 tokens<sup><xref ref-type="bibr" rid="CR4">4</xref></sup> (Supplementary Fig. <xref rid="MOESM1" ref-type="media">7</xref>). We passed each chunk to an LLM for the information extraction task and aggregated the labels from these segments to produce a final label for each note.</p><p id="Par54">For all LLMs, we compare the performance of CLEAR, chunk embeddings, and the full-note approach on the information extraction tasks as well as on three metrics related to inference efficiency. For the information extraction tasks, we report the average inference time per note, average model queries per note, and average tokens retrieved per note. We do not report inference time for GPT-4 since it was run using a proprietary API. We used the Wilcoxon Signed-Rank Test to compare the differences between the three methods<sup><xref ref-type="bibr" rid="CR82">82</xref></sup>.</p><p id="Par55">We tested two hypotheses regarding performance differences between CLEAR and chunk embedding. First, we hypothesized that chunk embeddings would perform worse than CLEAR when the retrieved chunks overlapped less with the chunks retrieved by CLEAR. To test this, we calculated ROUGE-L F-measure&#8212;a measure of the longest common subsequence between two strings&#8212;on the chunks retrieved by CLEAR and chunk embedding, treating the CLEAR chunks as the reference. We report ROUGE-L for cases where both CLEAR and chunk embeddings succeeded (true negatives or true positives), and for cases where CLEAR succeeded but chunk embeddings failed (false negatives and false positives). Second, we hypothesized that chunk embeddings would perform worse than CLEAR when the parts of the note retrieved by CLEAR were ranked lower by the chunk embedding model. To do this, we calculated the chunk embedding model ranking of the chunk that overlapped most with the CLEAR chunk (as measured by ROUGE-L).</p></sec><sec id="Sec16"><title>Model summary</title><p id="Par56">For NER, we relied on Flan-T5-XXL. LLM Augmentation used GPT-4. Our entity selection cosine similarity model was Bio+ClinicalBERT, and the entity filtering LLM was GPT-4. Information extraction was tested on six models: Med42&#8211;70b, Mixtral-8x7B-Instruct-v0.1, Llama-3&#8211;70b, Flan-T5-XXL, Flan-UL2, and GPT-4. The chunk embedding model was BAAI Generalized Embeddings Large English v1.5, and for model distillation, we fine-tuned Bio+ClinicalBERT (Supplementary Table <xref rid="MOESM1" ref-type="media">22</xref>). Model usage parameters for NER and information extraction are reported in Supplementary Table <xref rid="MOESM1" ref-type="media">23</xref>.</p></sec></sec><sec id="Sec17" sec-type="supplementary-material"><title>Supplementary information</title><p>
<supplementary-material content-type="local-data" id="MOESM1" position="float" orientation="portrait"><media xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="41746_2024_1377_MOESM1_ESM.pdf" position="float" orientation="portrait"><caption><p>Supplemental figures and tables</p></caption></media></supplementary-material>
</p></sec></body><back><fn-group><fn><p><bold>Publisher&#8217;s note</bold> Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn><fn><p>These authors contributed equally: Ivan Lopez, Akshay Swaminathan.</p></fn></fn-group><sec><title>Supplementary information</title><p>The online version contains supplementary material available at 10.1038/s41746-024-01377-1.</p></sec><ack><title>Acknowledgements</title><p>Dr. Chen has received research funding support in part by the NIH/National Institute of Allergy and Infectious Diseases (1R01AI17812101), NIH/National Institute on Drug Abuse Clinical Trials Network (UG1DA015815-CTN-0136), Gordon and Betty Moore Foundation (Grant #12409), Stanford Artificial Intelligence in Medicine and Imaging&#8212;Human-Centered Artificial Intelligence (AIMI-HAI) Partnership Grant, American Heart Association&#8212;Strategically Focused Research Network&#8212;Diversity in Clinical Trials, NIH-NCATS-CTSA grant (UL1TR003142) for common research resources. The content is solely the responsibility of the authors and does not necessarily represent the official views of the NIH, Stanford Healthcare, or any other organization.</p></ack><notes notes-type="author-contribution"><title>Author contributions</title><p>Conceptualization: I.L., A.S., K.V., F.N.H. Supervision: J.H.C., N.H.S. Writing: I.L., A.S., K.V., S.N. Data acquisition: I.L., F.N.H., J.H.C. Data analysis: I.L. A.S., S.N. Critical review: I.L., A.S., K.V., S.N., F.N.H., S.P.M., A.S.L., S.T., M.M., R.J.G., N.S., J.H.C. All authors read and approved the final manuscript and had final responsibility for the decision to submit it for publication.</p></notes><notes notes-type="data-availability"><title>Data availability</title><p>The Stanford MOUD Cohort Dataset used in this study contains identifiable protected health information and, therefore, cannot be shared publicly. Stanford University investigators with appropriate IRB approval can contact the authors directly regarding data access.</p></notes><notes notes-type="data-availability"><title>Code availability</title><p>The code used to run CLEAR and reproduce results can be found at <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://github.com/iv-lop/clear">https://github.com/iv-lop/clear</ext-link>.</p></notes><notes id="FPar1" notes-type="COI-statement"><title>Competing interests</title><p id="Par57">A.S. owns stock in Roche (RHHVF) and Cerebral Inc. and is an adviser to Daybreak Health and Cerebral Inc. S.N. owns stock in Meta, works at Insitro (an ML for drug discovery company), and owns stock options for Insitro. N.H.S. reported being a co-founder of Prealize Health (a predictive analytics company) and Atropos Health (an on-demand evidence generation company); receiving funding from the Gordon and Betty Moore Foundation for developing virtual model deployments; and serving on the Board of the Coalition for Healthcare AI (CHAI), a consensus-building organization providing guidelines for the responsible use of artificial intelligence in healthcare. J.H.C. reported being a co-founder of Reaction Explorer LLC, develops and licenses organic chemistry education software, paid consulting fees from Sutton Pierce, Younker Hyde MacFarlane, and Sykes McAllister as a medical expert witness, and paid consulting fees from ISHI Health. The remaining authors declare no competing interests.</p></notes><ref-list id="Bib1"><title>References</title><ref id="CR1"><label>1.</label><citation-alternatives><element-citation id="ec-CR1" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ross</surname><given-names>MK</given-names></name><name name-style="western"><surname>Wei</surname><given-names>W</given-names></name><name name-style="western"><surname>Ohno-Machado</surname><given-names>L</given-names></name></person-group><article-title>Big data and the electronic health record</article-title><source>Yearb. Med. Inform.</source><year>2014</year><volume>9</volume><fpage>97</fpage><lpage>104</lpage><pub-id pub-id-type="pmid">25123728</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.15265/IY-2014-0003</pub-id><pub-id pub-id-type="pmcid">PMC4287068</pub-id></element-citation><mixed-citation id="mc-CR1" publication-type="journal">Ross, M. K., Wei, W. &amp; Ohno-Machado, L. Big data and the electronic health record. <italic toggle="yes">Yearb. Med. Inform.</italic><bold>9</bold>, 97&#8211;104 (2014).<pub-id pub-id-type="pmid">25123728</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.15265/IY-2014-0003</pub-id><pub-id pub-id-type="pmcid">PMC4287068</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR2"><label>2.</label><citation-alternatives><element-citation id="ec-CR2" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Meystre</surname><given-names>SM</given-names></name><name name-style="western"><surname>Savova</surname><given-names>GK</given-names></name><name name-style="western"><surname>Kipper-Schuler</surname><given-names>KC</given-names></name><name name-style="western"><surname>Hurdle</surname><given-names>JF</given-names></name></person-group><article-title>Extracting information from textual documents in the electronic health record: a review of recent research.</article-title><source>Yearb. Med. Inform.</source><year>2008</year><volume>17</volume><fpage>128</fpage><lpage>144</lpage><pub-id pub-id-type="doi">10.1055/s-0038-1638592</pub-id><pub-id pub-id-type="pmid">18660887</pub-id></element-citation><mixed-citation id="mc-CR2" publication-type="journal">Meystre, S. M., Savova, G. K., Kipper-Schuler, K. C. &amp; Hurdle, J. F. Extracting information from textual documents in the electronic health record: a review of recent research. <italic toggle="yes">Yearb. Med. Inform.</italic><bold>17</bold>, 128&#8211;144 (2008).<pub-id pub-id-type="pmid">18660887</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR3"><label>3.</label><mixed-citation publication-type="other">Wornow, M. et al. Zero-shot clinical trial patient matching with LLMs. Preprint at 10.48550/arXiv.2402.05125 (2024).</mixed-citation></ref><ref id="CR4"><label>4.</label><citation-alternatives><element-citation id="ec-CR4" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Alsentzer</surname><given-names>E</given-names></name><etal/></person-group><article-title>Zero-shot interpretable phenotyping of postpartum hemorrhage using large language models</article-title><source>Npj Digit. Med.</source><year>2023</year><volume>6</volume><fpage>1</fpage><lpage>10</lpage><pub-id pub-id-type="doi">10.1038/s41746-023-00957-x</pub-id><pub-id pub-id-type="pmid">38036723</pub-id><pub-id pub-id-type="pmcid">PMC10689487</pub-id></element-citation><mixed-citation id="mc-CR4" publication-type="journal">Alsentzer, E. et al. Zero-shot interpretable phenotyping of postpartum hemorrhage using large language models. <italic toggle="yes">Npj Digit. Med.</italic><bold>6</bold>, 1&#8211;10 (2023).<pub-id pub-id-type="pmid">38036723</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1038/s41746-023-00957-x</pub-id><pub-id pub-id-type="pmcid">PMC10689487</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR5"><label>5.</label><citation-alternatives><element-citation id="ec-CR5" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Callahan</surname><given-names>A</given-names></name><name name-style="western"><surname>Shah</surname><given-names>NH</given-names></name><name name-style="western"><surname>Chen</surname><given-names>JH</given-names></name></person-group><article-title>Research and reporting considerations for observational studies using electronic health record data</article-title><source>Ann. Intern. Med.</source><year>2020</year><volume>172</volume><fpage>S79</fpage><lpage>S84</lpage><pub-id pub-id-type="doi">10.7326/M19-0873</pub-id><pub-id pub-id-type="pmid">32479175</pub-id><pub-id pub-id-type="pmcid">PMC7413106</pub-id></element-citation><mixed-citation id="mc-CR5" publication-type="journal">Callahan, A., Shah, N. H. &amp; Chen, J. H. Research and reporting considerations for observational studies using electronic health record data. <italic toggle="yes">Ann. Intern. Med.</italic><bold>172</bold>, S79&#8211;S84 (2020).<pub-id pub-id-type="pmid">32479175</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.7326/M19-0873</pub-id><pub-id pub-id-type="pmcid">PMC7413106</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR6"><label>6.</label><citation-alternatives><element-citation id="ec-CR6" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lopez</surname><given-names>I</given-names></name><etal/></person-group><article-title>Predicting premature discontinuation of medication for opioid use disorder from electronic medical records</article-title><source>Amia. Annu. Symp. Proc.</source><year>2024</year><volume>2023</volume><fpage>1067</fpage><lpage>1076</lpage><pub-id pub-id-type="pmid">38222349</pub-id><pub-id pub-id-type="pmcid">PMC10785878</pub-id></element-citation><mixed-citation id="mc-CR6" publication-type="journal">Lopez, I. et al. Predicting premature discontinuation of medication for opioid use disorder from electronic medical records. <italic toggle="yes">Amia. Annu. Symp. Proc.</italic><bold>2023</bold>, 1067&#8211;1076 (2024).<pub-id pub-id-type="pmid">38222349</pub-id><pub-id pub-id-type="pmcid">PMC10785878</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR7"><label>7.</label><citation-alternatives><element-citation id="ec-CR7" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zweigenbaum</surname><given-names>P</given-names></name><name name-style="western"><surname>Demner-Fushman</surname><given-names>D</given-names></name><name name-style="western"><surname>Yu</surname><given-names>H</given-names></name><name name-style="western"><surname>Cohen</surname><given-names>KB</given-names></name></person-group><article-title>Frontiers of biomedical text mining: current progress</article-title><source>Brief. Bioinform.</source><year>2007</year><volume>8</volume><fpage>358</fpage><lpage>375</lpage><pub-id pub-id-type="doi">10.1093/bib/bbm045</pub-id><pub-id pub-id-type="pmid">17977867</pub-id><pub-id pub-id-type="pmcid">PMC2516302</pub-id></element-citation><mixed-citation id="mc-CR7" publication-type="journal">Zweigenbaum, P., Demner-Fushman, D., Yu, H. &amp; Cohen, K. B. Frontiers of biomedical text mining: current progress. <italic toggle="yes">Brief. Bioinform.</italic><bold>8</bold>, 358&#8211;375 (2007).<pub-id pub-id-type="pmid">17977867</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1093/bib/bbm045</pub-id><pub-id pub-id-type="pmcid">PMC2516302</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR8"><label>8.</label><citation-alternatives><element-citation id="ec-CR8" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>Y</given-names></name><etal/></person-group><article-title>Clinical information extraction applications: a literature review</article-title><source>J. Biomed. Inform.</source><year>2018</year><volume>77</volume><fpage>34</fpage><lpage>49</lpage><pub-id pub-id-type="doi">10.1016/j.jbi.2017.11.011</pub-id><pub-id pub-id-type="pmid">29162496</pub-id><pub-id pub-id-type="pmcid">PMC5771858</pub-id></element-citation><mixed-citation id="mc-CR8" publication-type="journal">Wang, Y. et al. Clinical information extraction applications: a literature review. <italic toggle="yes">J. Biomed. Inform.</italic><bold>77</bold>, 34&#8211;49 (2018).<pub-id pub-id-type="pmid">29162496</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.jbi.2017.11.011</pub-id><pub-id pub-id-type="pmcid">PMC5771858</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR9"><label>9.</label><mixed-citation publication-type="other">Lample, G., Ballesteros, M., Subramanian, S., Kawakami, K. &amp; Dyer, C. Neural Architectures for Named Entity Recognition. in <italic toggle="yes">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</italic> (eds. Knight, K., Nenkova, A. &amp; Rambow, O.) 260&#8211;270 10.18653/v1/N16-1030 (Association for Computational Linguistics, San Diego, California, 2016).</mixed-citation></ref><ref id="CR10"><label>10.</label><mixed-citation publication-type="other">K&#229;geb&#228;ck, M. &amp; Salomonsson, H. Word Sense Disambiguation using a Bidirectional LSTM. in <italic toggle="yes">Proceedings of the 5th Workshop on Cognitive Aspects of the Lexicon (CogALex - V)</italic> (eds. Zock, M., Lenci, A. &amp; Evert, S.) 51&#8211;56 (The COLING 2016 Organizing Committee, Osaka, Japan, 2016).</mixed-citation></ref><ref id="CR11"><label>11.</label><citation-alternatives><element-citation id="ec-CR11" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wu</surname><given-names>H</given-names></name><etal/></person-group><article-title>A survey on clinical natural language processing in the United Kingdom from 2007 to 2022</article-title><source>Npj Digit. Med.</source><year>2022</year><volume>5</volume><fpage>1</fpage><lpage>15</lpage><pub-id pub-id-type="doi">10.1038/s41746-022-00730-6</pub-id><pub-id pub-id-type="pmid">36544046</pub-id><pub-id pub-id-type="pmcid">PMC9770568</pub-id></element-citation><mixed-citation id="mc-CR11" publication-type="journal">Wu, H. et al. A survey on clinical natural language processing in the United Kingdom from 2007 to 2022. <italic toggle="yes">Npj Digit. Med.</italic><bold>5</bold>, 1&#8211;15 (2022).<pub-id pub-id-type="pmid">36544046</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1038/s41746-022-00730-6</pub-id><pub-id pub-id-type="pmcid">PMC9770568</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR12"><label>12.</label><citation-alternatives><element-citation id="ec-CR12" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jung</surname><given-names>K</given-names></name><etal/></person-group><article-title>Functional evaluation of out-of-the-box text-mining tools for data-mining tasks</article-title><source>J. Am. Med. Inform. Assoc. JAMIA</source><year>2015</year><volume>22</volume><fpage>121</fpage><lpage>131</lpage><pub-id pub-id-type="doi">10.1136/amiajnl-2014-002902</pub-id><pub-id pub-id-type="pmid">25336595</pub-id><pub-id pub-id-type="pmcid">PMC4433377</pub-id></element-citation><mixed-citation id="mc-CR12" publication-type="journal">Jung, K. et al. Functional evaluation of out-of-the-box text-mining tools for data-mining tasks. <italic toggle="yes">J. Am. Med. Inform. Assoc. JAMIA</italic><bold>22</bold>, 121&#8211;131 (2015).<pub-id pub-id-type="pmid">25336595</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1136/amiajnl-2014-002902</pub-id><pub-id pub-id-type="pmcid">PMC4433377</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR13"><label>13.</label><citation-alternatives><element-citation id="ec-CR13" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Percha</surname><given-names>B</given-names></name></person-group><article-title>Modern clinical text mining: a guide and review</article-title><source>Annu. Rev. Biomed. Data Sci.</source><year>2021</year><volume>4</volume><fpage>165</fpage><lpage>187</lpage><pub-id pub-id-type="doi">10.1146/annurev-biodatasci-030421-030931</pub-id><pub-id pub-id-type="pmid">34465177</pub-id></element-citation><mixed-citation id="mc-CR13" publication-type="journal">Percha, B. Modern clinical text mining: a guide and review. <italic toggle="yes">Annu. Rev. Biomed. Data Sci.</italic><bold>4</bold>, 165&#8211;187 (2021).<pub-id pub-id-type="pmid">34465177</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1146/annurev-biodatasci-030421-030931</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR14"><label>14.</label><citation-alternatives><element-citation id="ec-CR14" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Agarwal</surname><given-names>AR</given-names></name><name name-style="western"><surname>Prichett</surname><given-names>L</given-names></name><name name-style="western"><surname>Jain</surname><given-names>A</given-names></name><name name-style="western"><surname>Srikumaran</surname><given-names>U</given-names></name></person-group><article-title>Assessment of use of ICD-9 and ICD-10 codes for social determinants of health in the US, 2011-2021</article-title><source>JAMA Netw. Open</source><year>2023</year><volume>6</volume><fpage>e2312538</fpage><pub-id pub-id-type="doi">10.1001/jamanetworkopen.2023.12538</pub-id><pub-id pub-id-type="pmid">37159201</pub-id><pub-id pub-id-type="pmcid">PMC10170331</pub-id></element-citation><mixed-citation id="mc-CR14" publication-type="journal">Agarwal, A. R., Prichett, L., Jain, A. &amp; Srikumaran, U. Assessment of use of ICD-9 and ICD-10 codes for social determinants of health in the US, 2011-2021. <italic toggle="yes">JAMA Netw. Open</italic><bold>6</bold>, e2312538 (2023).<pub-id pub-id-type="pmid">37159201</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1001/jamanetworkopen.2023.12538</pub-id><pub-id pub-id-type="pmcid">PMC10170331</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR15"><label>15.</label><citation-alternatives><element-citation id="ec-CR15" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Truong</surname><given-names>HP</given-names></name><etal/></person-group><article-title>Utilization of social determinants of health ICD-10 Z-codes among hospitalized patients in the United States, 2016&#8211;2017</article-title><source>Med. Care</source><year>2020</year><volume>58</volume><fpage>1037</fpage><pub-id pub-id-type="doi">10.1097/MLR.0000000000001418</pub-id><pub-id pub-id-type="pmid">32925453</pub-id><pub-id pub-id-type="pmcid">PMC7666017</pub-id></element-citation><mixed-citation id="mc-CR15" publication-type="journal">Truong, H. P. et al. Utilization of social determinants of health ICD-10 Z-codes among hospitalized patients in the United States, 2016&#8211;2017. <italic toggle="yes">Med. Care</italic><bold>58</bold>, 1037 (2020).<pub-id pub-id-type="pmid">32925453</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1097/MLR.0000000000001418</pub-id><pub-id pub-id-type="pmcid">PMC7666017</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR16"><label>16.</label><citation-alternatives><element-citation id="ec-CR16" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Swaminathan</surname><given-names>A</given-names></name><etal/></person-group><article-title>Selective prediction for extracting unstructured clinical data</article-title><source>J. Am. Med. Inform. Assoc.</source><year>2024</year><volume>31</volume><fpage>188</fpage><lpage>197</lpage><pub-id pub-id-type="doi">10.1093/jamia/ocad182</pub-id><pub-id pub-id-type="pmcid">PMC10746316</pub-id><pub-id pub-id-type="pmid">37769323</pub-id></element-citation><mixed-citation id="mc-CR16" publication-type="journal">Swaminathan, A. et al. Selective prediction for extracting unstructured clinical data. <italic toggle="yes">J. Am. Med. Inform. Assoc.</italic><bold>31</bold>, 188&#8211;197 (2024).<pub-id pub-id-type="doi" assigning-authority="pmc">10.1093/jamia/ocad182</pub-id><pub-id pub-id-type="pmcid">PMC10746316</pub-id><pub-id pub-id-type="pmid">37769323</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR17"><label>17.</label><citation-alternatives><element-citation id="ec-CR17" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>C</given-names></name><etal/></person-group><article-title>Predictive value of clinical complete response after chemoradiation for rectal cancer</article-title><source>J. Am. Coll. Surg.</source><year>2022</year><volume>235</volume><fpage>S51</fpage><pub-id pub-id-type="doi">10.1097/01.XCS.0000893308.54894.46</pub-id></element-citation><mixed-citation id="mc-CR17" publication-type="journal">Liu, C. et al. Predictive value of clinical complete response after chemoradiation for rectal cancer. <italic toggle="yes">J. Am. Coll. Surg.</italic><bold>235</bold>, S51 (2022).</mixed-citation></citation-alternatives></ref><ref id="CR18"><label>18.</label><citation-alternatives><element-citation id="ec-CR18" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Liao</surname><given-names>KP</given-names></name><etal/></person-group><article-title>Development of phenotype algorithms using electronic medical records and incorporating natural language processing</article-title><source>BMJ</source><year>2015</year><volume>350</volume><fpage>h1885</fpage><pub-id pub-id-type="doi">10.1136/bmj.h1885</pub-id><pub-id pub-id-type="pmid">25911572</pub-id><pub-id pub-id-type="pmcid">PMC4707569</pub-id></element-citation><mixed-citation id="mc-CR18" publication-type="journal">Liao, K. P. et al. Development of phenotype algorithms using electronic medical records and incorporating natural language processing. <italic toggle="yes">BMJ</italic><bold>350</bold>, h1885 (2015).<pub-id pub-id-type="pmid">25911572</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1136/bmj.h1885</pub-id><pub-id pub-id-type="pmcid">PMC4707569</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR19"><label>19.</label><mixed-citation publication-type="other">Huang, Z., Xu, W. &amp; Yu, K. Bidirectional LSTM-CRF models for sequence tagging. Preprint at <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="http://arxiv.org/abs/1508.01991">http://arxiv.org/abs/1508.01991</ext-link> (2015).</mixed-citation></ref><ref id="CR20"><label>20.</label><citation-alternatives><element-citation id="ec-CR20" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>Y</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Y</given-names></name><name name-style="western"><surname>Qi</surname><given-names>P</given-names></name><name name-style="western"><surname>Manning</surname><given-names>CD</given-names></name><name name-style="western"><surname>Langlotz</surname><given-names>CP</given-names></name></person-group><article-title>Biomedical and clinical English model packages for the Stanza Python NLP library</article-title><source>J. Am. Med. Inform. Assoc. JAMIA</source><year>2021</year><volume>28</volume><fpage>1892</fpage><lpage>1899</lpage><pub-id pub-id-type="doi">10.1093/jamia/ocab090</pub-id><pub-id pub-id-type="pmid">34157094</pub-id><pub-id pub-id-type="pmcid">PMC8363782</pub-id></element-citation><mixed-citation id="mc-CR20" publication-type="journal">Zhang, Y., Zhang, Y., Qi, P., Manning, C. D. &amp; Langlotz, C. P. Biomedical and clinical English model packages for the Stanza Python NLP library. <italic toggle="yes">J. Am. Med. Inform. Assoc. JAMIA</italic><bold>28</bold>, 1892&#8211;1899 (2021).<pub-id pub-id-type="pmid">34157094</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1093/jamia/ocab090</pub-id><pub-id pub-id-type="pmcid">PMC8363782</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR21"><label>21.</label><mixed-citation publication-type="other">Qi, P., Zhang, Y., Zhang, Y., Bolton, J. &amp; Manning, C. D. Stanza: a Python natural language processing toolkit for many human languages. in <italic toggle="yes">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations</italic> (eds. Celikyilmaz, A. &amp; Wen, T.-H.) 101&#8211;108 (Association for Computational Linguistics, Online). 10.18653/v1/2020.acl-demos.14 (2020).</mixed-citation></ref><ref id="CR22"><label>22.</label><citation-alternatives><element-citation id="ec-CR22" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Uzuner</surname><given-names>&#214;</given-names></name><name name-style="western"><surname>South</surname><given-names>BR</given-names></name><name name-style="western"><surname>Shen</surname><given-names>S</given-names></name><name name-style="western"><surname>DuVall</surname><given-names>SL</given-names></name></person-group><article-title>2010 i2b2/VA challenge on concepts, assertions, and relations in clinical text</article-title><source>J. Am. Med. Inform. Assoc. JAMIA</source><year>2011</year><volume>18</volume><fpage>552</fpage><lpage>556</lpage><pub-id pub-id-type="doi">10.1136/amiajnl-2011-000203</pub-id><pub-id pub-id-type="pmid">21685143</pub-id><pub-id pub-id-type="pmcid">PMC3168320</pub-id></element-citation><mixed-citation id="mc-CR22" publication-type="journal">Uzuner, &#214;., South, B. R., Shen, S. &amp; DuVall, S. L. 2010 i2b2/VA challenge on concepts, assertions, and relations in clinical text. <italic toggle="yes">J. Am. Med. Inform. Assoc. JAMIA</italic><bold>18</bold>, 552&#8211;556 (2011).<pub-id pub-id-type="pmid">21685143</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1136/amiajnl-2011-000203</pub-id><pub-id pub-id-type="pmcid">PMC3168320</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR23"><label>23.</label><citation-alternatives><element-citation id="ec-CR23" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Fries</surname><given-names>JA</given-names></name><etal/></person-group><article-title>Ontology-driven weak supervision for clinical entity classification in electronic health records</article-title><source>Nat. Commun.</source><year>2021</year><volume>12</volume><fpage>2017</fpage><pub-id pub-id-type="doi">10.1038/s41467-021-22328-4</pub-id><pub-id pub-id-type="pmid">33795682</pub-id><pub-id pub-id-type="pmcid">PMC8016863</pub-id></element-citation><mixed-citation id="mc-CR23" publication-type="journal">Fries, J. A. et al. Ontology-driven weak supervision for clinical entity classification in electronic health records. <italic toggle="yes">Nat. Commun.</italic><bold>12</bold>, 2017 (2021).<pub-id pub-id-type="pmid">33795682</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1038/s41467-021-22328-4</pub-id><pub-id pub-id-type="pmcid">PMC8016863</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR24"><label>24.</label><citation-alternatives><element-citation id="ec-CR24" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jaber</surname><given-names>A</given-names></name><name name-style="western"><surname>Mart&#237;nez</surname><given-names>P</given-names></name></person-group><article-title>Disambiguating clinical abbreviations using a one-fits-all classifier based on deep learning techniques</article-title><source>Methods Inf. Med.</source><year>2022</year><volume>61</volume><fpage>e28</fpage><lpage>e34</lpage><pub-id pub-id-type="doi">10.1055/s-0042-1742388</pub-id><pub-id pub-id-type="pmid">35104909</pub-id><pub-id pub-id-type="pmcid">PMC9246508</pub-id></element-citation><mixed-citation id="mc-CR24" publication-type="journal">Jaber, A. &amp; Mart&#237;nez, P. Disambiguating clinical abbreviations using a one-fits-all classifier based on deep learning techniques. <italic toggle="yes">Methods Inf. Med.</italic><bold>61</bold>, e28&#8211;e34 (2022).<pub-id pub-id-type="pmid">35104909</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1055/s-0042-1742388</pub-id><pub-id pub-id-type="pmcid">PMC9246508</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR25"><label>25.</label><mixed-citation publication-type="other">Gu, Y. et al. Domain-specific language model pretraining for biomedical natural language processing. <italic toggle="yes">ACM Trans Comput. Healthc</italic>. <bold>3</bold>, 1&#8211;23 (2022).</mixed-citation></ref><ref id="CR26"><label>26.</label><mixed-citation publication-type="other">Zambrano Chaves, J. et al. RaLEs: a Benchmark for Radiology Language Evaluations. in Advances in Neural Information Processing Systems (eds. Oh, A. et al.) vol. 36 74429&#8211;74454 (Curran Associates, Inc., 2023).</mixed-citation></ref><ref id="CR27"><label>27.</label><citation-alternatives><element-citation id="ec-CR27" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yan</surname><given-names>A</given-names></name><etal/></person-group><article-title>RadBERT: adapting transformer-based language models to radiology</article-title><source>Radiol. Artif. Intell.</source><year>2022</year><volume>4</volume><fpage>e210258</fpage><pub-id pub-id-type="doi">10.1148/ryai.210258</pub-id><pub-id pub-id-type="pmid">35923376</pub-id><pub-id pub-id-type="pmcid">PMC9344353</pub-id></element-citation><mixed-citation id="mc-CR27" publication-type="journal">Yan, A. et al. RadBERT: adapting transformer-based language models to radiology. <italic toggle="yes">Radiol. Artif. Intell.</italic><bold>4</bold>, e210258 (2022).<pub-id pub-id-type="pmid">35923376</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1148/ryai.210258</pub-id><pub-id pub-id-type="pmcid">PMC9344353</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR28"><label>28.</label><mixed-citation publication-type="other">Sushil, M., Ludwig, D., Butte, A. J. &amp; Rudrapatna, V. A. Developing a general-purpose clinical language inference model from a large corpus of clinical notes. arXiv.org <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://arxiv.org/abs/2210.06566v1">https://arxiv.org/abs/2210.06566v1</ext-link> (2022).</mixed-citation></ref><ref id="CR29"><label>29.</label><mixed-citation publication-type="other">Lin, B. Y. et al. Differentiable Open-Ended Commonsense Reasoning. in <italic toggle="yes">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</italic> (eds. Toutanova, K. et al.) 4611&#8211;4625. 10.18653/v1/2021.naacl-main.366 (Association for Computational Linguistics, Online, 2021).</mixed-citation></ref><ref id="CR30"><label>30.</label><mixed-citation publication-type="other">Klein, T. &amp; Nabi, M. Attention is (not) all you need for commonsense reasoning. <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://arxiv.org/abs/1905.13497v1">https://arxiv.org/abs/1905.13497v1</ext-link> (2019).</mixed-citation></ref><ref id="CR31"><label>31.</label><mixed-citation publication-type="other">Li, L., Xin, X. &amp; Guo, P. The exploration of the reasoning capability of BERT in relation extraction. in <italic toggle="yes">2020 10th International Conference on Information Science and Technology (ICIST)</italic> 219&#8211;228. 10.1109/ICIST49303.2020.9202183 (2020).</mixed-citation></ref><ref id="CR32"><label>32.</label><mixed-citation publication-type="other">Amirizaniani, M., Martin, E., Sivachenko, M., Mashhadi, A. &amp; Shah, C. Do LLMs exhibit human-like reasoning? Evaluating theory of mind in LLMs for open-ended responses. <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://arxiv.org/abs/2406.05659v1">https://arxiv.org/abs/2406.05659v1</ext-link> (2024).</mixed-citation></ref><ref id="CR33"><label>33.</label><citation-alternatives><element-citation id="ec-CR33" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Sushil</surname><given-names>M</given-names></name><etal/></person-group><article-title>CORAL: Expert-curated oncology reports to advance language model inference</article-title><source>NEJM AI</source><year>2024</year><volume>1</volume><fpage>AIdbp2300110</fpage><pub-id pub-id-type="doi">10.1056/AIdbp2300110</pub-id></element-citation><mixed-citation id="mc-CR33" publication-type="journal">Sushil, M. et al. CORAL: Expert-curated oncology reports to advance language model inference. <italic toggle="yes">NEJM AI</italic><bold>1</bold>, AIdbp2300110 (2024).</mixed-citation></citation-alternatives></ref><ref id="CR34"><label>34.</label><mixed-citation publication-type="other">Van Veen, D. et al. Adapted large language models can outperform medical experts in clinical text summarization. <italic toggle="yes">Nat. Med</italic>. 1&#8211;9 10.1038/s41591-024-02855-5 (2024).<pub-id pub-id-type="doi" assigning-authority="pmc">10.1038/s41591-024-02855-5</pub-id><pub-id pub-id-type="pmcid">PMC11479659</pub-id><pub-id pub-id-type="pmid">38413730</pub-id></mixed-citation></ref><ref id="CR35"><label>35.</label><mixed-citation publication-type="other">Tu, T. et al. Towards conversational diagnostic AI. Preprint at 10.48550/arXiv.2401.05654 (2024).</mixed-citation></ref><ref id="CR36"><label>36.</label><mixed-citation publication-type="other">Brown, T. et al. Language Models are Few-Shot Learners. in Advances in Neural Information Processing Systems vol. 33 1877&#8211;1901 (Curran Associates, Inc., 2020).</mixed-citation></ref><ref id="CR37"><label>37.</label><citation-alternatives><element-citation id="ec-CR37" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Moor</surname><given-names>M</given-names></name><etal/></person-group><article-title>Foundation models for generalist medical artificial intelligence</article-title><source>Nature</source><year>2023</year><volume>616</volume><fpage>259</fpage><lpage>265</lpage><pub-id pub-id-type="doi">10.1038/s41586-023-05881-4</pub-id><pub-id pub-id-type="pmid">37045921</pub-id></element-citation><mixed-citation id="mc-CR37" publication-type="journal">Moor, M. et al. Foundation models for generalist medical artificial intelligence. <italic toggle="yes">Nature</italic><bold>616</bold>, 259&#8211;265 (2023).<pub-id pub-id-type="pmid">37045921</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1038/s41586-023-05881-4</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR38"><label>38.</label><mixed-citation publication-type="other">Agrawal, M., Hegselmann, S., Lang, H., Kim, Y. &amp; Sontag, D. Large language models are few-shot clinical information extractors. in <italic toggle="yes">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</italic> (eds. Goldberg, Y., Kozareva, Z. &amp; Zhang, Y.) 1998&#8211;2022. 10.18653/v1/2022.emnlp-main.130 (Association for Computational Linguistics, Abu Dhabi, United Arab Emirates, 2022).</mixed-citation></ref><ref id="CR39"><label>39.</label><citation-alternatives><element-citation id="ec-CR39" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Guevara</surname><given-names>M</given-names></name><etal/></person-group><article-title>Large language models to identify social determinants of health in electronic health records</article-title><source>Npj Digit. Med.</source><year>2024</year><volume>7</volume><fpage>1</fpage><lpage>14</lpage><pub-id pub-id-type="doi">10.1038/s41746-023-00970-0</pub-id><pub-id pub-id-type="pmid">38200151</pub-id><pub-id pub-id-type="pmcid">PMC10781957</pub-id></element-citation><mixed-citation id="mc-CR39" publication-type="journal">Guevara, M. et al. Large language models to identify social determinants of health in electronic health records. <italic toggle="yes">Npj Digit. Med.</italic><bold>7</bold>, 1&#8211;14 (2024).<pub-id pub-id-type="pmid">38200151</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1038/s41746-023-00970-0</pub-id><pub-id pub-id-type="pmcid">PMC10781957</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR40"><label>40.</label><mixed-citation publication-type="other">Goel, A. et al. LLMs Accelerate Annotation for Medical Information Extraction. in <italic toggle="yes">Proceedings of the 3rd Machine Learning for Health Symposium</italic> 82&#8211;100 (PMLR, 2023).</mixed-citation></ref><ref id="CR41"><label>41.</label><mixed-citation publication-type="other">Mahbub, M. et al. Leveraging large language models to extract information on substance use disorder severity from clinical notes: a zero-shot learning approach. Preprint at <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="http://arxiv.org/abs/2403.12297">http://arxiv.org/abs/2403.12297</ext-link> (2024).</mixed-citation></ref><ref id="CR42"><label>42.</label><mixed-citation publication-type="other">Levy, M., Jacoby, A. &amp; Goldberg, Y. Same Task, More Tokens: the Impact of Input Length on the Reasoning Performance of Large Language Models. in <italic toggle="yes">Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics</italic> (Volume 1: Long Papers) (eds. Ku, L.-W., Martins, A. &amp; Srikumar, V.) 15339&#8211;15353. 10.18653/v1/2024.acl-long.818 (Association for Computational Linguistics, Bangkok, Thailand, 2024).</mixed-citation></ref><ref id="CR43"><label>43.</label><mixed-citation publication-type="other">Shaham, U., Ivgi, M., Efrat, A., Berant, J. &amp; Levy, O. ZeroSCROLLS: a zero-shot benchmark for long text understanding. <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://arxiv.org/abs/2305.14196v3">https://arxiv.org/abs/2305.14196v3</ext-link> (2023).</mixed-citation></ref><ref id="CR44"><label>44.</label><mixed-citation publication-type="other">Bai, Y. et al. LongBench: A bilingual, multitask benchmark for long context understanding. <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://arxiv.org/abs/2308.14508v1">https://arxiv.org/abs/2308.14508v1</ext-link> (2023).</mixed-citation></ref><ref id="CR45"><label>45.</label><mixed-citation publication-type="other">Li, J., Wang, M., Zheng, Z. &amp; Zhang, M. LooGLE: Can long-context language models understand long contexts? <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://arxiv.org/abs/2311.04939v1">https://arxiv.org/abs/2311.04939v1</ext-link> (2023).</mixed-citation></ref><ref id="CR46"><label>46.</label><mixed-citation publication-type="other">Gao, Y. et al. Retrieval-augmented generation for large language models: a survey. Preprint at 10.48550/arXiv.2312.10997 (2024).</mixed-citation></ref><ref id="CR47"><label>47.</label><mixed-citation publication-type="other">Agrawal, G., Kumarage, T., Alghamdi, Z. &amp; Liu, H. Can Knowledge Graphs Reduce Hallucinations in LLMs? : A Survey. in <italic toggle="yes">Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</italic> (Volume 1: Long Papers) (eds. Duh, K., Gomez, H. &amp; Bethard, S.) 3947&#8211;3960. 10.18653/v1/2024.naacl-long.219. (Association for Computational Linguistics, Mexico City, Mexico, 2024).</mixed-citation></ref><ref id="CR48"><label>48.</label><mixed-citation publication-type="other">Wu, Y. et al. Retrieve-rewrite-answer: a KG-to-text enhanced LLMs framework for knowledge graph question answering. Preprint at <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="http://arxiv.org/abs/2309.11206">http://arxiv.org/abs/2309.11206</ext-link> (2023).</mixed-citation></ref><ref id="CR49"><label>49.</label><mixed-citation publication-type="other">Baek, J., Aji, A. F. &amp; Saffari, A. Knowledge-Augmented Language Model Prompting for Zero-Shot Knowledge Graph Question Answering. in <italic toggle="yes">Proceedings of the 1st Workshop on Natural Language Reasoning and Structured Explanations (NLRSE)</italic> (eds. Dalvi Mishra, B., Durrett, G., Jansen, P., Neves Ribeiro, D. &amp; Wei, J.) 78&#8211;106. 10.18653/v1/2023.nlrse-1.7. (Association for Computational Linguistics, Toronto, Canada, 2023).</mixed-citation></ref><ref id="CR50"><label>50.</label><mixed-citation publication-type="other">Li, Y., Dong, B., Guerin, F. &amp; Lin, C. Compressing Context to Enhance Inference Efficiency of Large Language Models. in <italic toggle="yes">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</italic> (eds. Bouamor, H., Pino, J. &amp; Bali, K.) 6342&#8211;6353. 10.18653/v1/2023.emnlp-main.391 (Association for Computational Linguistics, Singapore, 2023).</mixed-citation></ref><ref id="CR51"><label>51.</label><mixed-citation publication-type="other">Mialon, G. et al. Augmented language models: a survey. Preprint at <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="http://arxiv.org/abs/2302.07842">http://arxiv.org/abs/2302.07842</ext-link> (2023).</mixed-citation></ref><ref id="CR52"><label>52.</label><citation-alternatives><element-citation id="ec-CR52" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>NF</given-names></name><etal/></person-group><article-title>Lost in the Middle: How Language Models Use Long Contexts</article-title><source>Trans. Assoc. Comput. Linguist.</source><year>2024</year><volume>12</volume><fpage>157</fpage><lpage>173</lpage><pub-id pub-id-type="doi">10.1162/tacl_a_00638</pub-id></element-citation><mixed-citation id="mc-CR52" publication-type="journal">Liu, N. F. et al. Lost in the Middle: How Language Models Use Long Contexts. <italic toggle="yes">Trans. Assoc. Comput. Linguist.</italic><bold>12</bold>, 157&#8211;173 (2024).</mixed-citation></citation-alternatives></ref><ref id="CR53"><label>53.</label><mixed-citation publication-type="other">Zhao, P. et al. Retrieval-augmented generation for AI-generated content: a survey. Preprint at <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="http://arxiv.org/abs/2402.19473">http://arxiv.org/abs/2402.19473</ext-link> (2024).</mixed-citation></ref><ref id="CR54"><label>54.</label><mixed-citation publication-type="other">Das, R. et al. Case-based Reasoning for Natural Language Queries over Knowledge Bases. in <italic toggle="yes">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</italic> (eds. Moens, M.-F., Huang, X., Specia, L. &amp; Yih, S. W.) 9594&#8211;9611. 10.18653/v1/2021.emnlp-main.755. (Association for Computational Linguistics, Online and Punta Cana, Dominican Republic, 2021).</mixed-citation></ref><ref id="CR55"><label>55.</label><mixed-citation publication-type="other">Hu, X., Wu, X., Shu, Y. &amp; Qu, Y. Logical Form Generation via Multi-task Learning for Complex Question Answering over Knowledge Bases. in <italic toggle="yes">Proceedings of the 29th International Conference on Computational Linguistics</italic> (eds. Calzolari, N. et al.) 1687&#8211;1696 (International Committee on Computational Linguistics, Gyeongju, Republic of Korea, 2022).</mixed-citation></ref><ref id="CR56"><label>56.</label><mixed-citation publication-type="other">Zhang, L. et al. FC-KBQA: A fine-to-coarse composition framework for knowledge base question answering. in <italic toggle="yes">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</italic> (eds. Rogers, A., Boyd-Graber, J. &amp; Okazaki, N.) 1002&#8211;1017 (Association for Computational Linguistics, Toronto, Canada, 2023). 10.18653/v1/2023.acl-long.57 (2023).</mixed-citation></ref><ref id="CR57"><label>57.</label><mixed-citation publication-type="other">Jiang, J. et al. StructGPT: A General Framework for Large Language Model to Reason over Structured Data. in <italic toggle="yes">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</italic> (eds. Bouamor, H., Pino, J. &amp; Bali, K.) 9237&#8211;9251 (Association for Computational Linguistics, Singapore, 2023). 10.18653/v1/2023.emnlp-main.574 (2023).</mixed-citation></ref><ref id="CR58"><label>58.</label><mixed-citation publication-type="other">Xu, Z. et al. Retrieval-Augmented Generation with Knowledge Graphs for Customer Service Question Answering. in <italic toggle="yes">Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval 2905&#8211;2909</italic>. 10.1145/3626772.3661370. (Association for Computing Machinery, New York, NY, USA, 2024).</mixed-citation></ref><ref id="CR59"><label>59.</label><citation-alternatives><element-citation id="ec-CR59" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Guo</surname><given-names>T</given-names></name><etal/></person-group><article-title>KnowledgeNavigator: leveraging large language models for enhanced reasoning over knowledge graph</article-title><source>Complex Intell. Syst.</source><year>2024</year><volume>10</volume><fpage>7063</fpage><lpage>7076</lpage><pub-id pub-id-type="doi">10.1007/s40747-024-01527-8</pub-id></element-citation><mixed-citation id="mc-CR59" publication-type="journal">Guo, T. et al. KnowledgeNavigator: leveraging large language models for enhanced reasoning over knowledge graph. <italic toggle="yes">Complex Intell. Syst.</italic><bold>10</bold>, 7063&#8211;7076 (2024).</mixed-citation></citation-alternatives></ref><ref id="CR60"><label>60.</label><mixed-citation publication-type="other">Ji, Z. et al. RHO: reducing hallucination in open-domain dialogues with knowledge grounding. in <italic toggle="yes">Findings of the Association for Computational Linguistics: ACL 2023</italic> (eds. Rogers, A., Boyd-Graber, J. &amp; Okazaki, N.) 4504&#8211;4522 (Association for Computational Linguistics, Toronto, Canada, 2023). 10.18653/v1/2023.findings-acl.275 (2023).</mixed-citation></ref><ref id="CR61"><label>61.</label><citation-alternatives><element-citation id="ec-CR61" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Al-Moslmi</surname><given-names>T</given-names></name><name name-style="western"><surname>Gallofr&#233; Oca&#241;a</surname><given-names>M</given-names></name><name name-style="western"><surname>Opdahl</surname><given-names>A</given-names></name><name name-style="western"><surname>Veres</surname><given-names>C</given-names></name></person-group><article-title>Named entity extraction for knowledge graphs: a literature overview</article-title><source>IEEE Access</source><year>2020</year><volume>8</volume><fpage>32862</fpage><lpage>32881</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2020.2973928</pub-id></element-citation><mixed-citation id="mc-CR61" publication-type="journal">Al-Moslmi, T., Gallofr&#233; Oca&#241;a, M., Opdahl, A. L. &amp; Veres, C. Named entity extraction for knowledge graphs: a literature overview. <italic toggle="yes">IEEE Access</italic><bold>8</bold>, 32862&#8211;32881 (2020).</mixed-citation></citation-alternatives></ref><ref id="CR62"><label>62.</label><citation-alternatives><element-citation id="ec-CR62" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Fleming</surname><given-names>SL</given-names></name><etal/></person-group><article-title>MedAlign: A Clinician-Generated Dataset for Instruction Following with Electronic Medical Records</article-title><source>Proc. AAAI Conf. Artif. Intell.</source><year>2024</year><volume>38</volume><fpage>22021</fpage><lpage>22030</lpage></element-citation><mixed-citation id="mc-CR62" publication-type="journal">Fleming, S. L. et al. MedAlign: A Clinician-Generated Dataset for Instruction Following with Electronic Medical Records. <italic toggle="yes">Proc. AAAI Conf. Artif. Intell.</italic><bold>38</bold>, 22021&#8211;22030 (2024).</mixed-citation></citation-alternatives></ref><ref id="CR63"><label>63.</label><citation-alternatives><element-citation id="ec-CR63" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Galea</surname><given-names>S</given-names></name><name name-style="western"><surname>Merchant</surname><given-names>RM</given-names></name><name name-style="western"><surname>Lurie</surname><given-names>N</given-names></name></person-group><article-title>The mental health consequences of COVID-19 and physical distancing: the need for prevention and early intervention</article-title><source>JAMA Intern. Med.</source><year>2020</year><volume>180</volume><fpage>817</fpage><lpage>818</lpage><pub-id pub-id-type="doi">10.1001/jamainternmed.2020.1562</pub-id><pub-id pub-id-type="pmid">32275292</pub-id></element-citation><mixed-citation id="mc-CR63" publication-type="journal">Galea, S., Merchant, R. M. &amp; Lurie, N. The mental health consequences of COVID-19 and physical distancing: the need for prevention and early intervention. <italic toggle="yes">JAMA Intern. Med.</italic><bold>180</bold>, 817&#8211;818 (2020).<pub-id pub-id-type="pmid">32275292</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1001/jamainternmed.2020.1562</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR64"><label>64.</label><citation-alternatives><element-citation id="ec-CR64" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Nateghi Haredasht</surname><given-names>F</given-names></name><etal/></person-group><article-title>Predictability of buprenorphine-naloxone treatment retention: A multi-site analysis combining electronic health records and machine learning</article-title><source>Addiction</source><year>2024</year><volume>119</volume><fpage>1792</fpage><lpage>1802</lpage><pub-id pub-id-type="doi">10.1111/add.16587</pub-id><pub-id pub-id-type="pmid">38923168</pub-id><pub-id pub-id-type="pmcid">PMC11891486</pub-id></element-citation><mixed-citation id="mc-CR64" publication-type="journal">Nateghi Haredasht, F. et al. Predictability of buprenorphine-naloxone treatment retention: A multi-site analysis combining electronic health records and machine learning. <italic toggle="yes">Addiction</italic><bold>119</bold>, 1792&#8211;1802 (2024).<pub-id pub-id-type="pmid">38923168</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1111/add.16587</pub-id><pub-id pub-id-type="pmcid">PMC11891486</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR65"><label>65.</label><mixed-citation publication-type="other">Datta, S. et al. A new paradigm for accelerating clinical data science at Stanford Medicine. Preprint at 10.48550/arXiv.2003.10534 (2020).</mixed-citation></ref><ref id="CR66"><label>66.</label><citation-alternatives><element-citation id="ec-CR66" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Irvin</surname><given-names>J</given-names></name><etal/></person-group><article-title>CheXpert: A Large Chest Radiograph Dataset with Uncertainty Labels and Expert Comparison</article-title><source>Proc. AAAI Conf. Artif. Intell.</source><year>2019</year><volume>33</volume><fpage>590</fpage><lpage>597</lpage></element-citation><mixed-citation id="mc-CR66" publication-type="journal">Irvin, J. et al. CheXpert: A Large Chest Radiograph Dataset with Uncertainty Labels and Expert Comparison. <italic toggle="yes">Proc. AAAI Conf. Artif. Intell.</italic><bold>33</bold>, 590&#8211;597 (2019).</mixed-citation></citation-alternatives></ref><ref id="CR67"><label>67.</label><citation-alternatives><element-citation id="ec-CR67" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chung</surname><given-names>HW</given-names></name><etal/></person-group><article-title>Scaling Instruction-Finetuned Language Models</article-title><source>J. Mach. Learn. Res.</source><year>2024</year><volume>25</volume><fpage>1</fpage><lpage>53</lpage></element-citation><mixed-citation id="mc-CR67" publication-type="journal">Chung, H. W. et al. Scaling Instruction-Finetuned Language Models. <italic toggle="yes">J. Mach. Learn. Res.</italic><bold>25</bold>, 1&#8211;53 (2024).</mixed-citation></citation-alternatives></ref><ref id="CR68"><label>68.</label><citation-alternatives><element-citation id="ec-CR68" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Do&#287;an</surname><given-names>RI</given-names></name><name name-style="western"><surname>Leaman</surname><given-names>R</given-names></name><name name-style="western"><surname>Lu</surname><given-names>Z</given-names></name></person-group><article-title>NCBI disease corpus: a resource for disease name recognition and concept normalization</article-title><source>J. Biomed. Inform.</source><year>2014</year><volume>47</volume><fpage>1</fpage><lpage>10</lpage><pub-id pub-id-type="doi">10.1016/j.jbi.2013.12.006</pub-id><pub-id pub-id-type="pmid">24393765</pub-id><pub-id pub-id-type="pmcid">PMC3951655</pub-id></element-citation><mixed-citation id="mc-CR68" publication-type="journal">Do&#287;an, R. I., Leaman, R. &amp; Lu, Z. NCBI disease corpus: a resource for disease name recognition and concept normalization. <italic toggle="yes">J. Biomed. Inform.</italic><bold>47</bold>, 1&#8211;10 (2014).<pub-id pub-id-type="pmid">24393765</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.jbi.2013.12.006</pub-id><pub-id pub-id-type="pmcid">PMC3951655</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR69"><label>69.</label><mixed-citation publication-type="other">Alsentzer, E. et al. Publicly Available Clinical BERT Embeddings. in <italic toggle="yes">Proceedings of the 2nd Clinical Natural Language Processing Workshop</italic> (eds. Rumshisky, A., Roberts, K., Bethard, S. &amp; Naumann, T.) 72&#8211;78. 10.18653/v1/W19-1909. (Association for Computational Linguistics, Minneapolis, Minnesota, USA, 2019).</mixed-citation></ref><ref id="CR70"><label>70.</label><citation-alternatives><element-citation id="ec-CR70" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bodenreider</surname><given-names>O</given-names></name></person-group><article-title>The Unified Medical Language System (UMLS): integrating biomedical terminology</article-title><source>Nucleic Acids Res</source><year>2004</year><volume>32</volume><fpage>D267</fpage><lpage>D270</lpage><pub-id pub-id-type="doi">10.1093/nar/gkh061</pub-id><pub-id pub-id-type="pmid">14681409</pub-id><pub-id pub-id-type="pmcid">PMC308795</pub-id></element-citation><mixed-citation id="mc-CR70" publication-type="journal">Bodenreider, O. The Unified Medical Language System (UMLS): integrating biomedical terminology. <italic toggle="yes">Nucleic Acids Res</italic><bold>32</bold>, D267&#8211;D270 (2004).<pub-id pub-id-type="pmid">14681409</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1093/nar/gkh061</pub-id><pub-id pub-id-type="pmcid">PMC308795</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR71"><label>71.</label><mixed-citation publication-type="other">Stearns, M. Q., Price, C., Spackman, K. A. &amp; Wang, A. Y. SNOMED clinical terms: overview of the development process and project status. <italic toggle="yes">Proc. AMIA Symp</italic>. <bold>2001</bold>, 662&#8211;666 (2001).<pub-id pub-id-type="pmcid">PMC2243297</pub-id><pub-id pub-id-type="pmid">11825268</pub-id></mixed-citation></ref><ref id="CR72"><label>72.</label><citation-alternatives><element-citation id="ec-CR72" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Eyre</surname><given-names>H</given-names></name><etal/></person-group><article-title>Launching into clinical space with medspaCy: a new clinical text processing toolkit in Python</article-title><source>AMIA. Annu. Symp. Proc.</source><year>2022</year><volume>2021</volume><fpage>438</fpage><lpage>447</lpage><pub-id pub-id-type="pmid">35308962</pub-id><pub-id pub-id-type="pmcid">PMC8861690</pub-id></element-citation><mixed-citation id="mc-CR72" publication-type="journal">Eyre, H. et al. Launching into clinical space with medspaCy: a new clinical text processing toolkit in Python. <italic toggle="yes">AMIA. Annu. Symp. Proc.</italic><bold>2021</bold>, 438&#8211;447 (2022).<pub-id pub-id-type="pmid">35308962</pub-id><pub-id pub-id-type="pmcid">PMC8861690</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR73"><label>73.</label><mixed-citation publication-type="other">Christophe, C. et al. Med42 -- Evaluating Fine-Tuning Strategies for Medical LLMs: Full-Parameter vs. Parameter-Efficient Approaches. Preprint at 10.48550/arXiv.2404.14779 (2024).</mixed-citation></ref><ref id="CR74"><label>74.</label><mixed-citation publication-type="other">AI, M. <italic toggle="yes">Mixtral of Experts</italic>. <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://mistral.ai/news/mixtral-of-experts/">https://mistral.ai/news/mixtral-of-experts/</ext-link> (2023).</mixed-citation></ref><ref id="CR75"><label>75.</label><mixed-citation publication-type="other">Grattafiori, A. et al. The Llama 3 Herd of Models. Preprint at 10.48550/arXiv.2407.21783 (2024).</mixed-citation></ref><ref id="CR76"><label>76.</label><mixed-citation publication-type="other">Tay, Y. et al. <italic toggle="yes">Unifying Language Learning Paradigms</italic>. <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://arxiv.org/abs/2205.05131">https://arxiv.org/abs/2205.05131</ext-link> (2023).</mixed-citation></ref><ref id="CR77"><label>77.</label><mixed-citation publication-type="other">OpenAI et al. <italic toggle="yes">GPT-4 Technical Report</italic>. <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://arxiv.org/abs/2303.08774v6">https://arxiv.org/abs/2303.08774v6</ext-link> (2023).</mixed-citation></ref><ref id="CR78"><label>78.</label><mixed-citation publication-type="other">Dong, Q. et al. A survey on in-context learning. <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://arxiv.org/abs/2301.00234v3">https://arxiv.org/abs/2301.00234v3</ext-link> (2022).</mixed-citation></ref><ref id="CR79"><label>79.</label><citation-alternatives><element-citation id="ec-CR79" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ratner</surname><given-names>A</given-names></name><etal/></person-group><article-title>Snorkel: rapid training data creation with weak supervision</article-title><source>Proc. VLDB Endow.</source><year>2017</year><volume>11</volume><fpage>269</fpage><lpage>282</lpage><pub-id pub-id-type="doi">10.14778/3157794.3157797</pub-id><pub-id pub-id-type="pmcid">PMC5951191</pub-id><pub-id pub-id-type="pmid">29770249</pub-id></element-citation><mixed-citation id="mc-CR79" publication-type="journal">Ratner, A. et al. Snorkel: rapid training data creation with weak supervision. <italic toggle="yes">Proc. VLDB Endow.</italic><bold>11</bold>, 269&#8211;282 (2017).<pub-id pub-id-type="doi" assigning-authority="pmc">10.14778/3157794.3157797</pub-id><pub-id pub-id-type="pmcid">PMC5951191</pub-id><pub-id pub-id-type="pmid">29770249</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR80"><label>80.</label><mixed-citation publication-type="other">OpenAI is an AI research and deployment company. <italic toggle="yes">Terms of Use</italic>. <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://openai.com/policies/terms-of-use/">https://openai.com/policies/terms-of-use/</ext-link> (2023).</mixed-citation></ref><ref id="CR81"><label>81.</label><mixed-citation publication-type="other">Xiao, S., Liu, Z., Zhang, P. &amp; Muennighoff, N. C-Pack: packaged resources to advance general Chinese embedding. Preprint at 10.48550/arXiv.2309.07597 (2023).</mixed-citation></ref><ref id="CR82"><label>82.</label><citation-alternatives><element-citation id="ec-CR82" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wilcoxon</surname><given-names>F</given-names></name></person-group><article-title>Individual comparisons by ranking methods</article-title><source>Biom. Bull.</source><year>1945</year><volume>1</volume><fpage>80</fpage><lpage>83</lpage><pub-id pub-id-type="doi">10.2307/3001968</pub-id></element-citation><mixed-citation id="mc-CR82" publication-type="journal">Wilcoxon, F. Individual comparisons by ranking methods. <italic toggle="yes">Biom. Bull.</italic><bold>1</bold>, 80&#8211;83 (1945).</mixed-citation></citation-alternatives></ref></ref-list></back></article></pmc-articleset>