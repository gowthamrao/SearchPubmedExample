<?xml version="1.0"  ?><!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd"><pmc-articleset><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xml:lang="en" article-type="research-article" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Healthcare (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Healthcare (Basel)</journal-id><journal-id journal-id-type="pmc-domain-id">2994</journal-id><journal-id journal-id-type="pmc-domain">healthcare</journal-id><journal-id journal-id-type="publisher-id">healthcare</journal-id><journal-title-group><journal-title>Healthcare</journal-title></journal-title-group><issn pub-type="epub">2227-9032</issn><publisher><publisher-name>Multidisciplinary Digital Publishing Institute  (MDPI)</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC10138364</article-id><article-id pub-id-type="pmcid-ver">PMC10138364.1</article-id><article-id pub-id-type="pmcaid">10138364</article-id><article-id pub-id-type="pmcaiid">10138364</article-id><article-id pub-id-type="pmid">37107987</article-id><article-id pub-id-type="doi">10.3390/healthcare11081152</article-id><article-id pub-id-type="publisher-id">healthcare-11-01152</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Geriatric Care Management System Powered by the IoT and Computer Vision Techniques</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-8787-3343</contrib-id><name name-style="western"><surname>Paulauskaite-Taraseviciene</surname><given-names initials="A">Agne</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><xref rid="af1-healthcare-11-01152" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-2425-7260</contrib-id><name name-style="western"><surname>Siaulys</surname><given-names initials="J">Julius</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><xref rid="af1-healthcare-11-01152" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-5412-3194</contrib-id><name name-style="western"><surname>Sutiene</surname><given-names initials="K">Kristina</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role><xref rid="af2-healthcare-11-01152" ref-type="aff">2</xref><xref rid="c1-healthcare-11-01152" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Petravicius</surname><given-names initials="T">Titas</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><xref rid="af1-healthcare-11-01152" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Navickas</surname><given-names initials="S">Skirmantas</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><xref rid="af1-healthcare-11-01152" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Oliandra</surname><given-names initials="M">Marius</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><xref rid="af1-healthcare-11-01152" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-0183-5168</contrib-id><name name-style="western"><surname>Rapalis</surname><given-names initials="A">Andrius</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><xref rid="af3-healthcare-11-01152" ref-type="aff">3</xref><xref rid="af4-healthcare-11-01152" ref-type="aff">4</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-0423-9153</contrib-id><name name-style="western"><surname>Balciunas</surname><given-names initials="J">Justinas</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><xref rid="af5-healthcare-11-01152" ref-type="aff">5</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name name-style="western"><surname>Alor-Hern&#225;ndez</surname><given-names initials="G">Giner</given-names></name><role>Academic Editor</role></contrib><contrib contrib-type="editor"><name name-style="western"><surname>Mej&#237;a-Miranda</surname><given-names initials="J">Jezreel</given-names></name><role>Academic Editor</role></contrib><contrib contrib-type="editor"><name name-style="western"><surname>S&#225;nchez-Cervantes</surname><given-names initials="JL">Jos&#233; Luis</given-names></name><role>Academic Editor</role></contrib><contrib contrib-type="editor"><name name-style="western"><surname>Rodr&#237;guez-Gonz&#225;lez</surname><given-names initials="A">Alejandro</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-healthcare-11-01152"><label>1</label>Faculty of Informatics, Kaunas University of Technology, Studentu 50, 51368 Kaunas, Lithuania</aff><aff id="af2-healthcare-11-01152"><label>2</label>Department of Mathematical Modeling, Kaunas University of Technology, Studentu 50, 51368 Kaunas, Lithuania</aff><aff id="af3-healthcare-11-01152"><label>3</label>Biomedical Engineering Institute, Kaunas University of Technology, K. Barsausko 59, 51423 Kaunas, Lithuania</aff><aff id="af4-healthcare-11-01152"><label>4</label>Faculty of Electrical and Electronics Engineering, Kaunas University of Technology, Studentu 48, 51367 Kaunas, Lithuania</aff><aff id="af5-healthcare-11-01152"><label>5</label>Faculty of Medicine, Vilnius University, Universiteto 3, 01513 Vilnius, Lithuania</aff><author-notes><corresp id="c1-healthcare-11-01152"><label>*</label>Correspondence: <email>kristina.sutiene@ktu.lt</email></corresp></author-notes><pub-date pub-type="epub"><day>17</day><month>4</month><year>2023</year></pub-date><pub-date pub-type="collection"><month>4</month><year>2023</year></pub-date><volume>11</volume><issue>8</issue><issue-id pub-id-type="pmc-issue-id">434474</issue-id><elocation-id>1152</elocation-id><history><date date-type="received"><day>22</day><month>2</month><year>2023</year></date><date date-type="rev-recd"><day>03</day><month>4</month><year>2023</year></date><date date-type="accepted"><day>13</day><month>4</month><year>2023</year></date></history><pub-history><event event-type="pmc-release"><date><day>17</day><month>04</month><year>2023</year></date></event><event event-type="pmc-live"><date><day>28</day><month>04</month><year>2023</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2024-09-16 16:25:37.563"><day>16</day><month>09</month><year>2024</year></date></event></pub-history><permissions><copyright-statement>&#169; 2023 by the authors.</copyright-statement><copyright-year>2023</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pmc-pdf" xlink:href="healthcare-11-01152.pdf"/><abstract><p>The digitalisation of geriatric care refers to the use of emerging technologies to manage and provide person-centered care to the elderly by collecting patients&#8217; data electronically and using them to streamline the care process, which improves the overall quality, accuracy, and efficiency of healthcare. In many countries, healthcare providers still rely on the manual measurement of bioparameters, inconsistent monitoring, and paper-based care plans to manage and deliver care to elderly patients. This can lead to a number of problems, including incomplete and inaccurate record-keeping, errors, and delays in identifying and resolving health problems. The purpose of this study is to develop a geriatric care management system that combines signals from various wearable sensors, noncontact measurement devices, and image recognition techniques to monitor and detect changes in the health status of a person. The system relies on deep learning algorithms and the Internet of Things (IoT) to identify the patient and their six most pertinent poses. In addition, the algorithm has been developed to monitor changes in the patient&#8217;s position over a longer period of time, which could be important for detecting health problems in a timely manner and taking appropriate measures. Finally, based on expert knowledge and a priori rules integrated in a decision tree-based model, the automated final decision on the status of nursing care plan is generated to support nursing staff.</p></abstract><kwd-group><kwd>geriatric care</kwd><kwd>IoT</kwd><kwd>vital parameters</kwd><kwd>posture recognition</kwd><kwd>image recognition</kwd><kwd>deep learning</kwd><kwd>non-contact monitoring</kwd></kwd-group><funding-group><award-group><funding-source>EIT Regional Innovation Scheme (EIT RIS)-EIT Health-Nursing.AI</funding-source><award-id>2021-RIS_Innovation-033</award-id></award-group><funding-statement>This research was supported by the project EIT Regional Innovation Scheme (EIT RIS)-EIT Health-Nursing.AI, 2022, Project ID: 2021-RIS_Innovation-033.</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="sec1-healthcare-11-01152"><title>1. Introduction</title><p>Geriatric care is a field of healthcare that focuses on the physical, mental, and social needs of older adults. As people age, they may experience physical, cognitive, and social changes that require special care and support. Geriatric care is based on the specific needs of older adults and aims to improve their health and well-being as well as manage age-related diseases and conditions so that they can maintain their independence, quality of life, and overall comfort. Such care often involves a multidisciplinary approach with care provided by a team of health care professionals, including physicians, nurses, therapists, and social workers, who are trained in gerontology and geriatrics [<xref rid="B1-healthcare-11-01152" ref-type="bibr">1</xref>,<xref rid="B2-healthcare-11-01152" ref-type="bibr">2</xref>]. The estimated number of dependent people in need of some form of long-term care in Europe is 30.8 million, and this is expected to increase to 38 million by 2050. Furthermore, the expected shortage of nurses will reach 2.3 million in 2030. By 2080, the population aged over 80 years and older in Europe will have multiplied by 2.5. It should be noted that the majority of dependent patients suffer from Alzheimer&#8217;s and chronic diseases, such as past myocardial infarction, congestive heart failure, cardiac arrhythmia, renal failure, and chronic pulmonary disease, have an increased risk of mortality in nursing homes [<xref rid="B3-healthcare-11-01152" ref-type="bibr">3</xref>].</p><p>Currently, the main problems are caused by the absence of tools to design automated care plans. The problems identified are related to the lack of digital evidence-based protocols for different situations and the nonadherence to existing protocols by nursing staff. Typically, an individualised nursing care plan is developed for the elderly patient upon admission to meet their needs. This plan is developed based on a thorough assessment of the person&#8217;s medical history and evidence-based care practices. As elderly individuals reside in nursing homes, it is common for their health to decline, which makes it crucial to monitor their health status while they are there. Thus, caregivers must regularly check important biometric data, such as blood pressure, heart rate, body temperature, and respiratory rate. Collecting and documenting patient vital signs data manually is a relatively slow and therefore inefficient process. Depending on the types of vital signs, it usually takes up to five minutes to assess three to six vital signs [<xref rid="B4-healthcare-11-01152" ref-type="bibr">4</xref>]. Moreover, this information is usually documented in paper form separately from the nursing care plans, and therefore, the whole process takes up to 13 min per patient [<xref rid="B5-healthcare-11-01152" ref-type="bibr">5</xref>]. Furthermore, care plans have to be regularly re-evaluated by comparing current and historical health records to look for abnormalities and changes that could have clinical significance. However, biometric data are documented separately from nursing care plans and records of doctors. With such fragmented data sources, the process is human-dependent, highly inefficient, and cumbersome and can take up to 37 min per patient [<xref rid="B5-healthcare-11-01152" ref-type="bibr">5</xref>,<xref rid="B6-healthcare-11-01152" ref-type="bibr">6</xref>]. Moreover, in the absence of a systematic approach in geriatric care management, it becomes challenging to quickly capture monitoring data and act on them. This can cause caregivers to miss any unusual changes in the biometric data, leading to delays in administering treatment.</p><p>During the course of our research, several hospices from Latvia, Estonia, and Poland (e.g., Orpea) were contacted, and it was concluded that geriatric care management systems with a digital care plan and remote monitoring solutions are currently not available in these markets. Facilities rely on outdated software that was developed for inpatient hospital services without taking into account the nursing care plan. In particular, in Scandinavian and UK markets (e.g., Appva), some tools have been developed that include a simple digitised nursing care plan without remote monitoring or decision support capabilities; however, none of these companies have shown an interest in providing the service in the Baltic States. Therefore, in many countries, including the Baltic States, nurses use paper-based care plan templates and manually prepare time-consuming documents. Consequently, data loss and missing information in care plans are common problems. Based on the problems identified during oral interviews and discussions with various stakeholders in Lithuania, the following needs for long-term care at home and in specialised institutions have been narrowed down as the most recurrent and yet relatively possible to complete with limited funding: (1)&#160;easily create nursing care plans for new patients with action protocols for nursing staff; (2)&#160;ensure adherence to and traceability of the execution of the protocols; (3)&#160;automate patient monitoring; (4) reduce manual paper documentation; (5) easily adapt nursing care plans according to changes in the health of the patient; and (6) enable a transition from reactive care to proactive care.</p><p>Digitalised care systems could be a solution to meet the multidimensional need to monitor whether elderly patients in geriatric care facilities are receiving optimal care, thus monitoring patients more efficiently and providing personalised care. Digitalisation also helps a relatively small number of healthcare workers to reduce the need for repetitive manual work and use the collected data for proactive decision making. Furthermore, the combination of Internet of Things (IoT) and artificial intelligence (AI) technologies can aid in the analysis of data and ensure continuous monitoring of elderly patients to positively impact their care and outcomes [<xref rid="B7-healthcare-11-01152" ref-type="bibr">7</xref>,<xref rid="B8-healthcare-11-01152" ref-type="bibr">8</xref>,<xref rid="B9-healthcare-11-01152" ref-type="bibr">9</xref>,<xref rid="B10-healthcare-11-01152" ref-type="bibr">10</xref>]. By collecting data on patient activity and health, advanced AI algorithms can analyse patterns and detect deviations from normal behaviour, allowing caregivers to respond in a timely manner.</p><p>In this study, we propose an intelligent geriatric care management system based on AI and IoT to track and detect changes in the health status of elderly patients, thus ensuring efficient digitalisation of personalised care plans. The proposed solution can be used to tackle two of the most urgent problems in the area: nursing staff shortages and the costly and inefficient long-term care process. Although home care for dependent and elderly people is becoming more and more popular, it is still not a viable option for everyone due to the expensive infrastructure required and the difficulties in gaining access to their homes in an emergency. Even if people choose to live in a nursing home, it is still difficult to monitor, care, and treat elderly residents on a regular basis. With the growing demand for healthcare nurses, fragmented remote health monitoring tools, and lack of existing solutions for real-time modifications of nursing care plans, it is crucial to have a cost-effective and semi-autonomous solution available in the market.</p></sec><sec id="sec2-healthcare-11-01152"><title>2. Related Works</title><p>In recent years, there has been a growing interest in the development of digital health solutions to support older people and promote healthy ageing [<xref rid="B11-healthcare-11-01152" ref-type="bibr">11</xref>,<xref rid="B12-healthcare-11-01152" ref-type="bibr">12</xref>]. However, elderly individuals are more likely to develop diseases such as dementia, diabetes, and cataracts, suffer from physical and cognitive impairments, and have low levels of physical activity, all of which lead to a continuous decline in their health. This makes it difficult for staff to keep track of elderly people, to monitor changes in their health, to record and store all readings systematically, and to always react quickly and appropriately to the changes and adjust the care plan. Furthermore, as life expectancy continues to increase, the need for nurses working in geriatrics is also increasing. As such, remote monitoring and wearable devices can be used to measure vital signals, evaluate physical activity, and inform caregivers or physicians about changes in their health, which aids in the early detection of health risks [<xref rid="B13-healthcare-11-01152" ref-type="bibr">13</xref>,<xref rid="B14-healthcare-11-01152" ref-type="bibr">14</xref>].</p><sec id="sec2dot1-healthcare-11-01152"><title>2.1. The Use of Wearable Devices</title><p>Recently, wearable technology has benefited from technological progress, as the size of devices has significantly reduced, while the efficiency of energy consumption has improved simultaneously [<xref rid="B15-healthcare-11-01152" ref-type="bibr">15</xref>]. In particular, wearable technology can be used for a variety of purposes, ranging from keeping track of physical activity to monitoring clinically important health and safety data. Wearable devices provide real-time monitoring of the wearer&#8217;s walking speed, respiratory rate, measuring sleep, energy expenditure, blood oxygen and pressure, and other related parameters [<xref rid="B16-healthcare-11-01152" ref-type="bibr">16</xref>]. Such devices can also be useful tools for people living with heart failure to facilitate exercise and recovery [<xref rid="B17-healthcare-11-01152" ref-type="bibr">17</xref>,<xref rid="B18-healthcare-11-01152" ref-type="bibr">18</xref>]. Comparatively, a study demonstrated the strong potential for improvement in healthcare through the use of wearable activity monitors in oncology trials [<xref rid="B19-healthcare-11-01152" ref-type="bibr">19</xref>]. The use of wearable technology to identify gait characteristics is another intriguing example [<xref rid="B20-healthcare-11-01152" ref-type="bibr">20</xref>], where lower limb joint angles and stride length were measured simultaneously with a prototype wearable sensor system. The study [<xref rid="B21-healthcare-11-01152" ref-type="bibr">21</xref>] investigated how a wearable device could help physicians to optimize antiepileptic treatment and prevent patients from sudden unexpected death due to epilepsy. For particular groups of individuals that suffer from chronic disease such as diabetes mellitus, cardiac disease, or chronic obstructive pulmonary disease, wearables may be used to monitor changes in health symptoms during treatment and may contribute to the personalisation of healthcare [<xref rid="B22-healthcare-11-01152" ref-type="bibr">22</xref>,<xref rid="B23-healthcare-11-01152" ref-type="bibr">23</xref>,<xref rid="B24-healthcare-11-01152" ref-type="bibr">24</xref>]. The use of wearables within a group of elderly population brings additional challenges. For example, it is very important to detect falls, which has already become a topic of particular importance in this field. For example, in [<xref rid="B25-healthcare-11-01152" ref-type="bibr">25</xref>], a framework was proposed for edge computing to detect individual&#8217;s falls using real-time monitoring by cost-effective wearable sensors. For this purpose, an IoT-based system that makes the use of big data, cloud computing, wireless sensor networks, and smart devices was developed and integrated with an LSTM model, showing very promising results for the detection of falls by elderly people in indoor circumstances. The validity and reliability of wearables have been addressed by many studies focusing on different classes of devices used to measure activity or biometric data [<xref rid="B26-healthcare-11-01152" ref-type="bibr">26</xref>,<xref rid="B27-healthcare-11-01152" ref-type="bibr">27</xref>,<xref rid="B28-healthcare-11-01152" ref-type="bibr">28</xref>,<xref rid="B29-healthcare-11-01152" ref-type="bibr">29</xref>]. Apparently, there is no consensus among researchers, as findings depend on the manufacturer, device type, and the purpose for which it was used. This is also true because devices are constantly being upgraded to new models, which suggests that their validity and reliability will improve with time.</p></sec><sec id="sec2dot2-healthcare-11-01152"><title>2.2. Contactless Measurement of Vital Signs</title><p>There are still some concerns regarding the reliability and accuracy of wearables to detect physical activity and evaluate health-related outcomes within elderly individuals, as they are generally designed primarily to collect biometric information during activities of daily living in the general population [<xref rid="B30-healthcare-11-01152" ref-type="bibr">30</xref>,<xref rid="B31-healthcare-11-01152" ref-type="bibr">31</xref>,<xref rid="B32-healthcare-11-01152" ref-type="bibr">32</xref>,<xref rid="B33-healthcare-11-01152" ref-type="bibr">33</xref>]. First, the ability of older people to recognise the need for wearables and properly use them poses new challenges. Second, the high prevalence of different diseases in this population and the heterogeneity associated with their lifestyle, needs, preferences, and health point to the need for wearable devices that are valid and reliable and that can accurately measure and monitor important signals. Additionally, taking into account the problems associated with time-inefficient work in care homes, contactless monitoring of vital signs may be beneficial for healthcare [<xref rid="B34-healthcare-11-01152" ref-type="bibr">34</xref>,<xref rid="B35-healthcare-11-01152" ref-type="bibr">35</xref>,<xref rid="B36-healthcare-11-01152" ref-type="bibr">36</xref>]. In particular, contactless measurement techniques can be applied to measure the respiratory rate and monitor the heart rate variability, which is one of the fourth most important vital parameters [<xref rid="B37-healthcare-11-01152" ref-type="bibr">37</xref>]. Monitoring the respiratory rate is useful for the recognition of psychophysiological conditions, the treatment of chronic diseases of the respiratory system, and the recognition of dangerous conditions [<xref rid="B38-healthcare-11-01152" ref-type="bibr">38</xref>,<xref rid="B39-healthcare-11-01152" ref-type="bibr">39</xref>]. Combining respiratory rate and heart rate data provides even more useful information on the condition of the cardiovascular system [<xref rid="B40-healthcare-11-01152" ref-type="bibr">40</xref>,<xref rid="B41-healthcare-11-01152" ref-type="bibr">41</xref>]. The most promising method of noncontact monitoring of the respiratory process is through infrared and near-infrared cameras [<xref rid="B42-healthcare-11-01152" ref-type="bibr">42</xref>,<xref rid="B43-healthcare-11-01152" ref-type="bibr">43</xref>]. An infrared camera is a device that can capture small temperature changes on the surface of an object and/or in the environment. This device can record the temperature fluctuations of airflow from the mouth or nose. Infrared cameras can successfully measure the respiration rate if advanced computer vision algorithms that are insensitive to constantly varying lighting and temperature conditions are applied.</p></sec><sec id="sec2dot3-healthcare-11-01152"><title>2.3. Benefits of Computer Vision Techniques</title><p>Image recognition is one of the main methods used to determine an individual&#8217;s pose and activity. The use of pose estimation technology in geriatric care offers several advantages, including the continuous monitoring of patients, early detection of potential health problems, essential data on the patient&#8217;s movements, and, in particular, the detection of extra situations (e.g., the person is lying on the ground and not moving) [<xref rid="B44-healthcare-11-01152" ref-type="bibr">44</xref>]. Pose estimation algorithms vary in complexity and accuracy, ranging from simple rule-based algorithms to more complex deep-learning-based algorithms. Simple algorithms may be faster and easier to implement but typically they are not as accurate as more complex ones. Deep-learning-based algorithms, on the other hand, may provide more accurate results but may be more computationally intensive and require large amounts of training data. Comparatively, deep-learning-based methods have shown great potential for improving the accuracy of human posture recognition, for both single individuals [<xref rid="B45-healthcare-11-01152" ref-type="bibr">45</xref>,<xref rid="B46-healthcare-11-01152" ref-type="bibr">46</xref>] and multiple individuals [<xref rid="B47-healthcare-11-01152" ref-type="bibr">47</xref>,<xref rid="B48-healthcare-11-01152" ref-type="bibr">48</xref>] in images or videos. In particular, methods such as the multisource deep model [<xref rid="B49-healthcare-11-01152" ref-type="bibr">49</xref>], the position refinement model [<xref rid="B50-healthcare-11-01152" ref-type="bibr">50</xref>,<xref rid="B51-healthcare-11-01152" ref-type="bibr">51</xref>], and the stacked hourglass network [<xref rid="B52-healthcare-11-01152" ref-type="bibr">52</xref>] have demonstrated the effectiveness of deep learning in human posture recognition. These methods use convolutional neural networks to extract features from input images and estimate the positions of human joints. However, the early detection of falls [<xref rid="B53-healthcare-11-01152" ref-type="bibr">53</xref>,<xref rid="B54-healthcare-11-01152" ref-type="bibr">54</xref>,<xref rid="B55-healthcare-11-01152" ref-type="bibr">55</xref>] is one of the most important functions of the geriatric care system as it allows prompt medical assistance to be provided and can prevent further injuries. Human fall detection systems can help to identify when a fall has occurred and alert caregivers or emergency services immediately. Therefore, various types of fall detection and prediction systems suggested in the field not only rely on image recognition techniques [<xref rid="B42-healthcare-11-01152" ref-type="bibr">42</xref>,<xref rid="B56-healthcare-11-01152" ref-type="bibr">56</xref>,<xref rid="B57-healthcare-11-01152" ref-type="bibr">57</xref>] but also employ other information sources, for example, biological factors or signals obtained by wearable devices that are more commonly used for fall risk assessments [<xref rid="B58-healthcare-11-01152" ref-type="bibr">58</xref>,<xref rid="B59-healthcare-11-01152" ref-type="bibr">59</xref>]. Although computer vision techniques have been used widely and very successfully in medicine, the monitoring and identification of patients in nursing homes should take into account the fact that image capture devices cannot always be used to track patients (e.g., hygiene rooms) according to privacy and ethical requirements [<xref rid="B60-healthcare-11-01152" ref-type="bibr">60</xref>,<xref rid="B61-healthcare-11-01152" ref-type="bibr">61</xref>]. In addition, capturing certain information with cameras may not always be possible due to changes in the environment or, for instance, in cases when the person reappears or is partially obscured by other objects, which poses the additional challenge of re-identifying the same individual. Therefore, it is important to determine which factors may be automatically recorded and tracked over time utilising image processing technology. It is also crucial that the solution is quick. As such, it is essential to carefully assess the trade-off between precision and speed in order to choose a solution that meets the specific requirements of the application.</p></sec></sec><sec id="sec3-healthcare-11-01152"><title>3. Materials and Methods</title><p>The proposed solution includes (1) an IoT module with integrated wearable and contactless devices; (2) an AI module that utilises deep learning architectures for the image recognition of patient posture and activity; and (3) a decision support module for generating the patient-personalised nursing care plan.</p><p>An IoT module has been developed to monitor and transfer data in real time. It consists of sensors connected to an Arduino microprocessor to monitor the patient&#8217;s vital signs. This module integrates not only body-worn devices that are networked but also a number of remote devices for monitoring health data. In general, such devices can collect and transmit the collected data, such as heart rate, body temperature, and physical activity, to a remote system or application, usually through wireless connectivity (e.g., Bluetooth, Wi-Fi). Some wearable health devices also have built-in sensors and algorithms that can perform basic health assessments, such as tracking sleep patterns, counting steps taken, and estimating calorie expenditure.</p><p>In this study, four IoT devices, a Fitbit wristband, smart scale, smart blood pressure device, and a camera, were used to monitor the health of elderly patients in a nursing home (<xref rid="healthcare-11-01152-t001" ref-type="table">Table 1</xref>). Data collected from these devices were sent to the server and processed to obtain the final decision (<xref rid="healthcare-11-01152-f001" ref-type="fig">Figure 1</xref>).</p><p>A patient room in a hospital for the elderly was equipped with cameras to continuously monitor the status of the patients in real time. The video footage from these cameras was sent directly to a server where it was stored, processed, and analysed using image processing algorithms. This was necessary to monitor patients&#8217; motor activity, changes, or progress in movement and consequently make the necessary changes to the care plan or react in emergency situations such as falls, pressure sores, etc. In parallel to the cameras, the patients were also given Fitbit wristbands for the additional monitoring of physiological parameters. These wristbands were equipped with sensors to monitor the patient&#8217;s vital signs, such as heart rate and respiratory rate. The data from the Fitbit bracelets were sent to Google Cloud and then to a server using APIs. The geriatric nurse also used specialised equipment to monitor the patients&#8217; weight and blood pressure. Withings body+ connected scales make it easy to monitor weight, BMI, fat, water, and body mass, which is later automatically synchronised with the smartphone via Wi-Fi or Bluetooth. In particular, monitoring the following parameters is important for patients at risk of complications such as high blood pressure, diabetes, etc.</p><p>One of the main limitations is that off-the-shelf IoT devices do not offer the option of sending data directly to a third-party server. As a result, all data must first pass through the provider&#8217;s cloud services and use their API. This also leads to software limitations, such as only allowing one IoT device of a certain type per account, making the data collection pipeline more complex than is necessary.</p><p>Data captured by all smart devices not only digitalise the tracking of key physiological parameters but also enable the investigation of dependencies between these indicators and a patient&#8217;s health status or its change, but only when a statistically reliable sample is collected. If computer-vision-based health monitoring is involved, real-time visual information collection must include data storage and analysis [<xref rid="B44-healthcare-11-01152" ref-type="bibr">44</xref>,<xref rid="B62-healthcare-11-01152" ref-type="bibr">62</xref>,<xref rid="B63-healthcare-11-01152" ref-type="bibr">63</xref>]. For the experiment, data collection started on 15 September 2022 and data were uploaded to the server Dell PowerEdge R7525 (AMD EPYC 7452 32-Core Processor/2350 MHz; 512 GB RAM; NVIDIA GA100 [A100 PCIe 40 GB], 2 &#215; 450 GB SSD; 2 &#215; 25 Gbps LAN MT27800 Family [ConnectX-5] 2 &#215; 100 GBps [ConnectX-6]). In total, 1.412 TB of data were accumulated during the observation period between 15 September 2022 and 28 December 2022. In addition to the data collected from the IoT devices, the system also allowed manual input from healthcare personnel. This included additional parameters that were not captured by the IoT devices, such as bedsores, changes in eating habits, changes in bowel movements, etc. These data were entered into an Excel spreadsheet by healthcare professionals and then automatically uploaded to the database.</p><p>By continuously monitoring a patient, wearable health devices can provide a more comprehensive view of a patient&#8217;s health status. However, it is important to ensure that the system is secure, respects the patient&#8217;s privacy, and complies with relevant regulations and standards [<xref rid="B64-healthcare-11-01152" ref-type="bibr">64</xref>,<xref rid="B65-healthcare-11-01152" ref-type="bibr">65</xref>]. However, it has been observed that wearable gadgets are frequently taken off and thrown away for either purposeful or unintentional reasons, so a balance needs to be struck between functionality, dependability, and cost. This is a common issue with wearable health monitoring devices, particularly among patients with dementia, who may forget where they have placed their device or may not understand the importance of wearing it consistently.</p><p>Non-contact monitoring of vital signs using cameras and image recognition techniques is a promising area of development in healthcare technology and has the potential to improve the accessibility, efficiency, and cost-effectiveness of vital sign monitoring. The use of AI-based image recognition algorithms, mainly deep learning architectures, allows images to be automatically analysed to assess vital signs.</p><p>YOLOv3 (You Only Look Once, Version 3) [<xref rid="B66-healthcare-11-01152" ref-type="bibr">66</xref>] is a real-time object detection algorithm that allows specific objects to be identified in videos. YOLOv3 uses a variant of the Darknet neural network architecture, specifically Darknet-53 as its backbone network. The architecture consists of 53 convolutional layers, which was trained on the ImageNet dataset, which was designed for computer vision research [<xref rid="B67-healthcare-11-01152" ref-type="bibr">67</xref>]. YOLOv3 also contains several key features that help to improve the detection accuracy and performance, including residual skip connections, upsampling, and multiscale detection. The most important feature of the algorithm is that it performs detection at three different scales by downsampling the dimensions of the input image by factors of 32, 16, and 8, respectively (see <xref rid="healthcare-11-01152-f002" ref-type="fig">Figure 2</xref>).</p><p>The AlphaPose algorithm allows us to detect keypoints in the bodies of several people with high accuracy in real-time video or images. The 17 keypoints detected by AlphaPose include the nose, eyes, ears, shoulders, elbows, wrists, hips, knees, and ankles (see <xref rid="healthcare-11-01152-f003" ref-type="fig">Figure 3</xref>). As the <xref rid="healthcare-11-01152-f003" ref-type="fig">Figure 3</xref> shows, the algorithm can successfully detect the following keypoints in video footage of a patient in a movement position. All of these keypoints are used to construct a human body skeleton representation, which can be used for various applications such as activity estimation [<xref rid="B68-healthcare-11-01152" ref-type="bibr">68</xref>], process recognition [<xref rid="B69-healthcare-11-01152" ref-type="bibr">69</xref>], and human fall detection in different environments [<xref rid="B54-healthcare-11-01152" ref-type="bibr">54</xref>,<xref rid="B55-healthcare-11-01152" ref-type="bibr">55</xref>,<xref rid="B70-healthcare-11-01152" ref-type="bibr">70</xref>,<xref rid="B71-healthcare-11-01152" ref-type="bibr">71</xref>].</p><p>In particular, a decision support system relies primarily on the expert knowledge of geriatric staff nurses who are experienced in developing nursing plans for patients with different health problems. Their expertise has been used to create the rules that guide the decisions made by nursing professionals which, in this case, are mapped into the output of how to proceed with the nursing plan. Individual experts suggest different decisions based on critical factors in certain cases, so it would seem reasonable to use Fuzzy logic or Neuro-fuzzy models, which are more similar to human thinking. However, given that most of the input variables are of the verbal and integer type, the use of such models will not be efficient. In addition, we do not have enough statistical data to create mappings between numerical values and verbal estimates (e.g., Breathing: Increased &#8594; <italic toggle="yes">X</italic> breaths per minute) and to create fuzzy sets based on this. Therefore, we decided to rely on the Decision Tree supervised learning approach which can handle both numeric and non-numeric values, has fast decision times, enables parameter optimisation, and has the possibility of refinement if the accuracy of the result is not satisfactory (e.g., Random Forest). In the decision support module, a Decision Tree with a Gini impurity value was used, and a prepruning process was applied to prevent overfitting. The Gini impurity value is given by
<disp-formula><mml:math id="mm1" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>G</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>c</mml:mi></mml:munderover><mml:msubsup><mml:mi>p</mml:mi><mml:mi>j</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is a proportion of observations that belong to class <italic toggle="yes">c</italic> for a particular node.</p><p>The fine-tuning of Decision Tree hyperparameters involves a depth limited to a maximum of 3 and a minimum number of samples equal to 6 in a finite node. An average classification error of 92% was achieved.</p><p>For patient reidentification, the study made use of the Bag of Visual Words (BOVW), since it has been proven to be successful in a number of computer vision tasks, including human reidentification and human action classification [<xref rid="B72-healthcare-11-01152" ref-type="bibr">72</xref>,<xref rid="B73-healthcare-11-01152" ref-type="bibr">73</xref>,<xref rid="B74-healthcare-11-01152" ref-type="bibr">74</xref>]. With the BOVW approach, local features (such as SIFT descriptors) from images are first extracted and then grouped into a visual vocabulary. Each image is then represented as a histogram of visual words, which may be used for classification or retrieval tasks using machine learning algorithms. More specifically, the K-means algorithm was trained using the final list of features that were retrieved from patient images. As a result, the features were grouped into visual words. Finally, a ML-based classifier was used to generate a categorisation of images based on a newly created vocabulary.</p><sec><title>Performance Metrics</title><p>The <italic toggle="yes">F</italic>1 score is a metric that is widely used to evaluate the performance of a classification model. For a multiclass classification, the <italic toggle="yes">F</italic>1 score for each class is calculated using the one-against-rest (OvR) method. In this approach, the metric for each class is determined separately. However, rather than assigning multiple <italic toggle="yes">F</italic>1 scores to each class, it is more common to take an average and obtain a single value to measure the overall performance. Three types of averaging methods are commonly used to calculate <italic toggle="yes">F</italic>1 scores in a multiclass classification, but only two of them are recommended for unbalanced data, as in our case. More specifically, macroaveraging calculates the <italic toggle="yes">F</italic>1 score for each class separately and derives an unweighted average of these scores. This means that each class is treated equally, regardless of the number of samples it contains. The macroaveraging <italic toggle="yes">F</italic>1 score is given by
<disp-formula><mml:math id="mm3" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>r</mml:mi><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>v</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mspace width="0.166667em"/><mml:mi>F</mml:mi><mml:mn>1</mml:mn><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup><mml:mspace width="4pt"/><mml:mi>F</mml:mi><mml:msub><mml:mn>1</mml:mn><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mi>n</mml:mi></mml:mfrac><mml:mo>,</mml:mo><mml:mspace width="1.em"/></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic toggle="yes">n</italic> is the number of classes. In contrast, a weighted averaging calculates the <italic toggle="yes">F</italic>1 score for each class separately and then takes the weighted average of these scores, where the weight for each class is proportional to the number of samples in that class. In this case, the <italic toggle="yes">F</italic>1 result is biased towards the larger classes, i.e.,
<disp-formula><mml:math id="mm4" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>W</mml:mi><mml:mi>e</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>v</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mspace width="0.166667em"/><mml:mi>F</mml:mi><mml:mn>1</mml:mn><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup><mml:mspace width="4pt"/><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#215;</mml:mo><mml:mi>F</mml:mi><mml:msub><mml:mn>1</mml:mn><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mi>n</mml:mi></mml:mfrac><mml:mo>,</mml:mo><mml:mspace width="1.em"/></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:msub><mml:mi>k</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mi>N</mml:mi></mml:mfrac></mml:mrow></mml:mrow></mml:math></inline-formula> is the weight of the class <italic toggle="yes">i</italic>, <italic toggle="yes">N</italic> is the total number of samples, and <inline-formula><mml:math id="mm6" overflow="scroll"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the number of samples in the class <italic toggle="yes">i</italic>.</p></sec></sec><sec sec-type="results" id="sec4-healthcare-11-01152"><title>4. Results</title><sec id="sec4dot1-healthcare-11-01152"><title>4.1. Implementation of the Geriatric Care System</title><p>The geriatric care plan system for end-users, i.e., nursing home staff, was created using C# programming language and the ASP.NET Core 6.0 framework for the back-end. The front-end was built using Node.js version&#160;19 and the Angular framework, while testing was carried out using Karma. PostgreSQL was used as an open-source relational database management system. The use of these technologies allowed developers to create a robust and scalable system that was able to handle the large amounts of data generated by IoT devices. In addition, Docker was used to containerise the software for deployment by combining the system and all of its dependencies into a single container that could be quickly deployed on any platform that is compatible with Docker. The architecture of the system is demonstrated in <xref rid="healthcare-11-01152-f004" ref-type="fig">Figure 4</xref>.</p><p>Wearable gadgets synchronise the data with cloud servers, since the data they generate needs to be processed and analysed. Once the data have been received by the cloud servers, the company&#8217;s server pulls the data from the Google cloud servers using API and then parses the files and saves information in the Postgre database. In contrast, the data captured by the cameras are sent directly to the server. This dataset is then processed in the back-end and analysed alongside the wearable data in order to provide a more comprehensive view of a patient&#8217;s health status. The main purpose of .NET backend is to act as a bridge, passing data between the Angular front-end and the Postgre database. The back-end is written using REST API methodology to provide a standardised way for different applications or devices to communicate.</p></sec><sec id="sec4dot2-healthcare-11-01152"><title>4.2. AI-Based Data Analytics and Decision Making</title><p>To prepare a nursing care plan, a rich set of data is collected about the patient, as summarised in <xref rid="healthcare-11-01152-t002" ref-type="table">Table 2</xref>. Then, the recommendations for the actions to be taken in a nursing plan are generated from the geriatric care management system.</p><p>For demonstration purposes, the collected data were analysed to detect possible dependencies. The radar graph below (see <xref rid="healthcare-11-01152-f005" ref-type="fig">Figure 5</xref>) is a single patient&#8217;s chart of selected vital signs over a 50-day observation period, displaying SBP (systolic blood pressure), DBP (diastolic blood pressure), HR (heart rate), SPO2 (oxygen saturation), sleeping hours, and weight measurements. The data analysis was carried out on three patients on the ward, but no significant dependencies between variables were identified. It may be assumed that that some trends could be determined if the data were gathered over a longer period of time and additional variables, such as pain level, temperature, and even verbal type indicators, were included.</p><p>The geriatric care personnel was responsible for writing the rules for the care plans. These rules were based on best practise and experience in the field and were designed to ensure that patients receive the most appropriate and effective care. More specifically, care plans were tailored to the specific needs of current and future patients, taking into account their medical history, current condition, and other relevant factors. The variables listed in <xref rid="healthcare-11-01152-t002" ref-type="table">Table 2</xref> were included in the care plan, as they indicate the patient&#8217;s medical history, current health status, and other relevant factors that can influence treatment. On the basis of this information, the initial set of rules covered a wide range of scenarios and options, but after optimisation, the patient care plan eventually consisted of 61 rules with the four possible outputs of the care plan: &#8220;Continue current treatment&#8221;, &#8220;Monitor&#8221;, &#8220;Adjust&#8221;, or &#8220;Extra situation&#8221; (see <xref rid="healthcare-11-01152-f006" ref-type="fig">Figure 6</xref>). All remaining cases that were not included in the rules were assigned to the care plan &#8220;Continue current treatment&#8221; by default.</p><p>In particular, the nursing care plan was designed to be flexible and adaptable to allow healthcare professionals to adjust the patient&#8217;s geriatric care according to his or her health status and changing needs. Those rules and the output generated by the geriatric care management system help healthcare personnel to respond more quickly to changes in a patient&#8217;s health, shape the patient-personalised geriatric care, reduce the risk of human error, and make better use of staff time by concentrating more on essential social support.</p><p><xref rid="healthcare-11-01152-f007" ref-type="fig">Figure 7</xref> shows a schema for an AI-based decision support system. Four of the 21 variables (see <xref rid="healthcare-11-01152-t002" ref-type="table">Table 2</xref>) are automatically registered; that is, three of them were retrieved from IoT devices and one (change in movement) was obtained from the camera. The value of the latter variable was generated from the AI-based image recognition module. The remaining variables were taken from the MS Excel spreadsheet file, where all data were entered manually.</p><sec><title>Image Recognition Solution</title><p>An AI-based image recognition module is a block consisting of several sequential algorithms that detects changes in the movement of a patient. In this project, we used the camera to film nursing home patients, that is, one room with three patients. The video was recorded at 1920&#160;&#215;&#160;1080 pixel resolution with a frequency of 10 FPS, therefore storing 10 unique images per second to obtain 10FPS&#160;&#215;&#160;60&#160;=&#160;600 images per minute. An image was analysed every five seconds with the assumption that no significant changes in motion would be detected in that time period.</p><p>The image processing included</p><list list-type="bullet"><list-item><p>Brightening: to increase the overall luminosity of the image, improve visibility, and increase the clarity of the image during low light conditions;</p></list-item><list-item><p>Cropping: to keep only regions of interest in the image;</p></list-item><list-item><p>Denoising: to remove noise from the image, typically by applying a low-pass filter. It also improved the quality and clarity of the image by removing noise, which could be especially useful if the image was taken under poor conditions or with a low-quality&#160;camera.</p></list-item><list-item><p>Edge detection: to identify edges in the image by finding points of a rapid intensity change. It can also be used to identify and extract features or objects in the image, such as lines, shapes, or boundaries.</p></list-item></list><p>After image processing, the algorithm integrating YOLOv3 and AlphaPose [<xref rid="B75-healthcare-11-01152" ref-type="bibr">75</xref>] was used to detect human poses. The algorithm includes the three main components [<xref rid="B76-healthcare-11-01152" ref-type="bibr">76</xref>]. First, the Symmetric Spatial Transformer Network (SSTN) takes the detected bounding boxes to generate pose proposals. The SSTN allows the spatial context and correlations between the keypoints to be captured, leading to more accurate pose estimates. Second, the Parametric Pose Non-Maximum-Suppression (NMS) is a component that is used to remove redundant pose detections and improve the overall accuracy of the pose estimation. Finally, the Pose-guided Proposals Generator is used to create a large sample of training proposals with the same distribution as the output of the human detector.</p><p>The next step is the problem of identifying and classifying patient postures, which in this case, included the following six postures: &#8220;walking&#8221;, &#8220;standing&#8221;, &#8220;sitting&#8221;, &#8220;fallen on the ground&#8221;, &#8220;lying in bed&#8221;, and &#8220;sleeping&#8221;. For the verification of all poses, a sequence of three images was taken for a period of 15 s, except for the last two poses. The poses of &#8220;sleeping&#8220; and &#8220;laying in bed&#8220; correlate with the parameters of the smart bracelet (sleep time and heart rate), so these parameters were also assessed. If the patient was found to be lying in bed, the assessment time was extended by up to one minute to identify whether the patient was &#8220;lying in bed&#8221; or &#8220;sleeping&#8221;. In particular, the pose was assessed every minute until a new pose was captured.</p><p>A pose change algorithm was developed to detect differences between adjacent images, that is, to identify that a person was walking rather than standing or that a person was just lying on a bed rather than sleeping. <xref rid="healthcare-11-01152-f008" ref-type="fig">Figure 8</xref> illustrates the example of three iterations of assessment frames of the &#8220;walking&#8221; pose taken every five seconds. Comparing the images taken every five seconds, we can see that the pose remained the same, although the frames were not identical and the patient&#8217;s coordinates varied.</p><p>In order to define changes in movement habits, an additional algorithm (see Algorithm&#160;1) was created to evaluate movement changes over a longer period of time <inline-formula><mml:math id="mm7" overflow="scroll"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>&#8722;</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, where <italic toggle="yes">m</italic> is a current time moment, <italic toggle="yes">n</italic> is a number of days before <inline-formula><mml:math id="mm8" overflow="scroll"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, 1 &#8804; n &#8804; 3. This algorithm calculates the duration (hours) in each pose per day. The percentage change is then evaluated, compared with threshold value <inline-formula><mml:math id="mm9" overflow="scroll"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and a response is generated that includes three possible values: &#8220;Unchanged&#8221;, &#8220;Slowed down&#8221;, or &#8220;Increased&#8221;. The pseudocode of the algorithm is provided below.
<array orientation="portrait"><tbody><tr><td align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1"><bold>Algorithm 1</bold>&#160;Evaluation of changes in movement habits</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><list list-type="simple"><list-item><p><inline-formula><mml:math id="mm10" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>c</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mi>H</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>W</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>k</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>h</mml:mi><mml:mi>r</mml:mi><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>S</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>h</mml:mi><mml:mi>r</mml:mi><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>S</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>h</mml:mi><mml:mi>r</mml:mi><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><p><inline-formula><mml:math id="mm11" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>12.5</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><p><inline-formula><mml:math id="mm12" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mo>(</mml:mo><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>&#8722;</mml:mo><mml:mi>b</mml:mi><mml:mo>)</mml:mo><mml:mo>/</mml:mo><mml:mi>b</mml:mi><mml:mo>)</mml:mo><mml:mo>*</mml:mo><mml:mn>100</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><p><bold>if</bold>&#160;<inline-formula><mml:math id="mm13" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:mi>A</mml:mi><mml:mi>c</mml:mi></mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>H</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mi>c</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mi>H</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#8804;</mml:mo><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>&#8743;</mml:mo><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:mi>A</mml:mi><mml:mi>c</mml:mi></mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>H</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mi>c</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mi>H</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#8804;</mml:mo><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>&#160;<bold>then</bold></p></list-item><list-item><p>&#160;&#160;&#160;&#160;<inline-formula><mml:math id="mm14" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>o</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>H</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>&#160;is slowed down</p></list-item><list-item><p><bold>else if</bold>&#160;<inline-formula><mml:math id="mm15" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:mi>A</mml:mi><mml:mi>c</mml:mi></mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>H</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mi>c</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mi>H</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#8805;</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>&#8743;</mml:mo><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:mi>A</mml:mi><mml:mi>c</mml:mi></mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>H</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mi>c</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mi>H</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#8805;</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>&#160;<bold>then</bold></p></list-item><list-item><p>&#160;&#160;&#160;&#160;<inline-formula><mml:math id="mm16" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>o</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>H</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>&#160;is increased</p></list-item><list-item><p><bold>else</bold>&#160;<inline-formula><mml:math id="mm17" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>o</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>H</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>&#160;is unchanged</p></list-item><list-item><p><bold>end if</bold></p></list-item></list></td></tr></tbody></array></p><p>For demonstration purposes, the identification of tough poses observed in the real-world environment is shown below. For instance, <xref rid="healthcare-11-01152-f009" ref-type="fig">Figure 9</xref> shows a skeleton-based posture recognition in various lighting environments. In well-illuminated areas, patients can be detected by identifying all skeleton keypoints (<xref rid="healthcare-11-01152-f009" ref-type="fig">Figure 9</xref>c). It has been observed that at night or at twilight/night, walking patients can be identified quite accurately with all keypoints (<xref rid="healthcare-11-01152-f009" ref-type="fig">Figure 9</xref>a,b), but when patients are sleeping with their blankets, few keypoints were successfully detected (<xref rid="healthcare-11-01152-f009" ref-type="fig">Figure 9</xref>d) or keypoints were not detected at all (<xref rid="healthcare-11-01152-f009" ref-type="fig">Figure 9</xref>a).</p><p>Another example demonstrates a skeleton-based posture recognition for two different scenarios. In <xref rid="healthcare-11-01152-f010" ref-type="fig">Figure 10</xref>a, the keypoints in the patient&#8217;s body were detected when the patient was lying on the ground, which refers to the status &#8220;falling on the ground&#8221;. To correctly recognise this pose, a training dataset with artificially simulated falling poses was created. Comparatively, <xref rid="healthcare-11-01152-f010" ref-type="fig">Figure 10</xref>b shows that the keypoints in the body were identified for all persons located in the ward, but the nursing personnel needed to be the exception. Therefore, additional data were collected to train a deep learning algorithm to distinguish staff from patients. Consequently, the nursing personnel was identified by their clothes, more specifically, white trousers and a blue top, which they had to always wear.</p></sec></sec><sec id="sec4dot3-healthcare-11-01152"><title>4.3. Experimental Results</title><p>A posture detection algorithm of captured video material was tested to identify six different poses. The results are summarised in a confusion matrix to evaluate the performance of the algorithm. More specifically, the confusion matrix provides a visual representation of the number of correct and incorrect predictions made by the classifier: the rows represent the actual class labels, while the columns represent the predicted class labels. The diagonal elements show the number of correct predictions (see <xref rid="healthcare-11-01152-f011" ref-type="fig">Figure 11</xref>).</p><p>The posture recognition algorithm was trained using 9300 labelled images and tested using 3792 images. An average posture recognition accuracy of 91.63% was achieved for the testing data set (<xref rid="healthcare-11-01152-f011" ref-type="fig">Figure 11</xref>). Posture labelling was performed manually on the images obtained from the video stream for training and testing purposes. The Receiver Operating Characteristic (ROC) curve of the stratified testing dataset is provided in <xref rid="healthcare-11-01152-f012" ref-type="fig">Figure 12</xref>.</p><p>The AUC values for each posture class ranged from 0.8790 to 0.9427, with the highest value obtained for the sitting posture class. The sleeping and lying in bed posture classes resulted in the lowest AUC values, with values of 0.9047 and 0.8790, respectively. These lower values suggest that it might be more difficult for the classifier to distinguish between these postures and others. Comparatively, the AUC value for the fallen on the ground posture class was 0.9177, which is slightly lower than those of the other more successfully recognised posture classes. This could be due to the lack of training data for this posture, which might have led to lower accuracy. Next, <xref rid="healthcare-11-01152-t003" ref-type="table">Table 3</xref> summarises the estimated values of precision, recall, and <italic toggle="yes">F</italic>1 score for each class of interest, together with macro and weighted <italic toggle="yes">F</italic>1 scores for the evaluation of the overall performance of the posture recognition algorithm.</p><p>The patient re-identification testing results are summarised in <xref rid="healthcare-11-01152-f013" ref-type="fig">Figure 13</xref>. The support vector machine (SVM) method was used to generate categories of images, providing labels for the patient classes. In our case, the maximum number of classes was set to four: three classes represented the maximum number of patients the ward can accommodate, while the separate class &#8220;None&#8221; referred to unauthorised individuals such as nursing staff, family members, doctors, or others. The class names for patients were labelled &#8220;First&#8221;, &#8220;Second&#8221;, and &#8220;Third&#8221; (see <xref rid="healthcare-11-01152-f013" ref-type="fig">Figure 13</xref>).</p><p>The confusion matrix in <xref rid="healthcare-11-01152-f013" ref-type="fig">Figure 13</xref> summarises how successfully the algorithm identifies three ward patients in common areas. One can observe that an accuracy level of 90% was obtained for the &#8220;First&#8221; class, a value of 88% was obtained for the &#8220;Second&#8221; class, and a value of 91% was obtained for the &#8220;Third&#8221; class. Although the lowest accuracy level of 87% was achieved in the "None&#8221; class, considering that there can be around 6&#8211;13 people in a single tray, this is a pretty good accuracy level. It was observed that female patients and nursing home staff were more easily recognised, but other patients, nonmedical nursing home staff, and visiting relatives were the most confused with these patients.</p><p>Finally, to conduct a real-time experiment, patient positioning verification was carried out. This included 16 scenarios with diverse positions. The results are summarised in <xref rid="healthcare-11-01152-t004" ref-type="table">Table 4</xref>. Two prediction errors were determined. More specifically, the patient was &#8220;lying in the bed&#8221;, but he was detected as &#8220;sleeping&#8221;, as he was covered up, his heart rate was reduced, he did not move for more than one minute, and it was night time. Another prediction error also related to the sleeping pose. The patient was lying in the bed without covering up; however, it was determined that he was not sleeping based on readings from the smart wristband. It should be noted that the prediction may also be impacted by ambient light conditions. From a technical perspective, the proposed system performed pose estimation with an average output time of 182 ms, including the algorithm used to predict the pose from the possible outcomes.</p><p>To test the correctness of the output of the geriatric care management system, different scenarios involving nursing home staff were developed. The results revealed that the system provided the correct output in all cases. The system was designed to generate changes to the treatment plan immediately after any changes are made. When a healthcare professional makes a change to the care plan, the system analyses the data from the patient&#8217;s IoT devices and determines the appropriate course of action. The system then automatically updates the results of the action to be taken for the individual patient and alerts the healthcare professional. This allows healthcare professionals to stay up-to-date with the patient&#8217;s condition and make any necessary adjustments to their treatment in a timely manner.</p></sec></sec><sec sec-type="discussion" id="sec5-healthcare-11-01152"><title>5. Discussion</title><p>There are a few areas for improvement, as the proposed geriatric management care system is still in its initial stage of functioning. Personality identification, which relates to the continuous contactless assessment of the patient, is the most challenging concern. Comparatively, wearable devices do not raise any questions at the moment; their purpose is clear, but elderly people have a problem with wearing them because they find them annoying. The creation of the nursing care plan itself could be fully automated later on, with a follow-up on what action should be taken when the situation changes. However, to fully automate it, a lot of statistical data are needed, including actions taken by nurses, from which the system could be learnt, that is, from the actions taken by the care worker on each individual situation. Taking into account the current data (<xref rid="healthcare-11-01152-t002" ref-type="table">Table 2</xref>), there are at least 20,155,392,000 possible combinations of parameters that define the health condition, which are likely to increase in the future due to the inclusion of additional parameters. For this purpose, a list of actions is provided in the geriatric care management system, from which the care worker must indicate (select from the list) what they intend to do. In this way, a dataset of situations and decisions with all the actions taken accordingly is continuously accumulated. Once a representative sample of data has been accumulated (say after at least one year), the correctness of the automated action is improved.</p><p>The challenge with consistent and accurate patient identification makes it reasonable to consider other methods of individual identification than BOVW. As patients usually stay in their own wards, the accuracy of identification is high when the patients are present and nursing staff visit them a certain number of times per day. However, the accuracy drops in common areas (e.g., resting, eating) because there are more patients and personnel present. Mainly because of their distinguishing clothes, nursing workers are simpler to identify (see column &#8220;None&#8221; in <xref rid="healthcare-11-01152-f014" ref-type="fig">Figure 14</xref>). However, the elderly patients themselves are more likely to be confused with each other in common areas, with a best individual identification result of 0.914 achieved (see <xref rid="healthcare-11-01152-f014" ref-type="fig">Figure 14</xref>).</p><p>As an alternative method, gait recognition (GR) technology can be used for patient identification. This method examines the uniqueness of an individual&#8217;s walking or running pattern using machine learning (ML) techniques [<xref rid="B77-healthcare-11-01152" ref-type="bibr">77</xref>]. More specifically, ML algorithms are trained to recognize subtle differences in a person&#8217;s gait and thus can use this information to identify individuals even if their face is obscured in the image [<xref rid="B78-healthcare-11-01152" ref-type="bibr">78</xref>,<xref rid="B79-healthcare-11-01152" ref-type="bibr">79</xref>]. An additional benefit of GR technology is that gait information can be used not only for personal identification, but also for medical purposes, such as monitoring and for the diagnosis and treatment of various movement disorders [<xref rid="B80-healthcare-11-01152" ref-type="bibr">80</xref>,<xref rid="B81-healthcare-11-01152" ref-type="bibr">81</xref>]. For example, gait recognition technology can be used to identify and diagnose various types of neurodegenerative diseases (such as Parkinson&#8217;s and Alzheimer&#8217;s disease) or assess the course of disease [<xref rid="B82-healthcare-11-01152" ref-type="bibr">82</xref>,<xref rid="B83-healthcare-11-01152" ref-type="bibr">83</xref>,<xref rid="B84-healthcare-11-01152" ref-type="bibr">84</xref>,<xref rid="B85-healthcare-11-01152" ref-type="bibr">85</xref>]. This can help doctors and healthcare professionals to develop more effective nursing care plans and interventions as well as monitor the progression of these conditions over time. However, GR technology usually requires a variety of sources or capture devices to gather data about an individual&#8217;s gait, including multiple video cameras, motion sensors, radars, and other specialized equipment [<xref rid="B79-healthcare-11-01152" ref-type="bibr">79</xref>,<xref rid="B86-healthcare-11-01152" ref-type="bibr">86</xref>]. In addition, the accuracy of gait recognition technology can be affected by a range of factors, including the angle at which the gait is captured.</p><p>Finally, it should be noted that elderly people are choosing to live independently at home for as long as possible. In such cases, intelligent geriatric care management system monitoring adapted to the individual home and operating remotely can be very helpful for ensuring that the elderly person is safe and providing faster reactions to emergencies (i.e., fall detection) and appropriate care. In the near future, we plan to develop the necessary software and hardware package (e.g., for the proper functioning of the system such as a stable internet connection) for the home care services and to test it in real-world environment with the possibility of transmitting the data to the responsible physician for monitoring.</p></sec><sec sec-type="conclusions" id="sec6-healthcare-11-01152"><title>6. Conclusions</title><p>In this study, a geriatric care management system based on IoT and AI algorithms was proposed to monitor some of the most important vital signs in a noncontact manner and to facilitate the adjustment of the care plan. The system provides an intelligent assistance function, which suggests how to proceed with the patient&#8217;s care plan based on the available data and the decision support module.</p><p>A built-in posture recognition algorithm allows staff to react quickly to extreme situations, which are highly expected at night or during peak working hours. Another algorithm was developed to monitor changes in a patient&#8217;s movement habits over a longer period of time, which can be important for detecting health problems more quickly and taking appropriate action. This is a value-added functionality of the system, as it is very difficult for nursing staff to do this in a natural way, as it is not possible to monitor every patient 24 h a day without smart technology. During this study, it was observed that the most confusing poses are &#8220;lying in bed&#8221; and &#8220;sleeping&#8221;. Detecting the individual or pose when the patient is fully or partially occluded is also quite challenging. However, capturing the pulse and sleep mode and combining these indicators with the outputs of the image recognition algorithms resulted in better detection of the &#8220;sleeping&#8221; and &#8220;lying in bed&#8221; poses, i.e., the accuracy was improved by around 15.48% and 22.06%, respectively. Additionally, the system is resistant to data deficiencies; if certain data are not received at the current time, the value is taken from the last time of recording. In any case, the final decision is made by the human, and in case of error or incorrect output, one has the opportunity to correct it.</p><p>Other concerns are ensuring that smart health monitoring devices are worn and maintained at all times, as patients often want to remove devices (particularly patients with a dementia), and nursing staff do not always notice quickly when the devices need to be loaded. Therefore, the involvement of care specialists is crucial to ensure the system operates effectively and efficiently. In addition, it is equally important to make sure that patients feel comfortable and moreover that their privacy and trust in smart technologies are maintained at the appropriate level. By involving nursing staff in the implementation process, they can provide valuable feedback, suggestions, and ideas, leading to a better overall outcome.</p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#8217;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, A.P.-T., J.S. and J.B.; methodology, A.P.-T., J.B.; software, J.S., S.N., M.O. and T.P.; validation, J.S., M.O., S.N. and T.P.; formal analysis, A.P.-T., J.S., K.S. and A.R.; investigation, A.P.-T., J.S., K.S., A.R. and J.B.; resources, J.B., A.P.-T. and A.R.; data curation, A.P.-T., J.B., J.S., S.N. and A.R.; writing&#8212;original draft preparation, A.P.-T., J.S., K.S., S.N.; writing&#8212;review and editing, A.P.-T., K.S. and J.S.; visualization, A.P.-T. and K.S; supervision, A.P.-T. and J.B.; project administration, J.B.; funding acquisition, J.B. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>The research study was reviewed and approved by the Kaunas Region Biomedical Research Ethics Committee (No. BE-2-24).</p></notes><notes><title>Informed Consent Statement</title><p>Informed consent was obtained from patients&#8217; relatives/caregivers involved in the study.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>The data are not publicly available due to privacy restrictions.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflict of interest.</p></notes><ref-list><title>References</title><ref id="B1-healthcare-11-01152"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ellis</surname><given-names>G.</given-names></name><name name-style="western"><surname>Sevdalis</surname><given-names>N.</given-names></name></person-group><article-title>Understanding and improving multidisciplinary team working in geriatric medicine</article-title><source>Age Ageing</source><year>2019</year><volume>48</volume><fpage>498</fpage><lpage>505</lpage><pub-id pub-id-type="doi">10.1093/ageing/afz021</pub-id><pub-id pub-id-type="pmid">30855656</pub-id></element-citation></ref><ref id="B2-healthcare-11-01152"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Elliott</surname><given-names>M.N.</given-names></name><name name-style="western"><surname>Beckett</surname><given-names>M.K.</given-names></name><name name-style="western"><surname>Cohea</surname><given-names>C.</given-names></name><name name-style="western"><surname>Lehrman</surname><given-names>W.G.</given-names></name><name name-style="western"><surname>Russ</surname><given-names>C.</given-names></name><name name-style="western"><surname>Cleary</surname><given-names>P.D.</given-names></name><name name-style="western"><surname>Giordano</surname><given-names>L.A.</given-names></name><name name-style="western"><surname>Goldstein</surname><given-names>E.</given-names></name><name name-style="western"><surname>Saliba</surname><given-names>D.</given-names></name></person-group><article-title>The hospital care experiences of older patients compared to younger patients</article-title><source>J. Am. Geriatr. Soc.</source><year>2022</year><volume>70</volume><fpage>3570</fpage><lpage>3577</lpage><pub-id pub-id-type="doi">10.1111/jgs.18003</pub-id><pub-id pub-id-type="pmid">35984089</pub-id><pub-id pub-id-type="pmcid">PMC10087850</pub-id></element-citation></ref><ref id="B3-healthcare-11-01152"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Reber</surname><given-names>K.C.</given-names></name><name name-style="western"><surname>Lindlbauer</surname><given-names>I.</given-names></name><name name-style="western"><surname>Schulz</surname><given-names>C.</given-names></name><name name-style="western"><surname>Rapp</surname><given-names>K.</given-names></name><name name-style="western"><surname>K&#246;nig</surname><given-names>H.H.</given-names></name></person-group><article-title>Impact of morbidity on care need increase and mortality in nursing homes: A retrospective longitudinal study using administrative claims data</article-title><source>BMC Geriatr.</source><year>2020</year><volume>20</volume><elocation-id>439</elocation-id><pub-id pub-id-type="doi">10.1186/s12877-020-01847-7</pub-id><pub-id pub-id-type="pmid">33129263</pub-id><pub-id pub-id-type="pmcid">PMC7603768</pub-id></element-citation></ref><ref id="B4-healthcare-11-01152"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Dall&#8217;Ora</surname><given-names>C.</given-names></name><name name-style="western"><surname>Griffiths</surname><given-names>P.</given-names></name><name name-style="western"><surname>Hope</surname><given-names>J.</given-names></name><name name-style="western"><surname>Briggs</surname><given-names>J.</given-names></name><name name-style="western"><surname>Jeremy</surname><given-names>J.</given-names></name><name name-style="western"><surname>Gerry</surname><given-names>S.</given-names></name><name name-style="western"><surname>Redfern</surname><given-names>O.</given-names></name></person-group><article-title>How long do nursing staff take to measure and record patients&#8217; vital signs observations in hospital? A time-and-motion study</article-title><source>Int. J. Nurs. Stud.</source><year>2021</year><volume>118</volume><fpage>103921</fpage><pub-id pub-id-type="doi">10.1016/j.ijnurstu.2021.103921</pub-id><pub-id pub-id-type="pmid">33812297</pub-id><pub-id pub-id-type="pmcid">PMC8249906</pub-id></element-citation></ref><ref id="B5-healthcare-11-01152"><label>5.</label><element-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Tang</surname><given-names>V.</given-names></name><name name-style="western"><surname>Choy</surname><given-names>K.</given-names></name><name name-style="western"><surname>Ho</surname><given-names>G.</given-names></name><name name-style="western"><surname>Lam</surname><given-names>H.</given-names></name><name name-style="western"><surname>Tsang</surname><given-names>Y.P.</given-names></name></person-group><article-title>An IoMT-based geriatric care management system for achieving smart health in nursing homes</article-title><source>Ind. Manag. Data Syst.</source><year>2019</year><comment><italic toggle="yes">ahead-of-print</italic></comment><pub-id pub-id-type="doi">10.1108/IMDS-01-2019-0024</pub-id></element-citation></ref><ref id="B6-healthcare-11-01152"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Flores-Martin</surname><given-names>D.</given-names></name><name name-style="western"><surname>Rojo</surname><given-names>J.</given-names></name><name name-style="western"><surname>Moguel</surname><given-names>E.</given-names></name><name name-style="western"><surname>Berrocal</surname><given-names>J.</given-names></name><name name-style="western"><surname>Murillo</surname><given-names>J.M.</given-names></name><name name-style="western"><surname>Cai</surname><given-names>Z.</given-names></name></person-group><article-title>Smart Nursing Homes: Self-Management Architecture Based on IoT and Machine Learning for Rural Areas</article-title><source>Wirel. Commun. Mob. Comput.</source><year>2021</year><volume>2021</volume><pub-id pub-id-type="doi">10.1155/2021/8874988</pub-id></element-citation></ref><ref id="B7-healthcare-11-01152"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lu</surname><given-names>Z.X.</given-names></name><name name-style="western"><surname>Qian</surname><given-names>P.</given-names></name><name name-style="western"><surname>Bi</surname><given-names>D.</given-names></name><name name-style="western"><surname>Ye</surname><given-names>Z.W.</given-names></name><name name-style="western"><surname>He</surname><given-names>X.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>Y.H.</given-names></name><name name-style="western"><surname>Su</surname><given-names>L.</given-names></name><name name-style="western"><surname>Li</surname><given-names>S.L.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>Z.L.</given-names></name></person-group><article-title>Application of AI and IoT in Clinical Medicine: Summary and Challenges</article-title><source>Curr. Med Sci.</source><year>2021</year><volume>41</volume><fpage>1134</fpage><lpage>1150</lpage><pub-id pub-id-type="doi">10.1007/s11596-021-2486-z</pub-id><pub-id pub-id-type="pmid">34939144</pub-id><pub-id pub-id-type="pmcid">PMC8693843</pub-id></element-citation></ref><ref id="B8-healthcare-11-01152"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Mbunge</surname><given-names>E.</given-names></name><name name-style="western"><surname>Muchemwa</surname><given-names>B.</given-names></name><name name-style="western"><surname>Jiyane</surname><given-names>S.</given-names></name><name name-style="western"><surname>Batani</surname><given-names>J.</given-names></name></person-group><article-title>Sensors and healthcare 5.0: Transformative shift in virtual care through emerging digital health technologies</article-title><source>Glob. Health J.</source><year>2021</year><volume>5</volume><fpage>169</fpage><lpage>177</lpage><pub-id pub-id-type="doi">10.1016/j.glohj.2021.11.008</pub-id></element-citation></ref><ref id="B9-healthcare-11-01152"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Khan</surname><given-names>M.F.</given-names></name><name name-style="western"><surname>Ghazal</surname><given-names>T.M.</given-names></name><name name-style="western"><surname>Said</surname><given-names>R.A.</given-names></name><name name-style="western"><surname>Fatima</surname><given-names>A.</given-names></name><name name-style="western"><surname>Abbas</surname><given-names>S.</given-names></name><name name-style="western"><surname>Khan</surname><given-names>M.A.</given-names></name><name name-style="western"><surname>Issa</surname><given-names>G.F.</given-names></name><name name-style="western"><surname>Ahmad</surname><given-names>M.</given-names></name><name name-style="western"><surname>Khan</surname><given-names>M.A.</given-names></name></person-group><article-title>An IoMT-Enabled Smart Healthcare Model to Monitor Elderly People Using Machine Learning Technique</article-title><source>Comput. Intell. Neurosci.</source><year>2021</year><volume>2021</volume><fpage>1</fpage><lpage>10</lpage><pub-id pub-id-type="doi">10.1155/2021/2487759</pub-id><pub-id pub-id-type="pmcid">PMC8639263</pub-id><pub-id pub-id-type="pmid">34868288</pub-id></element-citation></ref><ref id="B10-healthcare-11-01152"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Alshamrani</surname><given-names>M.</given-names></name></person-group><article-title>IoT and artificial intelligence implementations for remote healthcare monitoring systems: A survey</article-title><source>J. King Saud Univ.&#8212;Comput. Inf. Sci.</source><year>2022</year><volume>34</volume><fpage>4687</fpage><lpage>4701</lpage><pub-id pub-id-type="doi">10.1016/j.jksuci.2021.06.005</pub-id></element-citation></ref><ref id="B11-healthcare-11-01152"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ienca</surname><given-names>M.</given-names></name><name name-style="western"><surname>Schneble</surname><given-names>C.</given-names></name><name name-style="western"><surname>Kressig</surname><given-names>R.W.</given-names></name><name name-style="western"><surname>Wangmo</surname><given-names>T.</given-names></name></person-group><article-title>Digital health interventions for healthy ageing: A qualitative user evaluation and ethical assessment</article-title><source>BMC Geriatr.</source><year>2021</year><volume>21</volume><elocation-id>412</elocation-id><pub-id pub-id-type="doi">10.1186/s12877-021-02338-z</pub-id><pub-id pub-id-type="pmid">34215209</pub-id><pub-id pub-id-type="pmcid">PMC8252216</pub-id></element-citation></ref><ref id="B12-healthcare-11-01152"><label>12.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Andreoni</surname><given-names>G.</given-names></name><name name-style="western"><surname>Mambrettii</surname><given-names>C.</given-names></name></person-group><article-title>Privacy and Security Concerns in IoT-Based Healthcare Systems</article-title><source>Digital Health Technology for Better Aging</source><publisher-name>Springer</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2021</year><fpage>365</fpage><pub-id pub-id-type="doi">10.1007/978-3-030-72663-8</pub-id></element-citation></ref><ref id="B13-healthcare-11-01152"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kekade</surname><given-names>S.</given-names></name><name name-style="western"><surname>Hseieh</surname><given-names>C.H.</given-names></name><name name-style="western"><surname>Islam</surname><given-names>M.M.</given-names></name><name name-style="western"><surname>Atique</surname><given-names>S.</given-names></name><name name-style="western"><surname>Mohammed Khalfan</surname><given-names>A.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Y.C.</given-names></name><name name-style="western"><surname>Abdul</surname><given-names>S.S.</given-names></name></person-group><article-title>The usefulness and actual use of wearable devices among the elderly population</article-title><source>Comput. Methods Programs Biomed.</source><year>2018</year><volume>153</volume><fpage>137</fpage><lpage>159</lpage><pub-id pub-id-type="doi">10.1016/j.cmpb.2017.10.008</pub-id><pub-id pub-id-type="pmid">29157447</pub-id></element-citation></ref><ref id="B14-healthcare-11-01152"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chandrasekaran</surname><given-names>R.</given-names></name><name name-style="western"><surname>Katthula</surname><given-names>V.</given-names></name><name name-style="western"><surname>Moustakas</surname><given-names>E.</given-names></name></person-group><article-title>Too old for technology? Use of wearable healthcare devices by older adults and their willingness to share health data with providers</article-title><source>Health Inform. J.</source><year>2021</year><volume>27</volume><fpage>14604582211058073</fpage><pub-id pub-id-type="doi">10.1177/14604582211058073</pub-id><pub-id pub-id-type="pmid">34802315</pub-id></element-citation></ref><ref id="B15-healthcare-11-01152"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Prieto-Avalos</surname><given-names>G.</given-names></name><name name-style="western"><surname>Cruz-Ramos</surname><given-names>N.A.</given-names></name><name name-style="western"><surname>Alor-Hern&#225;ndez</surname><given-names>G.</given-names></name><name name-style="western"><surname>S&#225;nchez-Cervantes</surname><given-names>J.L.</given-names></name><name name-style="western"><surname>Rodr&#237;guez-Mazahua</surname><given-names>L.</given-names></name><name name-style="western"><surname>Guarneros-Nolasco</surname><given-names>L.R.</given-names></name></person-group><article-title>Wearable Devices for Physical Monitoring of Heart: A Review</article-title><source>Biosensors</source><year>2022</year><volume>12</volume><elocation-id>292</elocation-id><pub-id pub-id-type="doi">10.3390/bios12050292</pub-id><pub-id pub-id-type="pmid">35624593</pub-id><pub-id pub-id-type="pmcid">PMC9138373</pub-id></element-citation></ref><ref id="B16-healthcare-11-01152"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lu</surname><given-names>L.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Xie</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Gao</surname><given-names>F.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>S.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Ye</surname><given-names>Z.</given-names></name></person-group><article-title>Wearable health devices in health care: Narrative systematic review</article-title><source>JMIR mHealth uHealth</source><year>2020</year><volume>8</volume><fpage>e18907</fpage><pub-id pub-id-type="doi">10.2196/18907</pub-id><pub-id pub-id-type="pmid">33164904</pub-id><pub-id pub-id-type="pmcid">PMC7683248</pub-id></element-citation></ref><ref id="B17-healthcare-11-01152"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Singhal</surname><given-names>A.</given-names></name><name name-style="western"><surname>Cowie</surname><given-names>M.R.</given-names></name></person-group><article-title>The Role of Wearables in Heart Failure</article-title><source>Curr. Heart Fail. Rep.</source><year>2020</year><volume>17</volume><fpage>125</fpage><lpage>132</lpage><pub-id pub-id-type="doi">10.1007/s11897-020-00467-x</pub-id><pub-id pub-id-type="pmid">32494944</pub-id><pub-id pub-id-type="pmcid">PMC7343723</pub-id></element-citation></ref><ref id="B18-healthcare-11-01152"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Alharbi</surname><given-names>M.</given-names></name><name name-style="western"><surname>Straiton</surname><given-names>N.</given-names></name><name name-style="western"><surname>Gallagher</surname><given-names>R.</given-names></name></person-group><article-title>Harnessing the Potential of Wearable Activity Trackers for Heart Failure Self-Care</article-title><source>Curr. Heart Fail. Rep.</source><year>2017</year><volume>14</volume><fpage>23</fpage><lpage>29</lpage><pub-id pub-id-type="doi">10.1007/s11897-017-0318-z</pub-id><pub-id pub-id-type="pmid">28181075</pub-id></element-citation></ref><ref id="B19-healthcare-11-01152"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Gresham</surname><given-names>G.</given-names></name><name name-style="western"><surname>Schrack</surname><given-names>J.</given-names></name><name name-style="western"><surname>Gresham</surname><given-names>L.M.</given-names></name><name name-style="western"><surname>Shinde</surname><given-names>A.M.</given-names></name><name name-style="western"><surname>Hendifar</surname><given-names>A.E.</given-names></name><name name-style="western"><surname>Tuli</surname><given-names>R.</given-names></name><name name-style="western"><surname>Rimel</surname><given-names>B.</given-names></name><name name-style="western"><surname>Figlin</surname><given-names>R.</given-names></name><name name-style="western"><surname>Meinert</surname><given-names>C.L.</given-names></name><name name-style="western"><surname>Piantadosi</surname><given-names>S.</given-names></name></person-group><article-title>Wearable activity monitors in oncology trials: Current use of an emerging technology</article-title><source>Contemp. Clin. Trials</source><year>2018</year><volume>64</volume><fpage>13</fpage><lpage>21</lpage><pub-id pub-id-type="doi">10.1016/j.cct.2017.11.002</pub-id><pub-id pub-id-type="pmid">29129704</pub-id><pub-id pub-id-type="pmcid">PMC12004692</pub-id></element-citation></ref><ref id="B20-healthcare-11-01152"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Watanabe</surname><given-names>T.</given-names></name><name name-style="western"><surname>Saito</surname><given-names>H.</given-names></name><name name-style="western"><surname>Koike</surname><given-names>E.</given-names></name><name name-style="western"><surname>Nitta</surname><given-names>K.</given-names></name></person-group><article-title>A Preliminary Test of Measurement of Joint Angles and Stride Length with Wireless Inertial Sensors for Wearable Gait Evaluation System</article-title><source>Comput. Intell. Neurosci.</source><year>2011</year><volume>2011</volume><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="doi">10.1155/2011/975193</pub-id><pub-id pub-id-type="pmid">21941531</pub-id><pub-id pub-id-type="pmcid">PMC3175383</pub-id></element-citation></ref><ref id="B21-healthcare-11-01152"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ryvlin</surname><given-names>P.</given-names></name><name name-style="western"><surname>Ciumas</surname><given-names>C.</given-names></name><name name-style="western"><surname>Wisniewski</surname><given-names>I.</given-names></name><name name-style="western"><surname>Beniczky</surname><given-names>S.</given-names></name></person-group><article-title>Wearable devices for sudden unexpected death in epilepsy prevention</article-title><source>Epilepsia</source><year>2018</year><volume>59</volume><issue>(Suppl. 1)</issue><fpage>61</fpage><lpage>66</lpage><pub-id pub-id-type="doi">10.1111/epi.14054</pub-id><pub-id pub-id-type="pmid">29873831</pub-id></element-citation></ref><ref id="B22-healthcare-11-01152"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Takei</surname><given-names>K.</given-names></name><name name-style="western"><surname>Honda</surname><given-names>W.</given-names></name><name name-style="western"><surname>Harada</surname><given-names>S.</given-names></name><name name-style="western"><surname>Arie</surname><given-names>T.</given-names></name><name name-style="western"><surname>Akita</surname><given-names>S.</given-names></name></person-group><article-title>Toward flexible and wearable human-interactive health-monitoring devices</article-title><source>Adv. Healthc. Mater.</source><year>2015</year><volume>4</volume><fpage>487</fpage><lpage>500</lpage><pub-id pub-id-type="doi">10.1002/adhm.201400546</pub-id><pub-id pub-id-type="pmid">25425072</pub-id></element-citation></ref><ref id="B23-healthcare-11-01152"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kamei</surname><given-names>T.</given-names></name><name name-style="western"><surname>Kanamori</surname><given-names>T.</given-names></name><name name-style="western"><surname>Yamamoto</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Edirippulige</surname><given-names>S.</given-names></name></person-group><article-title>The use of wearable devices in chronic disease management to enhance adherence and improve telehealth outcomes: A systematic review and meta-analysis</article-title><source>J. Telemed. Telecare</source><year>2022</year><volume>28</volume><fpage>342</fpage><lpage>359</lpage><pub-id pub-id-type="doi">10.1177/1357633X20937573</pub-id><pub-id pub-id-type="pmid">32819184</pub-id></element-citation></ref><ref id="B24-healthcare-11-01152"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yu</surname><given-names>S.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>X.</given-names></name></person-group><article-title>The Impact of Wearable Devices on Physical Activity for Chronic Disease Patients: Findings from the 2019 Health Information National Trends Survey</article-title><source>Int. J. Environ. Res. Public Health</source><year>2023</year><volume>20</volume><elocation-id>887</elocation-id><pub-id pub-id-type="doi">10.3390/ijerph20010887</pub-id><pub-id pub-id-type="pmid">36613207</pub-id><pub-id pub-id-type="pmcid">PMC9820171</pub-id></element-citation></ref><ref id="B25-healthcare-11-01152"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kulurkar</surname><given-names>P.</given-names></name><name name-style="western"><surname>kumar Dixit</surname><given-names>C.</given-names></name><name name-style="western"><surname>Bharathi</surname><given-names>V.</given-names></name><name name-style="western"><surname>Monikavishnuvarthini</surname><given-names>A.</given-names></name><name name-style="western"><surname>Dhakne</surname><given-names>A.</given-names></name><name name-style="western"><surname>Preethi</surname><given-names>P.</given-names></name></person-group><article-title>AI based elderly fall prediction system using wearable sensors: A smart home-care technology with IOT</article-title><source>Meas. Sensors</source><year>2023</year><volume>25</volume><fpage>100614</fpage><pub-id pub-id-type="doi">10.1016/j.measen.2022.100614</pub-id></element-citation></ref><ref id="B26-healthcare-11-01152"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Cudejko</surname><given-names>T.</given-names></name><name name-style="western"><surname>Button</surname><given-names>K.</given-names></name><name name-style="western"><surname>Al-Amri</surname><given-names>M.</given-names></name></person-group><article-title>Validity and reliability of accelerations and orientations measured using wearable sensors during functional activities</article-title><source>Sci. Rep.</source><year>2022</year><volume>12</volume><fpage>14619</fpage><pub-id pub-id-type="doi">10.1038/s41598-022-18845-x</pub-id><pub-id pub-id-type="pmid">36028523</pub-id><pub-id pub-id-type="pmcid">PMC9417076</pub-id></element-citation></ref><ref id="B27-healthcare-11-01152"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Fuller</surname><given-names>D.</given-names></name><name name-style="western"><surname>Colwell</surname><given-names>E.</given-names></name><name name-style="western"><surname>Low</surname><given-names>J.</given-names></name><name name-style="western"><surname>Orychock</surname><given-names>K.</given-names></name><name name-style="western"><surname>Tobin</surname><given-names>M.A.</given-names></name><name name-style="western"><surname>Simango</surname><given-names>B.</given-names></name><name name-style="western"><surname>Buote</surname><given-names>R.</given-names></name><name name-style="western"><surname>Heerden</surname><given-names>D.V.</given-names></name><name name-style="western"><surname>Luan</surname><given-names>H.</given-names></name><name name-style="western"><surname>Cullen</surname><given-names>K.</given-names></name><etal/></person-group><article-title>Reliability and Validity of Commercially Available Wearable Devices for Measuring Steps, Energy Expenditure, and Heart Rate: Systematic Review</article-title><source>JMIR mHealth uHealth</source><year>2020</year><volume>8</volume><fpage>e18694</fpage><pub-id pub-id-type="doi">10.2196/18694</pub-id><pub-id pub-id-type="pmid">32897239</pub-id><pub-id pub-id-type="pmcid">PMC7509623</pub-id></element-citation></ref><ref id="B28-healthcare-11-01152"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Patel</surname><given-names>V.</given-names></name><name name-style="western"><surname>Orchanian-Cheff</surname><given-names>A.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>R.</given-names></name></person-group><article-title>Evaluating the Validity and Utility of Wearable Technology for Continuously Monitoring Patients in a Hospital Setting: Systematic Review</article-title><source>JMIR mHealth uHealth</source><year>2021</year><volume>9</volume><fpage>e17411</fpage><pub-id pub-id-type="doi">10.2196/17411</pub-id><pub-id pub-id-type="pmid">34406121</pub-id><pub-id pub-id-type="pmcid">PMC8411322</pub-id></element-citation></ref><ref id="B29-healthcare-11-01152"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chan</surname><given-names>A.</given-names></name><name name-style="western"><surname>Chan</surname><given-names>D.</given-names></name><name name-style="western"><surname>Lee</surname><given-names>H.</given-names></name><name name-style="western"><surname>Ng</surname><given-names>C.C.</given-names></name><name name-style="western"><surname>Yeo</surname><given-names>A.H.L.</given-names></name></person-group><article-title>Reporting adherence, validity and physical activity measures of wearable activity trackers in medical research: A systematic review</article-title><source>Int. J. Med Inform.</source><year>2022</year><volume>160</volume><fpage>104696</fpage><pub-id pub-id-type="doi">10.1016/j.ijmedinf.2022.104696</pub-id><pub-id pub-id-type="pmid">35121356</pub-id></element-citation></ref><ref id="B30-healthcare-11-01152"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Teixeira</surname><given-names>E.</given-names></name><name name-style="western"><surname>Fonseca</surname><given-names>H.</given-names></name><name name-style="western"><surname>Diniz-Sousa</surname><given-names>F.</given-names></name><name name-style="western"><surname>Veras</surname><given-names>L.</given-names></name><name name-style="western"><surname>Boppre</surname><given-names>G.</given-names></name><name name-style="western"><surname>Oliveira</surname><given-names>J.</given-names></name><name name-style="western"><surname>Pinto</surname><given-names>D.</given-names></name><name name-style="western"><surname>Alves</surname><given-names>A.J.</given-names></name><name name-style="western"><surname>Barbosa</surname><given-names>A.</given-names></name><name name-style="western"><surname>Mendes</surname><given-names>R.</given-names></name><etal/></person-group><article-title>Wearable Devices for Physical Activity and Healthcare Monitoring in Elderly People: A Critical Review</article-title><source>Geriatrics</source><year>2021</year><volume>6</volume><elocation-id>38</elocation-id><pub-id pub-id-type="doi">10.3390/geriatrics6020038</pub-id><pub-id pub-id-type="pmid">33917104</pub-id><pub-id pub-id-type="pmcid">PMC8167657</pub-id></element-citation></ref><ref id="B31-healthcare-11-01152"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Moore</surname><given-names>K.</given-names></name><name name-style="western"><surname>O&#8217;Shea</surname><given-names>E.</given-names></name><name name-style="western"><surname>Kenny</surname><given-names>L.</given-names></name><name name-style="western"><surname>Barton</surname><given-names>J.</given-names></name><name name-style="western"><surname>Tedesco</surname><given-names>S.</given-names></name><name name-style="western"><surname>Sica</surname><given-names>M.</given-names></name><name name-style="western"><surname>Crowe</surname><given-names>C.</given-names></name><name name-style="western"><surname>Alamaki</surname><given-names>A.</given-names></name><name name-style="western"><surname>Condell</surname><given-names>J.</given-names></name><name name-style="western"><surname>Nordstrom</surname><given-names>A.</given-names></name><etal/></person-group><article-title>Older Adults&#8217; Experiences With Using Wearable Devices: Qualitative Systematic Review and Meta-synthesis</article-title><source>JMIR mHealth uHealth</source><year>2021</year><volume>9</volume><fpage>e23832</fpage><pub-id pub-id-type="doi">10.2196/23832</pub-id><pub-id pub-id-type="pmid">34081020</pub-id><pub-id pub-id-type="pmcid">PMC8212622</pub-id></element-citation></ref><ref id="B32-healthcare-11-01152"><label>32.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Koerber</surname><given-names>D.</given-names></name><name name-style="western"><surname>Khan</surname><given-names>S.</given-names></name><name name-style="western"><surname>Shamsheri</surname><given-names>T.</given-names></name><name name-style="western"><surname>Kirubarajan</surname><given-names>A.</given-names></name><name name-style="western"><surname>Mehta</surname><given-names>S.</given-names></name></person-group><article-title>Accuracy of Heart Rate Measurement with Wrist-Worn Wearable Devices in Various Skin Tones: A Systematic Review</article-title><source>J. Racial Ethn. Health Disparities</source><year>2022</year><pub-id pub-id-type="doi">10.1007/s40615-022-01446-9</pub-id><pub-id pub-id-type="pmcid">PMC9662769</pub-id><pub-id pub-id-type="pmid">36376641</pub-id></element-citation></ref><ref id="B33-healthcare-11-01152"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ferguson</surname><given-names>C.</given-names></name><name name-style="western"><surname>Hickman</surname><given-names>L.D.</given-names></name><name name-style="western"><surname>Turkmani</surname><given-names>S.</given-names></name><name name-style="western"><surname>Breen</surname><given-names>P.</given-names></name><name name-style="western"><surname>Gargiulo</surname><given-names>G.</given-names></name><name name-style="western"><surname>Inglis</surname><given-names>S.C.</given-names></name></person-group><article-title>&#8220;Wearables only work on patients that wear them&#8221;: Barriers and facilitators to the adoption of wearable cardiac monitoring technologies</article-title><source>Cardiovasc. Digit. Health J.</source><year>2021</year><volume>2</volume><fpage>137</fpage><lpage>147</lpage><pub-id pub-id-type="doi">10.1016/j.cvdhj.2021.02.001</pub-id><pub-id pub-id-type="pmid">35265900</pub-id><pub-id pub-id-type="pmcid">PMC8890057</pub-id></element-citation></ref><ref id="B34-healthcare-11-01152"><label>34.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kristoffersson</surname><given-names>A.</given-names></name><name name-style="western"><surname>Lind&#233;n</surname><given-names>M.</given-names></name></person-group><article-title>Wearable Sensors for Monitoring and Preventing Noncommunicable Diseases: A Systematic Review</article-title><source>Information</source><year>2020</year><volume>11</volume><elocation-id>521</elocation-id><pub-id pub-id-type="doi">10.3390/info11110521</pub-id></element-citation></ref><ref id="B35-healthcare-11-01152"><label>35.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Rohmetra</surname><given-names>H.</given-names></name><name name-style="western"><surname>Raghunath</surname><given-names>N.</given-names></name><name name-style="western"><surname>Narang</surname><given-names>P.</given-names></name><name name-style="western"><surname>Chamola</surname><given-names>V.</given-names></name><name name-style="western"><surname>Guizani</surname><given-names>M.</given-names></name><name name-style="western"><surname>Lakkaniga</surname><given-names>R.</given-names></name></person-group><article-title>AI-enabled remote monitoring of vital signs for COVID-19: Methods, Prospects and Challenges</article-title><source>Computing</source><year>2021</year><volume>105</volume><fpage>783</fpage><lpage>809</lpage><pub-id pub-id-type="doi">10.1007/s00607-021-00937-7</pub-id></element-citation></ref><ref id="B36-healthcare-11-01152"><label>36.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Guo</surname><given-names>K.</given-names></name><name name-style="western"><surname>Zhai</surname><given-names>T.</given-names></name><name name-style="western"><surname>Purushothama</surname><given-names>M.H.</given-names></name><name name-style="western"><surname>Dobre</surname><given-names>A.</given-names></name><name name-style="western"><surname>Meah</surname><given-names>S.</given-names></name><name name-style="western"><surname>Pashollari</surname><given-names>E.</given-names></name><name name-style="western"><surname>Vaish</surname><given-names>A.</given-names></name><name name-style="western"><surname>DeWilde</surname><given-names>C.</given-names></name><name name-style="western"><surname>Islam</surname><given-names>M.N.</given-names></name></person-group><article-title>Contactless Vital Sign Monitoring System for In-Vehicle Driver Monitoring Using a Near-Infrared Time-of-Flight Camera</article-title><source>Appl. Sci.</source><year>2022</year><volume>12</volume><elocation-id>4416</elocation-id><pub-id pub-id-type="doi">10.3390/app12094416</pub-id></element-citation></ref><ref id="B37-healthcare-11-01152"><label>37.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Guo</surname><given-names>K.</given-names></name><name name-style="western"><surname>Zhai</surname><given-names>T.</given-names></name><name name-style="western"><surname>Pashollari</surname><given-names>E.</given-names></name><name name-style="western"><surname>Varlamos</surname><given-names>C.J.</given-names></name><name name-style="western"><surname>Ahmed</surname><given-names>A.</given-names></name><name name-style="western"><surname>Islam</surname><given-names>M.N.</given-names></name></person-group><article-title>Contactless Vital Sign Monitoring System for Heart and Respiratory Rate Measurements with Motion Compensation Using a Near-Infrared Time-of-Flight Camera</article-title><source>Appl. Sci.</source><year>2021</year><volume>11</volume><elocation-id>10913</elocation-id><pub-id pub-id-type="doi">10.3390/app112210913</pub-id></element-citation></ref><ref id="B38-healthcare-11-01152"><label>38.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jelin&#269;i&#263;</surname><given-names>V.</given-names></name><name name-style="western"><surname>Diest</surname><given-names>I.V.</given-names></name><name name-style="western"><surname>Torta</surname><given-names>D.M.</given-names></name><name name-style="western"><surname>von Leupoldt</surname><given-names>A.</given-names></name></person-group><article-title>The breathing brain: The potential of neural oscillations for the understanding of respiratory perception in health and disease</article-title><source>Psychophysiology</source><year>2022</year><volume>59</volume><fpage>e13844</fpage><pub-id pub-id-type="doi">10.1111/psyp.13844</pub-id><pub-id pub-id-type="pmid">34009644</pub-id></element-citation></ref><ref id="B39-healthcare-11-01152"><label>39.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Andrea</surname><given-names>N.</given-names></name><name name-style="western"><surname>Carlo</surname><given-names>M.</given-names></name><name name-style="western"><surname>Emiliano</surname><given-names>S.</given-names></name><name name-style="western"><surname>Massimo</surname><given-names>S.</given-names></name></person-group><article-title>The Importance of Respiratory Rate Monitoring: From Healthcare to Sport and Exercise</article-title><source>Sensors</source><year>2020</year><volume>20</volume><elocation-id>6396</elocation-id><pub-id pub-id-type="doi">10.3390/s20216396</pub-id><pub-id pub-id-type="pmid">33182463</pub-id><pub-id pub-id-type="pmcid">PMC7665156</pub-id></element-citation></ref><ref id="B40-healthcare-11-01152"><label>40.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Baumert</surname><given-names>M.</given-names></name><name name-style="western"><surname>Linz</surname><given-names>D.</given-names></name><name name-style="western"><surname>Stone</surname><given-names>K.</given-names></name><name name-style="western"><surname>McEvoy</surname><given-names>R.D.</given-names></name><name name-style="western"><surname>Cummings</surname><given-names>S.</given-names></name><name name-style="western"><surname>Redline</surname><given-names>S.</given-names></name><name name-style="western"><surname>Mehra</surname><given-names>R.</given-names></name><name name-style="western"><surname>Immanuel</surname><given-names>S.</given-names></name></person-group><article-title>Mean nocturnal respiratory rate predicts cardiovascular and all-cause mortality in community-dwelling older men and women</article-title><source>Eur. Respir. J.</source><year>2019</year><volume>54</volume><fpage>1802175</fpage><pub-id pub-id-type="doi">10.1183/13993003.02175-2018</pub-id><pub-id pub-id-type="pmid">31151958</pub-id><pub-id pub-id-type="pmcid">PMC7864583</pub-id></element-citation></ref><ref id="B41-healthcare-11-01152"><label>41.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Fox</surname><given-names>H.</given-names></name><name name-style="western"><surname>Rudolph</surname><given-names>V.</given-names></name><name name-style="western"><surname>Munt</surname><given-names>O.</given-names></name><name name-style="western"><surname>Malouf</surname><given-names>G.</given-names></name><name name-style="western"><surname>Graml</surname><given-names>A.</given-names></name><name name-style="western"><surname>Bitter</surname><given-names>T.</given-names></name><name name-style="western"><surname>Oldenburg</surname><given-names>O.</given-names></name></person-group><article-title>Early identification of heart failure deterioration through respiratory monitoring with adaptive servo-ventilation</article-title><source>J. Sleep Res.</source><year>2023</year><volume>32</volume><fpage>e13749</fpage><pub-id pub-id-type="doi">10.1111/jsr.13749</pub-id><pub-id pub-id-type="pmid">36222010</pub-id></element-citation></ref><ref id="B42-healthcare-11-01152"><label>42.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Scebba</surname><given-names>G.</given-names></name><name name-style="western"><surname>Da Poian</surname><given-names>G.</given-names></name><name name-style="western"><surname>Karlen</surname><given-names>W.</given-names></name></person-group><article-title>Multispectral Video Fusion for Non-Contact Monitoring of Respiratory Rate and Apnea</article-title><source>IEEE Trans. Biomed. Eng.</source><year>2021</year><volume>68</volume><fpage>350</fpage><lpage>359</lpage><pub-id pub-id-type="doi">10.1109/TBME.2020.2993649</pub-id><pub-id pub-id-type="pmid">32396069</pub-id></element-citation></ref><ref id="B43-healthcare-11-01152"><label>43.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Nakagawa</surname><given-names>K.</given-names></name><name name-style="western"><surname>Sankai</surname><given-names>Y.</given-names></name></person-group><article-title>Noncontact Vital Sign Monitoring System with Dual Infrared Imaging for Discriminating Respiration Mode</article-title><source>Adv. Biomed. Eng.</source><year>2021</year><volume>10</volume><fpage>80</fpage><lpage>89</lpage><pub-id pub-id-type="doi">10.14326/abe.10.80</pub-id></element-citation></ref><ref id="B44-healthcare-11-01152"><label>44.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yacchirema</surname><given-names>D.C.</given-names></name><name name-style="western"><surname>de Puga</surname><given-names>J.S.</given-names></name><name name-style="western"><surname>Palau</surname><given-names>C.E.</given-names></name><name name-style="western"><surname>Esteve</surname><given-names>M.</given-names></name></person-group><article-title>Fall detection system for elderly people using IoT and ensemble machine learning algorithm</article-title><source>Pers. Ubiquitous Comput.</source><year>2019</year><volume>23</volume><fpage>801</fpage><lpage>817</lpage><pub-id pub-id-type="doi">10.1007/s00779-018-01196-8</pub-id></element-citation></ref><ref id="B45-healthcare-11-01152"><label>45.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Esmaeili</surname><given-names>B.</given-names></name><name name-style="western"><surname>AkhavanPour</surname><given-names>A.</given-names></name><name name-style="western"><surname>Bosaghzadeh</surname><given-names>A.</given-names></name></person-group><article-title>An Ensemble Model For Human Posture Recognition</article-title><source>Proceedings of the 2020 International Conference on Machine Vision and Image Processing (MVIP)</source><conf-loc>Teheren, Iran</conf-loc><conf-date>18&#8211;20 February 2020</conf-date><fpage>1</fpage><lpage>7</lpage><pub-id pub-id-type="doi">10.1109/MVIP49855.2020.9116911</pub-id></element-citation></ref><ref id="B46-healthcare-11-01152"><label>46.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Artacho</surname><given-names>B.</given-names></name><name name-style="western"><surname>Savakis</surname><given-names>A.E.</given-names></name></person-group><article-title>UniPose: Unified Human Pose Estimation in Single Images and Videos</article-title><source>CoRR</source><year>2020</year><fpage>abs/2001.08095</fpage><pub-id pub-id-type="doi" assigning-authority="pmc">10.1109/TPAMI.2021.3124736</pub-id><pub-id pub-id-type="pmid">34727028</pub-id></element-citation></ref><ref id="B47-healthcare-11-01152"><label>47.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Insafutdinov</surname><given-names>E.</given-names></name><name name-style="western"><surname>Pishchulin</surname><given-names>L.</given-names></name><name name-style="western"><surname>Andres</surname><given-names>B.</given-names></name><name name-style="western"><surname>Andriluka</surname><given-names>M.</given-names></name><name name-style="western"><surname>Schiele</surname><given-names>B.</given-names></name></person-group><article-title>DeeperCut: A Deeper, Stronger, and Faster Multi-Person Pose Estimation Model</article-title><source>CoRR</source><year>2016</year><fpage>abs/1605.03170</fpage></element-citation></ref><ref id="B48-healthcare-11-01152"><label>48.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>J.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>C.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Mao</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Fang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Lu</surname><given-names>C.</given-names></name></person-group><article-title>CrowdPose: Efficient Crowded Scenes Pose Estimation and a New Benchmark</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019</source><conf-loc>Long Beach, CA, USA</conf-loc><conf-date>16&#8211;20 June 2019</conf-date><fpage>10863</fpage><lpage>10872</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2019.01112</pub-id></element-citation></ref><ref id="B49-healthcare-11-01152"><label>49.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Ouyang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Chu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>X.</given-names></name></person-group><article-title>Multi-source Deep Learning for Human Pose Estimation</article-title><source>Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Columbus, OH, USA</conf-loc><conf-date>23&#8211;28 June 2014</conf-date><fpage>2337</fpage><lpage>2344</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2014.299</pub-id></element-citation></ref><ref id="B50-healthcare-11-01152"><label>50.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Moon</surname><given-names>G.</given-names></name><name name-style="western"><surname>Chang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Lee</surname><given-names>K.M.</given-names></name></person-group><article-title>PoseFix: Model-Agnostic General Human Pose Refinement Network</article-title><source>Proceedings of the 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Long Beach, CA, USA</conf-loc><conf-date>15&#8211;20 June 2019</conf-date><fpage>7765</fpage><lpage>7773</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2019.00796</pub-id></element-citation></ref><ref id="B51-healthcare-11-01152"><label>51.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Nie</surname><given-names>X.</given-names></name><name name-style="western"><surname>Feng</surname><given-names>J.</given-names></name><name name-style="western"><surname>Xing</surname><given-names>J.</given-names></name><name name-style="western"><surname>Xiao</surname><given-names>S.</given-names></name><name name-style="western"><surname>Yan</surname><given-names>S.</given-names></name></person-group><article-title>Hierarchical Contextual Refinement Networks for Human Pose Estimation</article-title><source>IEEE Trans. Image Process.</source><year>2019</year><volume>28</volume><fpage>924</fpage><lpage>936</lpage><pub-id pub-id-type="doi">10.1109/TIP.2018.2872628</pub-id><pub-id pub-id-type="pmid">30296223</pub-id></element-citation></ref><ref id="B52-healthcare-11-01152"><label>52.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Newell</surname><given-names>A.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>K.</given-names></name><name name-style="western"><surname>Deng</surname><given-names>J.</given-names></name></person-group><article-title>Stacked Hourglass Networks for Human Pose Estimation</article-title><source>Proceedings of the Computer Vision&#8211;ECCV 2016</source><conf-loc>Amsterdam, The Netherlands</conf-loc><conf-date>11&#8211;14 October 2016</conf-date><fpage>483</fpage><lpage>499</lpage></element-citation></ref><ref id="B53-healthcare-11-01152"><label>53.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>N&#250;&#241;ez-Marcos</surname><given-names>A.</given-names></name><name name-style="western"><surname>Azkune</surname><given-names>G.</given-names></name><name name-style="western"><surname>Arganda-Carreras</surname><given-names>I.</given-names></name></person-group><article-title>Vision-Based Fall Detection with Convolutional Neural Networks</article-title><source>Wirel. Commun. Mob. Comput.</source><year>2017</year><volume>2017</volume><fpage>1</fpage><lpage>16</lpage><pub-id pub-id-type="doi">10.1155/2017/9474806</pub-id></element-citation></ref><ref id="B54-healthcare-11-01152"><label>54.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Xu</surname><given-names>C.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Guo</surname><given-names>B.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>C.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Deng</surname><given-names>X.</given-names></name></person-group><article-title>Fall Detection in Elevator Cages Based on XGBoost and LSTM</article-title><source>Proceedings of the 2021 26th International Conference on Automation and Computing (ICAC)</source><conf-loc>Portsmouth, UK</conf-loc><conf-date>2&#8211;4 September 2021</conf-date><fpage>1</fpage><lpage>6</lpage><pub-id pub-id-type="doi">10.23919/ICAC50006.2021.9594123</pub-id></element-citation></ref><ref id="B55-healthcare-11-01152"><label>55.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Ren</surname><given-names>X.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>Y.</given-names></name></person-group><article-title>Human Fall Detection Model with Lightweight Network and Tracking in Video</article-title><source>Proceedings of the 2021 5th International Conference on Computer Science and Artificial Intelligence, CSAI 2021</source><conf-loc>Beijing, China</conf-loc><conf-date>4&#8211;6 December 2021</conf-date><fpage>1</fpage><lpage>7</lpage><pub-id pub-id-type="doi">10.1145/3507548.3507549</pub-id></element-citation></ref><ref id="B56-healthcare-11-01152"><label>56.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>De Miguel</surname><given-names>K.</given-names></name><name name-style="western"><surname>Brunete</surname><given-names>A.</given-names></name><name name-style="western"><surname>Hernando</surname><given-names>M.</given-names></name><name name-style="western"><surname>Gambao</surname><given-names>E.</given-names></name></person-group><article-title>Home Camera-Based Fall Detection System for the Elderly</article-title><source>Sensors</source><year>2017</year><volume>17</volume><elocation-id>2864</elocation-id><pub-id pub-id-type="doi">10.3390/s17122864</pub-id><pub-id pub-id-type="pmid">29232846</pub-id><pub-id pub-id-type="pmcid">PMC5751723</pub-id></element-citation></ref><ref id="B57-healthcare-11-01152"><label>57.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Sadreazami</surname><given-names>H.</given-names></name><name name-style="western"><surname>Bolic</surname><given-names>M.</given-names></name><name name-style="western"><surname>Rajan</surname><given-names>S.</given-names></name></person-group><article-title>Contactless Fall Detection Using Time-Frequency Analysis and Convolutional Neural Networks</article-title><source>IEEE Trans. Ind. Informatics</source><year>2021</year><volume>17</volume><fpage>6842</fpage><lpage>6851</lpage><pub-id pub-id-type="doi">10.1109/TII.2021.3049342</pub-id></element-citation></ref><ref id="B58-healthcare-11-01152"><label>58.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Butt</surname><given-names>F.S.</given-names></name><name name-style="western"><surname>La Blunda</surname><given-names>L.</given-names></name><name name-style="western"><surname>Wagner</surname><given-names>M.F.</given-names></name><name name-style="western"><surname>Schafer</surname><given-names>J.</given-names></name><name name-style="western"><surname>Medina-Bulo</surname><given-names>I.</given-names></name><name name-style="western"><surname>Gomez-Ullate</surname><given-names>D.</given-names></name></person-group><article-title>Fall Detection from Electrocardiogram (ECG) Signals and Classification by Deep Transfer Learning</article-title><source>Information</source><year>2021</year><volume>12</volume><elocation-id>63</elocation-id><pub-id pub-id-type="doi">10.3390/info12020063</pub-id></element-citation></ref><ref id="B59-healthcare-11-01152"><label>59.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bhattacharya</surname><given-names>A.</given-names></name><name name-style="western"><surname>Vaughan</surname><given-names>R.</given-names></name></person-group><article-title>Deep Learning Radar Design for Breathing and Fall Detection</article-title><source>IEEE Sensors J.</source><year>2020</year><volume>20</volume><fpage>5072</fpage><lpage>5085</lpage><pub-id pub-id-type="doi">10.1109/JSEN.2020.2967100</pub-id></element-citation></ref><ref id="B60-healthcare-11-01152"><label>60.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Martinez-Martin</surname><given-names>N.</given-names></name><name name-style="western"><surname>Luo</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Kaushal</surname><given-names>A.</given-names></name><name name-style="western"><surname>Adeli</surname><given-names>E.</given-names></name><name name-style="western"><surname>Haque</surname><given-names>A.</given-names></name><name name-style="western"><surname>Kelly</surname><given-names>S.S.</given-names></name><name name-style="western"><surname>Wieten</surname><given-names>S.</given-names></name><name name-style="western"><surname>Cho</surname><given-names>M.K.</given-names></name><name name-style="western"><surname>Magnus</surname><given-names>D.</given-names></name><name name-style="western"><surname>Fei-Fei</surname><given-names>L.</given-names></name><etal/></person-group><article-title>Ethical issues in using ambient intelligence in health-care settings</article-title><source>Lancet Digit. Health</source><year>2021</year><volume>3</volume><fpage>e115</fpage><lpage>e123</lpage><pub-id pub-id-type="doi">10.1016/S2589-7500(20)30275-2</pub-id><pub-id pub-id-type="pmid">33358138</pub-id><pub-id pub-id-type="pmcid">PMC8310737</pub-id></element-citation></ref><ref id="B61-healthcare-11-01152"><label>61.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Esteva</surname><given-names>A.</given-names></name><name name-style="western"><surname>Chou</surname><given-names>K.</given-names></name><name name-style="western"><surname>Yeung</surname><given-names>S.</given-names></name><name name-style="western"><surname>Naik</surname><given-names>N.</given-names></name><name name-style="western"><surname>Madani</surname><given-names>A.</given-names></name><name name-style="western"><surname>Mottaghi</surname><given-names>A.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Topol</surname><given-names>E.</given-names></name><name name-style="western"><surname>Dean</surname><given-names>J.</given-names></name><name name-style="western"><surname>Socher</surname><given-names>R.</given-names></name></person-group><article-title>Deep learning-enabled medical computer vision</article-title><source>NPJ Digit. Med.</source><year>2021</year><volume>4</volume><fpage>5</fpage><pub-id pub-id-type="doi">10.1038/s41746-020-00376-2</pub-id><pub-id pub-id-type="pmid">33420381</pub-id><pub-id pub-id-type="pmcid">PMC7794558</pub-id></element-citation></ref><ref id="B62-healthcare-11-01152"><label>62.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Babar</surname><given-names>M.</given-names></name><name name-style="western"><surname>Rahman</surname><given-names>A.</given-names></name><name name-style="western"><surname>Arif</surname><given-names>F.</given-names></name><name name-style="western"><surname>Jeon</surname><given-names>G.</given-names></name></person-group><article-title>Energy-harvesting based on internet of things and big data analytics for smart health monitoring</article-title><source>Sustain. Comput. Inform. Syst.</source><year>2018</year><volume>20</volume><fpage>155</fpage><lpage>164</lpage><pub-id pub-id-type="doi">10.1016/j.suscom.2017.10.009</pub-id></element-citation></ref><ref id="B63-healthcare-11-01152"><label>63.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Syed</surname><given-names>L.</given-names></name><name name-style="western"><surname>Jabeen</surname><given-names>S.</given-names></name><name name-style="western"><surname>Manimala</surname><given-names>S.</given-names></name><name name-style="western"><surname>Elsayed</surname><given-names>H.A.</given-names></name></person-group><article-title>Data Science Algorithms and Techniques for Smart Healthcare Using IoT and Big Data Analytics</article-title><source>Smart Techniques for a Smarter Planet: Towards Smarter Algorithms</source><publisher-name>Springer International Publishing</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2019</year><fpage>211</fpage><lpage>241</lpage><pub-id pub-id-type="doi">10.1007/978-3-030-03131-2_11</pub-id></element-citation></ref><ref id="B64-healthcare-11-01152"><label>64.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Tawalbeh</surname><given-names>L.</given-names></name><name name-style="western"><surname>Muheidat</surname><given-names>F.</given-names></name><name name-style="western"><surname>Tawalbeh</surname><given-names>M.</given-names></name><name name-style="western"><surname>Quwaider</surname><given-names>M.</given-names></name></person-group><article-title>IoT Privacy and Security: Challenges and Solutions</article-title><source>Appl. Sci.</source><year>2020</year><volume>10</volume><elocation-id>4102</elocation-id><pub-id pub-id-type="doi">10.3390/app10124102</pub-id></element-citation></ref><ref id="B65-healthcare-11-01152"><label>65.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Awotunde</surname><given-names>J.B.</given-names></name><name name-style="western"><surname>Jimoh</surname><given-names>R.G.</given-names></name><name name-style="western"><surname>Folorunso</surname><given-names>S.O.</given-names></name><name name-style="western"><surname>Adeniyi</surname><given-names>E.A.</given-names></name><name name-style="western"><surname>Abiodun</surname><given-names>K.M.</given-names></name><name name-style="western"><surname>Banjo</surname><given-names>O.O.</given-names></name></person-group><article-title>Privacy and Security Concerns in IoT-Based Healthcare Systems</article-title><source>The Fusion of Internet of Things, Artificial Intelligence, and Cloud Computing in Health Care</source><publisher-name>Springer International Publishing</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2021</year><fpage>105</fpage><lpage>134</lpage><pub-id pub-id-type="doi">10.1007/978-3-030-75220-0_6</pub-id></element-citation></ref><ref id="B66-healthcare-11-01152"><label>66.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Redmon</surname><given-names>J.</given-names></name><name name-style="western"><surname>Farhadi</surname><given-names>A.</given-names></name></person-group><article-title>YOLOv3: An Incremental Improvement</article-title><source>arXiv</source><year>2018</year><pub-id pub-id-type="doi">10.48550/ARXIV.1804.02767</pub-id><pub-id pub-id-type="arxiv">1804.02767</pub-id></element-citation></ref><ref id="B67-healthcare-11-01152"><label>67.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Russakovsky</surname><given-names>O.</given-names></name><name name-style="western"><surname>Deng</surname><given-names>J.</given-names></name><name name-style="western"><surname>Su</surname><given-names>H.</given-names></name><name name-style="western"><surname>Krause</surname><given-names>J.</given-names></name><name name-style="western"><surname>Satheesh</surname><given-names>S.</given-names></name><name name-style="western"><surname>Ma</surname><given-names>S.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Karpathy</surname><given-names>A.</given-names></name><name name-style="western"><surname>Khosla</surname><given-names>A.</given-names></name><name name-style="western"><surname>Bernstein</surname><given-names>M.S.</given-names></name><etal/></person-group><article-title>ImageNet Large Scale Visual Recognition Challenge</article-title><source>Int. J. Comput. Vis.</source><year>2015</year><volume>115</volume><fpage>211</fpage><lpage>252</lpage><pub-id pub-id-type="doi">10.1007/s11263-015-0816-y</pub-id></element-citation></ref><ref id="B68-healthcare-11-01152"><label>68.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Pan</surname><given-names>C.</given-names></name><name name-style="western"><surname>Cao</surname><given-names>H.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Song</surname><given-names>X.</given-names></name><name name-style="western"><surname>Li</surname><given-names>M.</given-names></name></person-group><article-title>Driver activity recognition using spatial-temporal graph convolutional LSTM networks with attention mechanism</article-title><source>IET Intell. Transp. Syst.</source><year>2020</year><volume>15</volume><fpage>297</fpage><lpage>307</lpage><pub-id pub-id-type="doi">10.1049/itr2.12025</pub-id></element-citation></ref><ref id="B69-healthcare-11-01152"><label>69.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Vasconez</surname><given-names>J.</given-names></name><name name-style="western"><surname>Admoni</surname><given-names>H.</given-names></name><name name-style="western"><surname>Cheein</surname><given-names>F.A.</given-names></name></person-group><article-title>A methodology for semantic action recognition based on pose and human-object interaction in avocado harvesting processes</article-title><source>Comput. Electron. Agric.</source><year>2021</year><volume>184</volume><fpage>106057</fpage><pub-id pub-id-type="doi">10.1016/j.compag.2021.106057</pub-id></element-citation></ref><ref id="B70-healthcare-11-01152"><label>70.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>C.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>X.</given-names></name></person-group><article-title>Bed-Leaving Action Recognition Based on YOLOv3 and AlphaPose</article-title><source>Proceedings of the 2022 the 5th International Conference on Image and Graphics Processing (ICIGP), ICIGP 2022</source><conf-loc>Beijing, China</conf-loc><conf-date>7&#8211;9 January 2022</conf-date><fpage>117</fpage><lpage>123</lpage><pub-id pub-id-type="doi">10.1145/3512388.3512406</pub-id></element-citation></ref><ref id="B71-healthcare-11-01152"><label>71.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Zhao</surname><given-names>X.</given-names></name><name name-style="western"><surname>Hou</surname><given-names>F.</given-names></name><name name-style="western"><surname>Su</surname><given-names>J.</given-names></name><name name-style="western"><surname>Davis</surname><given-names>L.</given-names></name></person-group><article-title>An Alphapose-Based Pedestrian Fall Detection Algorithm</article-title><source>Proceedings of the Artificial Intelligence and Security</source><conf-loc>Qinghai, China</conf-loc><conf-date>15&#8211;20 July 2022</conf-date><fpage>650</fpage><lpage>660</lpage></element-citation></ref><ref id="B72-healthcare-11-01152"><label>72.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Cort&#233;s</surname><given-names>X.</given-names></name><name name-style="western"><surname>Conte</surname><given-names>D.</given-names></name><name name-style="western"><surname>Cardot</surname><given-names>H.</given-names></name></person-group><article-title>A new bag of visual words encoding method for human action recognition</article-title><source>Proceedings of the 2018 24th International Conference on Pattern Recognition (ICPR)</source><conf-loc>Beijing, China</conf-loc><conf-date>20&#8211;24 August 2018</conf-date><fpage>2480</fpage><lpage>2485</lpage><pub-id pub-id-type="doi">10.1109/ICPR.2018.8545886</pub-id></element-citation></ref><ref id="B73-healthcare-11-01152"><label>73.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Aslan</surname><given-names>M.</given-names></name><name name-style="western"><surname>Durdu</surname><given-names>A.</given-names></name><name name-style="western"><surname>Sabanci</surname><given-names>K.</given-names></name></person-group><article-title>Human action recognition with bag of visual words using different machine learning methods and hyperparameter optimization</article-title><source>Neural Comput. Appl.</source><year>2020</year><volume>32</volume><fpage>8585</fpage><lpage>8597</lpage><pub-id pub-id-type="doi">10.1007/s00521-019-04365-9</pub-id></element-citation></ref><ref id="B74-healthcare-11-01152"><label>74.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Nazir</surname><given-names>S.</given-names></name><name name-style="western"><surname>Yousaf</surname><given-names>M.H.</given-names></name><name name-style="western"><surname>Velastin</surname><given-names>S.A.</given-names></name></person-group><article-title>Evaluating a bag-of-visual features approach using spatio-temporal features for action recognition</article-title><source>Comput. Electr. Eng.</source><year>2018</year><volume>72</volume><fpage>660</fpage><lpage>669</lpage><pub-id pub-id-type="doi">10.1016/j.compeleceng.2018.01.037</pub-id></element-citation></ref><ref id="B75-healthcare-11-01152"><label>75.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Fang</surname><given-names>H.S.</given-names></name><name name-style="western"><surname>Li</surname><given-names>J.</given-names></name><name name-style="western"><surname>Tang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>C.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Xiu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Y.L.</given-names></name><name name-style="western"><surname>Lu</surname><given-names>C.</given-names></name></person-group><article-title>AlphaPose: Whole-Body Regional Multi-Person Pose Estimation and Tracking in Real-Time</article-title><source>arXiv</source><year>2022</year><pub-id pub-id-type="arxiv">2211.03375</pub-id><pub-id pub-id-type="doi">10.1109/TPAMI.2022.3222784</pub-id><pub-id pub-id-type="pmid">37145952</pub-id></element-citation></ref><ref id="B76-healthcare-11-01152"><label>76.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Fang</surname><given-names>H.S.</given-names></name><name name-style="western"><surname>Xie</surname><given-names>S.</given-names></name><name name-style="western"><surname>Tai</surname><given-names>Y.W.</given-names></name><name name-style="western"><surname>Lu</surname><given-names>C.</given-names></name></person-group><article-title>RMPE: Regional Multi-person Pose Estimation</article-title><source>arXiv</source><year>2016</year><pub-id pub-id-type="doi">10.48550/ARXIV.1612.00137</pub-id><pub-id pub-id-type="arxiv">1612.00137</pub-id></element-citation></ref><ref id="B77-healthcare-11-01152"><label>77.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wan</surname><given-names>C.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>L.</given-names></name><name name-style="western"><surname>Phoha</surname><given-names>V.V.</given-names></name></person-group><article-title>A Survey on Gait Recognition</article-title><source>ACM Comput. Surv.</source><year>2018</year><volume>51</volume><fpage>1</fpage><lpage>35</lpage><pub-id pub-id-type="doi">10.1145/3230633</pub-id></element-citation></ref><ref id="B78-healthcare-11-01152"><label>78.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Semwal</surname><given-names>V.B.</given-names></name><name name-style="western"><surname>Mazumdar</surname><given-names>A.</given-names></name><name name-style="western"><surname>Jha</surname><given-names>A.</given-names></name><name name-style="western"><surname>Gaud</surname><given-names>N.</given-names></name><name name-style="western"><surname>Bijalwan</surname><given-names>V.</given-names></name></person-group><article-title>Speed, Cloth and Pose Invariant Gait Recognition-Based Person Identification</article-title><source>Machine Learning: Theoretical Foundations and Practical Applications</source><publisher-name>Springer Singapore</publisher-name><publisher-loc>Singapore</publisher-loc><year>2021</year><fpage>39</fpage><lpage>56</lpage><pub-id pub-id-type="doi">10.1007/978-981-33-6518-6_3</pub-id></element-citation></ref><ref id="B79-healthcare-11-01152"><label>79.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Elharrouss</surname><given-names>O.</given-names></name><name name-style="western"><surname>Almaadeed</surname><given-names>N.</given-names></name><name name-style="western"><surname>Al-ma&#8217;adeed</surname><given-names>S.</given-names></name><name name-style="western"><surname>Bouridane</surname><given-names>A.</given-names></name></person-group><article-title>Gait recognition for person re-identification</article-title><source>J. Supercomput.</source><year>2021</year><volume>77</volume><fpage>3653</fpage><lpage>3672</lpage><pub-id pub-id-type="doi">10.1007/s11227-020-03409-5</pub-id></element-citation></ref><ref id="B80-healthcare-11-01152"><label>80.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Sun</surname><given-names>F.</given-names></name><name name-style="western"><surname>Zang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Gravina</surname><given-names>R.</given-names></name><name name-style="western"><surname>Fortino</surname><given-names>G.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Y.</given-names></name></person-group><article-title>Gait-based identification for elderly users in wearable healthcare systems</article-title><source>Inf. Fusion</source><year>2020</year><volume>53</volume><fpage>134</fpage><lpage>144</lpage><pub-id pub-id-type="doi">10.1016/j.inffus.2019.06.023</pub-id></element-citation></ref><ref id="B81-healthcare-11-01152"><label>81.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>C.</given-names></name><name name-style="western"><surname>Zheng</surname><given-names>B.</given-names></name><name name-style="western"><surname>Guo</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Duan</surname><given-names>X.</given-names></name><name name-style="western"><surname>Wulamu</surname><given-names>A.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>D.</given-names></name></person-group><article-title>Wearable Devices for Gait Analysis in Intelligent Healthcare</article-title><source>Front. Comput. Sci.</source><year>2021</year><volume>3</volume><fpage>661676</fpage><pub-id pub-id-type="doi">10.3389/fcomp.2021.661676</pub-id></element-citation></ref><ref id="B82-healthcare-11-01152"><label>82.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhao</surname><given-names>A.</given-names></name><name name-style="western"><surname>Li</surname><given-names>J.</given-names></name><name name-style="western"><surname>Dong</surname><given-names>J.</given-names></name><name name-style="western"><surname>Qi</surname><given-names>L.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Li</surname><given-names>N.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>H.</given-names></name></person-group><article-title>Multimodal Gait Recognition for Neurodegenerative Diseases</article-title><source>IEEE Trans. Cybern.</source><year>2022</year><volume>52</volume><fpage>9439</fpage><lpage>9453</lpage><pub-id pub-id-type="doi">10.1109/TCYB.2021.3056104</pub-id><pub-id pub-id-type="pmid">33705337</pub-id></element-citation></ref><ref id="B83-healthcare-11-01152"><label>83.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Din</surname><given-names>S.</given-names></name><name name-style="western"><surname>Elshehabi</surname><given-names>M.</given-names></name><name name-style="western"><surname>Galna</surname><given-names>B.</given-names></name><name name-style="western"><surname>Hobert</surname><given-names>M.</given-names></name><name name-style="western"><surname>Warmerdam</surname><given-names>E.</given-names></name><name name-style="western"><surname>S&#252;nkel</surname><given-names>U.</given-names></name><name name-style="western"><surname>Brockmann</surname><given-names>K.</given-names></name><name name-style="western"><surname>Metzger</surname><given-names>F.</given-names></name><name name-style="western"><surname>Hansen</surname><given-names>C.</given-names></name><name name-style="western"><surname>Berg</surname><given-names>D.</given-names></name><etal/></person-group><article-title>Gait analysis with wearables predicts conversion to Parkinson disease</article-title><source>Ann. Neurol.</source><year>2019</year><volume>86</volume><fpage>357</fpage><lpage>367</lpage><pub-id pub-id-type="doi">10.1002/ana.25548</pub-id><pub-id pub-id-type="pmid">31294853</pub-id><pub-id pub-id-type="pmcid">PMC6899833</pub-id></element-citation></ref><ref id="B84-healthcare-11-01152"><label>84.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Rucco</surname><given-names>R.</given-names></name><name name-style="western"><surname>Agosti</surname><given-names>V.</given-names></name><name name-style="western"><surname>Jacini</surname><given-names>F.</given-names></name><name name-style="western"><surname>Sorrentino</surname><given-names>P.</given-names></name><name name-style="western"><surname>Varriale</surname><given-names>P.</given-names></name><name name-style="western"><surname>De Stefano</surname><given-names>M.</given-names></name><name name-style="western"><surname>Milan</surname><given-names>G.</given-names></name><name name-style="western"><surname>Montella</surname><given-names>P.</given-names></name><name name-style="western"><surname>Sorrentino</surname><given-names>G.</given-names></name></person-group><article-title>Spatio-temporal and kinematic gait analysis in patients with Frontotemporal dementia and Alzheimer&#8217;s disease through 3D motion capture</article-title><source>Gait Posture</source><year>2017</year><volume>52</volume><fpage>312</fpage><lpage>317</lpage><pub-id pub-id-type="doi">10.1016/j.gaitpost.2016.12.021</pub-id><pub-id pub-id-type="pmid">28038340</pub-id></element-citation></ref><ref id="B85-healthcare-11-01152"><label>85.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>de Oliveira Silva</surname><given-names>F.</given-names></name><name name-style="western"><surname>Ferreira</surname><given-names>J.V.</given-names></name><name name-style="western"><surname>Pl&#225;cido</surname><given-names>J.</given-names></name><name name-style="western"><surname>Chagas</surname><given-names>D.</given-names></name><name name-style="western"><surname>Praxedes</surname><given-names>J.</given-names></name><name name-style="western"><surname>Guimar&#227;es</surname><given-names>C.</given-names></name><name name-style="western"><surname>Batista</surname><given-names>L.A.</given-names></name><name name-style="western"><surname>Laks</surname><given-names>J.</given-names></name><name name-style="western"><surname>Deslandes</surname><given-names>A.C.</given-names></name></person-group><article-title>Gait analysis with videogrammetry can differentiate healthy elderly, mild cognitive impairment, and Alzheimer&#8217;s disease: A cross-sectional study</article-title><source>Exp. Gerontol.</source><year>2020</year><volume>131</volume><fpage>110816</fpage><pub-id pub-id-type="doi">10.1016/j.exger.2019.110816</pub-id><pub-id pub-id-type="pmid">31862421</pub-id></element-citation></ref><ref id="B86-healthcare-11-01152"><label>86.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yamada</surname><given-names>H.</given-names></name><name name-style="western"><surname>Ahn</surname><given-names>J.</given-names></name><name name-style="western"><surname>Mozos</surname><given-names>O.</given-names></name><name name-style="western"><surname>Iwashita</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Kurazume</surname><given-names>R.</given-names></name></person-group><article-title>Gait-based person identification using 3D LiDAR and long short-term memory deep networks</article-title><source>Adv. Robot.</source><year>2020</year><volume>34</volume><fpage>1201</fpage><lpage>1211</lpage><pub-id pub-id-type="doi">10.1080/01691864.2020.1793812</pub-id></element-citation></ref></ref-list></back><floats-group><fig position="float" id="healthcare-11-01152-f001" orientation="portrait"><label>Figure 1</label><caption><p>Data collection pipeline of the GCM system.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="healthcare-11-01152-g001.jpg"/></fig><fig position="float" id="healthcare-11-01152-f002" orientation="portrait"><label>Figure 2</label><caption><p>The architecture of YOLOv3 algorithm.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="healthcare-11-01152-g002.jpg"/></fig><fig position="float" id="healthcare-11-01152-f003" orientation="portrait"><label>Figure 3</label><caption><p>AlphaPose algorithm illustration: keypoints on patients&#8217; bodies in video footage.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="healthcare-11-01152-g003.jpg"/></fig><fig position="float" id="healthcare-11-01152-f004" orientation="portrait"><label>Figure 4</label><caption><p>UML deployment diagram of the geriatric care system architecture.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="healthcare-11-01152-g004.jpg"/></fig><fig position="float" id="healthcare-11-01152-f005" orientation="portrait"><label>Figure 5</label><caption><p>Six different health parameters collected for a single patient.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="healthcare-11-01152-g005.jpg"/></fig><fig position="float" id="healthcare-11-01152-f006" orientation="portrait"><label>Figure 6</label><caption><p>Examples of different care plan with IF-THEN rules defined by the staff.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="healthcare-11-01152-g006.jpg"/></fig><fig position="float" id="healthcare-11-01152-f007" orientation="portrait"><label>Figure 7</label><caption><p>Schematic diagram of proposed geriatric care management systems.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="healthcare-11-01152-g007.jpg"/></fig><fig position="float" id="healthcare-11-01152-f008" orientation="portrait"><label>Figure 8</label><caption><p>Three iterations of the assessment frames of the patient in the &#8220;walking&#8221; pose taken every five seconds.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="healthcare-11-01152-g008.jpg"/></fig><fig position="float" id="healthcare-11-01152-f009" orientation="portrait"><label>Figure 9</label><caption><p>Examples of skeleton-based posture recognition in various ambient light conditions: (<bold>a</bold>) patient walks in a semi-lit environment; (<bold>b</bold>) patient walks during the night; (<bold>c</bold>) two patients sit in a fully-lit environment; (<bold>d</bold>) patient is lying down at night.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="healthcare-11-01152-g009.jpg"/></fig><fig position="float" id="healthcare-11-01152-f010" orientation="portrait"><label>Figure 10</label><caption><p>Examples of skeleton-based posture recognition in different scenarios: (<bold>a</bold>) the patient is lying on the ground; (<bold>b</bold>) patients are visited by nursing staff.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="healthcare-11-01152-g010.jpg"/></fig><fig position="float" id="healthcare-11-01152-f011" orientation="portrait"><label>Figure 11</label><caption><p>Testing results: confusion matrix of posture classification.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="healthcare-11-01152-g011.jpg"/></fig><fig position="float" id="healthcare-11-01152-f012" orientation="portrait"><label>Figure 12</label><caption><p>Testing results: ROC curve of the posture recognition algorithm.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="healthcare-11-01152-g012.jpg"/></fig><fig position="float" id="healthcare-11-01152-f013" orientation="portrait"><label>Figure 13</label><caption><p>Confusion matrix showing the re-identification of three patients (referred to by the class labels &#8220;First", &#8220;Second", and &#8220;Third".)</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="healthcare-11-01152-g013.jpg"/></fig><fig position="float" id="healthcare-11-01152-f014" orientation="portrait"><label>Figure 14</label><caption><p>Confusion matrix of the re-identification of three patients (referred to class labels &#8220;First&#8221;, &#8220;Second&#8221;, and &#8220;Third&#8221;) in the two common areas of nursing homes.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="healthcare-11-01152-g014.jpg"/></fig><table-wrap position="float" id="healthcare-11-01152-t001" orientation="portrait"><object-id pub-id-type="pii">healthcare-11-01152-t001_Table 1</object-id><label>Table 1</label><caption><p>Types of IoT devices used in the research.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">IoT Device Type</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Device Name</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Camera</td><td align="center" valign="middle" rowspan="1" colspan="1">EZVIZ CS-C3TN 1920&#160;&#215;&#160;1080</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Wrist band</td><td align="center" valign="middle" rowspan="1" colspan="1">Fitbit Charge 5</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Blood Pressure</td><td align="center" valign="middle" rowspan="1" colspan="1">Withings BPM Connect</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Scales</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Withings Body+</td></tr></tbody></table></table-wrap><table-wrap position="float" id="healthcare-11-01152-t002" orientation="portrait"><object-id pub-id-type="pii">healthcare-11-01152-t002_Table 2</object-id><label>Table 2</label><caption><p>Patient information.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">No</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Variable</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Definition</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Instances of Possible Values/Range</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">1</td><td align="left" valign="middle" rowspan="1" colspan="1">FN</td><td align="left" valign="middle" rowspan="1" colspan="1">First name</td><td align="left" valign="middle" rowspan="1" colspan="1">-</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">2</td><td align="left" valign="middle" rowspan="1" colspan="1">LN</td><td align="left" valign="middle" rowspan="1" colspan="1">Last name</td><td align="left" valign="middle" rowspan="1" colspan="1">-</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">3</td><td align="left" valign="middle" rowspan="1" colspan="1">BD</td><td align="left" valign="middle" rowspan="1" colspan="1">Birth date</td><td align="left" valign="middle" rowspan="1" colspan="1">yyyy/mm/dd</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">HE</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Height</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.20 m&#8211;2.20 m</td></tr><tr><td colspan="4" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">Input data</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">1</td><td align="left" valign="middle" rowspan="1" colspan="1">MoveC</td><td align="left" valign="middle" rowspan="1" colspan="1">Movement capabilities</td><td align="left" valign="middle" rowspan="1" colspan="1">Lying; sitting in a wheelchair; with assistive devices; etc.</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">2</td><td align="left" valign="middle" rowspan="1" colspan="1">RiskC</td><td align="left" valign="middle" rowspan="1" colspan="1">Risk of collapse</td><td align="left" valign="middle" rowspan="1" colspan="1">None; low; medium; high</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">3</td><td align="left" valign="middle" rowspan="1" colspan="1">Bedsores</td><td align="left" valign="middle" rowspan="1" colspan="1">Bedsores</td><td align="left" valign="middle" rowspan="1" colspan="1">Yes; no</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">4</td><td align="left" valign="middle" rowspan="1" colspan="1">Diseases</td><td align="left" valign="middle" rowspan="1" colspan="1">All patient&#8217;s diseases</td><td align="left" valign="middle" rowspan="1" colspan="1">Heart failure; Alzheimer; dementia; Cancer; etc.</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">5</td><td align="left" valign="middle" rowspan="1" colspan="1">Med</td><td align="left" valign="middle" rowspan="1" colspan="1">Taken medications</td><td align="left" valign="middle" rowspan="1" colspan="1">Antibiotics; antihypertensives; antidepressants; etc.</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">6</td><td align="left" valign="middle" rowspan="1" colspan="1">BMI</td><td align="left" valign="middle" rowspan="1" colspan="1">BMI unit change per week</td><td align="left" valign="middle" rowspan="1" colspan="1">&lt;0.5 plus; &lt;0.5 minus; 0.5&#8211;1 plus; etc.</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">7</td><td align="left" valign="middle" rowspan="1" colspan="1">MoveH</td><td align="left" valign="middle" rowspan="1" colspan="1">Movement habits</td><td align="left" valign="middle" rowspan="1" colspan="1">Unchanged; slowed down; increased; falling on the ground</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">8</td><td align="left" valign="middle" rowspan="1" colspan="1">EatH</td><td align="left" valign="middle" rowspan="1" colspan="1">Eating habits</td><td align="left" valign="middle" rowspan="1" colspan="1">Parenteral nutrition; fed by another person; independent eating; etc.</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">9</td><td align="left" valign="middle" rowspan="1" colspan="1">EatC</td><td align="left" valign="middle" rowspan="1" colspan="1">Eating capabilities</td><td align="left" valign="middle" rowspan="1" colspan="1">Swallows solid food; swallows only mashed food; swallows only liquids; etc.</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">10</td><td align="left" valign="middle" rowspan="1" colspan="1">Bowel</td><td align="left" valign="middle" rowspan="1" colspan="1">Bowel habits</td><td align="left" valign="middle" rowspan="1" colspan="1">Regular bowel movements; diarrhoea; constipation; faecal incontinence</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">11</td><td align="left" valign="middle" rowspan="1" colspan="1">Sleep</td><td align="left" valign="middle" rowspan="1" colspan="1">Sleeping</td><td align="left" valign="middle" rowspan="1" colspan="1">&lt;4 h; 4&#8211;6 h; 6&#8211;8 h; &gt;8 h; apnoea</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">12</td><td align="left" valign="middle" rowspan="1" colspan="1">Breath</td><td align="left" valign="middle" rowspan="1" colspan="1">Breathing</td><td align="left" valign="middle" rowspan="1" colspan="1">Increased; slowing down; with apnoeas</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">13</td><td align="left" valign="middle" rowspan="1" colspan="1">PL</td><td align="left" valign="middle" rowspan="1" colspan="1">Pulse</td><td align="left" valign="middle" rowspan="1" colspan="1">Normal; bradycardia; tachycardia</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">14</td><td align="left" valign="middle" rowspan="1" colspan="1">BP</td><td align="left" valign="middle" rowspan="1" colspan="1">Blood Pressure</td><td align="left" valign="middle" rowspan="1" colspan="1">Normotension; hypotension; hypertension mild; hypertension moderate; hypertension severe; etc.</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">15</td><td align="left" valign="middle" rowspan="1" colspan="1">Temp</td><td align="left" valign="middle" rowspan="1" colspan="1">Temperature</td><td align="left" valign="middle" rowspan="1" colspan="1">&lt;36.0 &#176;C; 36.0&#8211;37.4 &#176;C; etc.</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">16</td><td align="left" valign="middle" rowspan="1" colspan="1">Sat</td><td align="left" valign="middle" rowspan="1" colspan="1">Saturation</td><td align="left" valign="middle" rowspan="1" colspan="1">&#8805;94%; &lt;94%</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">17</td><td align="left" valign="middle" rowspan="1" colspan="1">Urine</td><td align="left" valign="middle" rowspan="1" colspan="1">Daily urine output</td><td align="left" valign="middle" rowspan="1" colspan="1">Concentrated urine; very frequent; etc.</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">18</td><td align="left" valign="middle" rowspan="1" colspan="1">Fluid</td><td align="left" valign="middle" rowspan="1" colspan="1">Fluid tracking</td><td align="left" valign="middle" rowspan="1" colspan="1">&lt;500 mL; &#8805;500 mL</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">19</td><td align="left" valign="middle" rowspan="1" colspan="1">Gly</td><td align="left" valign="middle" rowspan="1" colspan="1">Glycaemia</td><td align="left" valign="middle" rowspan="1" colspan="1">&lt;2.5 mmol/l; &#8805;2.5 mmol/l</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">20</td><td align="left" valign="middle" rowspan="1" colspan="1">Con</td><td align="left" valign="middle" rowspan="1" colspan="1">Consciousness</td><td align="left" valign="middle" rowspan="1" colspan="1">Unchanged; changed; unconscious</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">21</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Pain</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Perceived level of pain</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">None; mild; moderate; severe; unbearable</td></tr><tr><td colspan="4" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">Output data</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Plan</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Nursing plan</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Continue current plan; monitor; adjust; extra situation</td></tr></tbody></table></table-wrap><table-wrap position="float" id="healthcare-11-01152-t003" orientation="portrait"><object-id pub-id-type="pii">healthcare-11-01152-t003_Table 3</object-id><label>Table 3</label><caption><p>Testing results: performance metrics of the posture recognition algorithm.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Class</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Precision</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Recall</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1"><italic toggle="yes">F</italic>1 Score</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">Walking (WAL)</td><td align="left" valign="middle" rowspan="1" colspan="1">0.9554</td><td align="left" valign="middle" rowspan="1" colspan="1">0.9374</td><td align="left" valign="middle" rowspan="1" colspan="1">0.9463</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Standing (STA)</td><td align="left" valign="middle" rowspan="1" colspan="1">0.8722</td><td align="left" valign="middle" rowspan="1" colspan="1">0.9163</td><td align="left" valign="middle" rowspan="1" colspan="1">0.8937</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Sitting (SIT)</td><td align="left" valign="middle" rowspan="1" colspan="1">0.9406</td><td align="left" valign="middle" rowspan="1" colspan="1">0.9427</td><td align="left" valign="middle" rowspan="1" colspan="1">0.9416</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Fallen on the ground (FOG)</td><td align="left" valign="middle" rowspan="1" colspan="1">0.9354</td><td align="left" valign="middle" rowspan="1" colspan="1">0.8333</td><td align="left" valign="middle" rowspan="1" colspan="1">0.8814</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Lying in bed (LIB)</td><td align="left" valign="middle" rowspan="1" colspan="1">0.8951</td><td align="left" valign="middle" rowspan="1" colspan="1">0.8878</td><td align="left" valign="middle" rowspan="1" colspan="1">0.8914</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Sleeping (SLE)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.8844</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.9047</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.8944</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
</td><td colspan="2" align="center" valign="middle" rowspan="1">Macro <italic toggle="yes">F</italic>1 score</td><td align="left" valign="middle" rowspan="1" colspan="1">0.9082</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td colspan="2" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">Weighted <italic toggle="yes">F</italic>1 score</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.9125</td></tr></tbody></table></table-wrap><table-wrap position="float" id="healthcare-11-01152-t004" orientation="portrait"><object-id pub-id-type="pii">healthcare-11-01152-t004_Table 4</object-id><label>Table 4</label><caption><p>Real-time scenario testing results of posture recognition.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">No.</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Actual Pose</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Predicted Pose</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Ambient Lighting</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Confidence</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">1</td><td align="left" valign="middle" rowspan="1" colspan="1">Walking</td><td align="left" valign="middle" rowspan="1" colspan="1">Walking</td><td align="left" valign="middle" rowspan="1" colspan="1">Day time (well-lit)</td><td align="left" valign="middle" rowspan="1" colspan="1">98.0%</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">2</td><td align="left" valign="middle" rowspan="1" colspan="1">Sitting</td><td align="left" valign="middle" rowspan="1" colspan="1">Sitting</td><td align="left" valign="middle" rowspan="1" colspan="1">Day time (well-lit)</td><td align="left" valign="middle" rowspan="1" colspan="1">97.5%</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">3</td><td align="left" valign="middle" rowspan="1" colspan="1">Sitting</td><td align="left" valign="middle" rowspan="1" colspan="1">Sitting</td><td align="left" valign="middle" rowspan="1" colspan="1">Day time (well-lit)</td><td align="left" valign="middle" rowspan="1" colspan="1">98.2%</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">4</td><td align="left" valign="middle" rowspan="1" colspan="1">Lying in bed</td><td align="left" valign="middle" rowspan="1" colspan="1">Sleeping</td><td align="left" valign="middle" rowspan="1" colspan="1">Night time (poorly lit)</td><td align="left" valign="middle" rowspan="1" colspan="1">89.3%</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">5</td><td align="left" valign="middle" rowspan="1" colspan="1">Standing</td><td align="left" valign="middle" rowspan="1" colspan="1">Standing</td><td align="left" valign="middle" rowspan="1" colspan="1">Day time (perfect)</td><td align="left" valign="middle" rowspan="1" colspan="1">99.7%</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">6</td><td align="left" valign="middle" rowspan="1" colspan="1">Lying in bed</td><td align="left" valign="middle" rowspan="1" colspan="1">Lying in bed</td><td align="left" valign="middle" rowspan="1" colspan="1">Evening time (semi-lit)</td><td align="left" valign="middle" rowspan="1" colspan="1">87.9%</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">7</td><td align="left" valign="middle" rowspan="1" colspan="1">Sleeping</td><td align="left" valign="middle" rowspan="1" colspan="1">Lying in bed</td><td align="left" valign="middle" rowspan="1" colspan="1">Evening time (semi-lit)</td><td align="left" valign="middle" rowspan="1" colspan="1">88.6%</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">8</td><td align="left" valign="middle" rowspan="1" colspan="1">Standing</td><td align="left" valign="middle" rowspan="1" colspan="1">Standing</td><td align="left" valign="middle" rowspan="1" colspan="1">Day time (perfect)</td><td align="left" valign="middle" rowspan="1" colspan="1">99.1%</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">9</td><td align="left" valign="middle" rowspan="1" colspan="1">Sleeping</td><td align="left" valign="middle" rowspan="1" colspan="1">Sleeping</td><td align="left" valign="middle" rowspan="1" colspan="1">Night time (poorly lit)</td><td align="left" valign="middle" rowspan="1" colspan="1">85.4%</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">10</td><td align="left" valign="middle" rowspan="1" colspan="1">Walking</td><td align="left" valign="middle" rowspan="1" colspan="1">Walking</td><td align="left" valign="middle" rowspan="1" colspan="1">Day time (perfect)</td><td align="left" valign="middle" rowspan="1" colspan="1">93.6%</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">11</td><td align="left" valign="middle" rowspan="1" colspan="1">Lying in bed</td><td align="left" valign="middle" rowspan="1" colspan="1">Lying in bed</td><td align="left" valign="middle" rowspan="1" colspan="1">Day time (perfect)</td><td align="left" valign="middle" rowspan="1" colspan="1">94.2%</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">12</td><td align="left" valign="middle" rowspan="1" colspan="1">Standing</td><td align="left" valign="middle" rowspan="1" colspan="1">Standing</td><td align="left" valign="middle" rowspan="1" colspan="1">Day time (perfect)</td><td align="left" valign="middle" rowspan="1" colspan="1">99.3%</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">13</td><td align="left" valign="middle" rowspan="1" colspan="1">Walking</td><td align="left" valign="middle" rowspan="1" colspan="1">Walking</td><td align="left" valign="middle" rowspan="1" colspan="1">Night time (poorly lit)</td><td align="left" valign="middle" rowspan="1" colspan="1">96.0%</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">14</td><td align="left" valign="middle" rowspan="1" colspan="1">Sitting</td><td align="left" valign="middle" rowspan="1" colspan="1">Sitting</td><td align="left" valign="middle" rowspan="1" colspan="1">Day time (perfect)</td><td align="left" valign="middle" rowspan="1" colspan="1">98.5%</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">15</td><td align="left" valign="middle" rowspan="1" colspan="1">Sleeping</td><td align="left" valign="middle" rowspan="1" colspan="1">Sleeping</td><td align="left" valign="middle" rowspan="1" colspan="1">Day time (perfect)</td><td align="left" valign="middle" rowspan="1" colspan="1">91.0%</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">16</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Fallen on the ground</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Fallen on the ground</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Day time (perfect)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">99.8%</td></tr></tbody></table></table-wrap></floats-group></article></pmc-articleset>